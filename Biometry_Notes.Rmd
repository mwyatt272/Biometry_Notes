---
title: "Biometry Notes"
author: "Spring 2021"
output:
  html_document:
    number_sections: FALSE
    anchor_sections: TRUE
    fig_height: 5
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    toc_depth: 4
    theme: lumen
---
Week 1 Lecture
========================================================

Basic Outline
------------------------

First half: 

- R
- bootstrap, jackknife, and other randomization techniques
- hypothesis testing
- probability distributions 
- the “classic tests” of statistics
- graphical analysis of data 


Second half:

- regression (incl. ANOVA, ANCOVA)
- model building 
- model criticism 
- non-linear regression 
- multivariate regression

## Basic Probability Theory

Let's imagine that we have a bag with a mix of regular and peanut M&Ms. Each M&M has two traits: Color and Type.

$$ \sum_{all \: colors} P(color) = 1 $$
$$ \sum_{all \: types} P(types) = ? $$

### Intersection

```{r echo=FALSE, fig.align='center', fig.cap='Red shading represents the intersection. Source: Wikimedia Commons', out.width='25%'}
knitr::include_graphics("/Users/meganwyatt/Desktop/Biometry2021-master/inter.png")
```

Now let's pull one M&M out of the bag. *If* the color distribution of chocolate M&Ms and peanut M&Ms is the same, then these two traits are independent, and we can write the probability of being *both* Green and Peanut as  

$$ P(Green \: AND \:  Peanut) = P(Green \cap Peanut) = P(Green) \cdot P(Peanut) $$

This is called a Joint Probability and we usually write it as $P(Green,Peanut)$. *This only works if these two traits are independent of one another.* If color and type were not independent of one another, we would have to calculate the joint probability differently, but in the vast majority of cases we are working with data that we assuming are independent of one another. In these cases, the joint probability is simply the product of all the individual probabilities.

### Union

```{r echo=FALSE, fig.align='center', fig.cap='Red shading represents the union. Source: Wikimedia Commons', out.width='25%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/union.png')
```
 

$$ \begin{align*} 
P(Green \: OR \: Peanut) &= P(Green \cup Peanut) \\ 
&= P(Green) + P(Peanut) - P(Green \cap Peanut) \end{align*} $$ 

**Question: Why do we have to subtract off the intersection?**

If we do not subtract off the intersection, then the probability of Green AND Peanut will be double counted.

### Complement:

The complement of a trait represents everything that does *not* have that trait.

```{r echo=FALSE, fig.align='center', fig.cap='Red shading represents the complement. Source: Wikimedia Commons', out.width='25%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/comp.png')
```

$$ P(Green^c) = 1 - P(Green)  $$


## Multiple events

Let's consider what happens when we pull 2 M&Ms out of the bag

$$ P (Green \: AND \: THEN \: Blue) = P(Green) \cdot P(Blue) $$

**Question: What if we didn't care about the order?**

If we do not care about the order, then the combination of one Green M&M and one Blue M&M could have come about because we drew a Blue M&M and then a Green, or a Green and then a Blue. Because there are two ways to get this outcome (and they are mutually exclusive, so we can simply add the two probabilities), the total probability is simply 2 $\times$ P(Green) $\times$ P(Blue).

## Conditionals

$$ P(A \mid B) = P(A \: conditional \: on \: B) $$

$$ \begin{align*}  P(A,B) = P(A \cap B) &= P(A \mid B) \cdot P(B) \\
&= P(B \mid A) \cdot P(A) \end{align*} $$

## Bayes Theorem

$$ P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A) $$

$$ P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}$$

In Bayesian analyses (which we will not get into this semester), we are using this to calculate the probability of certain model parameters conditional on the data you have. But to find out more, you'll have to take BEE 569.

$$ P(parameters \mid data) \cdot P(data) = P(data \mid parameters) \cdot P(parameters) $$

$$ P(parameters \mid data) = \frac{P(data \mid parameters) \cdot P(parameters)}{P(data)}$$

## A few foundational ideas

There are a few statistics (a *statistic* is just something calculated from data) that we will need to know right at the beginning.

For illustration purposes, lets assume we have the following (sorted) series of data points: (1,3,3,4,7,8,13)

There are three statistics relating the "central tendancy": the *mean* (the average value; 5.57), the *mode* (the most common value; 3), and the *median* (the "middle" value; 4). We often denote the mean of a variable with a bar, as in $\bar{x}$. There are also two statistics relating to how much variation there is in the data. The *variance* measures the average squared distance between each point and the mean. For reasons that we will discuss in lab, we estimate the variance using the following formula

$$
\mbox{variance}_{unbiased} = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}
$$
rather than the more intuitive

$$
\mbox{variance}_{biased} = \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}
$$

Keep in mind that variance measures a distance squared. So if your data represent heights in m, than the variance will have units $m^{2}$ or square-meters.

The standard deviation is simply the square-root of variance, and is often denoted by the symbol $\sigma$.
$$\sigma = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}$$

If you were handed a distribution and you were asked to measure a characteristic "fatness" for the distribution, your estimate would be approximately $\sigma$. Note that $\sigma$ has the same units as the original data, so if your data were in meters, $\sigma$ would also be in meters.

We won't get to Normal distributions properly until Week 3, but we will need one fact about the "Standard Normal Distribution" now. The Standard Normal distribution is a Normal (or Gaussian, bell-shaped) distribution with mean equal to zero and standard deviation equal to 1. 68$\%$ of the probability is contained within 1 standard deviation of the mean (so from -$\sigma$ to +$\sigma$), and 95$\%$ of the probability is contained within 2 standard deviations of the mean (so from -2$\sigma$ to +2$\sigma$). (Actually, 95$\%$ is contained with 1.96 standard deviations, so sometimes we will use the more precise 1.96 and sometimes you will see this rounded to 2.)

Overview of Univariate Distributions 
--------------------------------

Discrete Distributions

- Binomial

- Multinomial

- Poisson

- Geometric

Continuous Distributions

- Normal/Gaussian

- Beta

- Gamma

- Student's t

- $\chi^2$


What can you ask of a distribution?
--------------------------------

- **Probability Density Function**: $P(x_1<X<x_2)$ (continuous distributions)

**Stop: Let's pause for a second and discuss the probability density function. This is a concdept that student's often struggle with. What is the interpretation of $P(x)$? What is $P(x=3)$? Can $P(x)$ ever be negative? [No.] Can $P(x)$ ever be greater than 1? [Yes! Why?]**

- **Probability Mass Function**: $P(X=x_1)$ (discrete distributions)

- **Cumulative Density Function (CDF)**: What is $P(X \le X^*)$?

- **Quantiles of the distributions**: What is $X^{*}$ if $P(X \le X^{*})=0.37$?

- **Sample from the distribution**: With a large enough sample, the histogram will come very close to the underlying PDF.

Note that the CDF is the integration of the PDF, and the PDF is the derivative of the CDF, so if you have one of these you can always get the other. Likewise, you can always get from the quantiles to the CDF (and then to the PDF). These three things are all equally informative about the shape of the distribution.


### Expected Value of a Random Variable

In probability theory the expected value of a random variable is the weighted average of all possible values that this random variable can take on. The weights used in computing this average correspond to the probabilities in case of a discrete random variable, or densities in case of continious random variable.


### Discrete Case

$$ X = \{X_1, X_2,...,X_k\} \\
E[X] = \sum_{i=1}^n{X_i \cdot P(X_i)}$$

- Example: Draw numbered balls with numbers 1, 2, 3, 4 and 5 with probabilities 0.1, 0.1, 0.1, 0.1, 0.6.

$$ \begin{align*} E[X] &= (0.1 \cdot 1) + (0.1 \cdot 2) + (0.1 \cdot 3) + (0.1 \cdot 4) + (0.6 \cdot 5) \\ &=4 \end{align*}$$


### Continuous Case

$$ E[X] = \int_{-\infty}^{\infty}{X \cdot f(X)dX}$$

Note that the function $f(x)$ in the above equation is the probability density function.

## A Brief Introduction to Scientific Method 

INDUCTIVE reasoning:

A set of specific observations $\rightarrow$ A general principle

Example: I observe a number of elephants and they were all gray. Therefore, all elephants are gray.


DEDUCTIVE reasoning:

A general principle $\rightarrow$ A set of predictions or explanations

Example: All elephants are gray. Therefore, I predict that this new (as yet undiscovered) species of elephant will be gray.

QUESTION: If this new species of elephant is green, what does this do to our hypothesis that all elephants are gray?


Some terminology:

**Null Hypothesis**: A statement encapsulating "no effect"

**Alternative Hypothesis**: A statement encapsulating "an effect""

- Fisher: Null hypothesis only

- Neyman and Pearson: H0 and H1, Weigh risk of of false positive against the false negative

- We use a hyprid approach


```{r echo=FALSE, fig.align='center', fig.cap='Hypothetico-deductive view of the scientific method. Photo Source: LSE Library', out.width='75%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/popper.png')
```

Not all hypotheses are created equal. Consider the following two hypotheses:

H$_{1}$: There are vultures in the local park

H$_{2}$: There are no vultures in the local park

Which of these two ways of framing the null hypothesis can be rejected by data?

**Hypothesis can only be rejected, they can never be accepted!**

\ 

"Based on the data obtained, we reject the null hypothesis that..."

\ or

"Based on the data obained, we fail to reject the null hypothesis that..."


More terminology


**Population**: Entire collection of individuals a researcher is interested in.

**Model**: Mathematical description of the quantity of interest. It combines a general description (functional form) with parameters (population parameters) that take specific values.

**Population paramater**: Some measure of a population (mean, standard deviation, range, etc.). Because populations are typically very large this quantity is unknown (and usually unknowable).

**Sample**: A subset of the population selected for the purposes of making inference for the popualtion.

**Sample Statistic**: Some measure of this sample that is used to infer the true value of the associated populatipn parameter.

An example:

**Population**: Fish density in a lake

**Sample**: You do 30 net tows and count all the fish in each tow

**Model**: $Y_i \sim Binom(p,N)$


The basic outline of statistical inference

sample(data) $\rightarrow$ sample statistics $\rightarrow$ ESTIMATOR $\rightarrow$ population parameter $\rightarrow$ underlying distribution


Estimators are imperfect tools

- Bias: The expected value $\neq$ population parameter

- Not consistent: As $n \to \infty$ sample statistic $\neq$ population parameter

- Variance


Week 2 Lecture
========================================================

Hypothesis testing and p-values
--------------------

There are few topics in statistics more controversial than the various philosophies behind null hypothesis testing. Over the next two weeks we will learn about the two paradigms (Fisher vs. Neyman-Pearson), the hybrid approach mostly commonly used in ecology, and criticisms of the whole enterprise. Bayesian statistics takes an entirely different approach than either Fisher or Neyman-Pearson, and the Bayesian approach resolves many of the inconsistencies involved with frequentist statistics, but at the expense of increased computation (and the use of prior information…). 

We frame decision-making in terms of a null and an alternative hypothesis.

$H_{0}$  vs. $H_{A}$

To take Karl Popper’s famous example:

$H_{0}$: There are no vultures in the park.

$H_{A}$: There are vultures in the park.

Note that the data may reject the null hypothesis (for example, finding vultures in the park), or the data may fail to reject the null hypothesis, but it can never prove the null hypothesis. **We cannot prove there are no vultures in the park.** We can only say that we were not able to find any vultures in the park, and therefore cannot reject the null hypothesis.

Fisher’s original context for developing significance testing was agricultural experiments that could be easily replicated. Fisher’s threshold of 0.05 was an arbitrary threshold for an effect to be considered worthy of continued experimentation. Any experiment that failed to reach this threshold would not be pursued. Experiments that gave “significant” results would be subject to additional experiments. These additional experiments may prove the original effect to be a fluke (and experiments would cease) or the additional experiments may provide confirmatory evidence that the effect was real. 

<a id="6steps"></a><span style="color: teal;">**Null hypothesis testing (as I will teach it) involves 6 steps.**</span>

**Step #1**: Specify a null hypothesis $H_{0}$
(Note that I do not include specification of the alternative hypothesis $H_{A}$ here. While the alternative hypothesis is useful as a mental construct, the basic approach deals only with $H_{0}$ and does not require a $H_{A}$.)

*Example*: In an experiment on vaccine efficacy, the null hypothesis would be that the probability of coronovirus infection is the same in the vaccinated group as in the control group that received only a placebo. Mathematically, this could be stated:

$H_{0}: P_{\small{\mbox{infection}}}^{\small{\mbox{vaccinated}}}=P_{\small{\mbox{infection}}}^{\small{\mbox{control}}}$

**Step #2**: Specific an appropriate test statistic T.
A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic. 

generic $T=f(X)$

specific $T^{*}$=T($X_{1}$,$X_{2}$,…,$X_{n}$)

We will introduce many more test statistics in the weeks to come.

*Example*: In our example with the vaccine trial, a reasonable test statistic would be 

$T=P_{\small{\mbox{infection}}}^{\small{\mbox{vaccinated}}}-P_{\small{\mbox{infection}}}^{\small{\mbox{control}}}$.

(This would be the best test statistic if probabilities were Normally distributed, but they are not. However, this test statistic is reasonable and highly intuitive and illustrates the basic point.)

**Step #3**: Determine the distribution of the test statistic under the null hypothesis $H_{0}$.
A test statistic is a statistical quantity that has a statistical distribution. 

$f(T│H_{0})$

Notice that this is the probability of obtaining the test statistic T **given** the null distribution, it is **not**
$f(H_{0}│T)$.

The test statistic and its distribution under the null hypothesis is the statistical test.

Test = Test statistic + Distribution of test statistic under $H_{0}$

**Step #4**: Collect data and calculate T*
Collect data by taking random samples from your population and calculate the test statistic from the sample data.

**Step #5**: Calculate a p-value

Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis 

$P(T^{*}│H_{0})$=p-value

**Step #6**: Interpret the p-value

Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected)

*Example*: In our vaccination example, we would look at the difference in these two populations (control vs. vaccinated) and if the difference between those two probabilities was larger than we would expect to occur under the null hypothesis (which, but assuming that the vaccine is no better than a placebo, assumes that these two groups are equivalent and therefore that any differences are due to random chance alone), then we would reject the null hypothesis that the vaccine is equivalent to the placebo. (Note that I haven't said anything about whether the vaccine lowers infection rates; in a few minutes we'll discuss one-tailed vs. two-tailed tests.)

**These steps apply for both parametric and non-parametric statistics.** Here we are introducing hypothesis testing through the lens of randomization procedures, but the same steps will be used again when we get into statistics involving parametric distributions (i.e. statistical distributions of known form and described by a finite number of parameters) and their properties. As you will see in a few weeks, most standard statistical tests involve a test statistic with a known distribution under the null hypothesis; here the distribution under the null hypothesis needs to be generated by randomization (randomization test). (We are starting with the randomization-based procedures because there is no math involved and it is more intuitive.)

The basic idea underlying all statistical tests: <span style="color: teal;">**What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the “p-value”). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis.**</span>

p-value = $P(data|H_{0})$

<a id="pvalint"></a><span style="color: teal;">**Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is not a statement about the probability that the null hypothesis is true.**</span> This logic can go wrong!!

Example:
	If a person is an American, he is probably not a member of Congress.
	This person is a member of Congress.
	Therefore he is probably not an American.

Let's draw a null distribution. In order to interpret the statistical test, we need to know whether we want a one-tailed test or a two-tailed test. <a id="tails"></a>In a one-tailed test, we would reject the null hypothesis only if the test statistic is larger than expected under in the null in one direction (5$%$ in one tail). In a two-tailed test, we would reject the null if the test statistic is larger in either direction (2.5$%$ in both tails).

**Example**: Let’s say I’m looking at the change in auto accident mortalities after a ban is enacted on driving while texting. We would expect that auto accident mortality would decrease after a ban on texting while driving. Let’s say, for arguments sake, that our test statistic T is the change in accident deaths

$H_{0}$: T=0 (no change in deaths)

$H_{A}$: T<0 (decline in deaths)

Another possible formulation of the null and alternative hypotheses is

$H_{0}$: T=0 (no change in deaths)

$H_{A}$: T $\neq$ 0 (increase or decline in deaths)

Why does it matter?

Consider the first case. To reject the null hypothesis, you would have to show that the measured decline $T^*$ was so large as to be very unlikely to have occurred by random chance assuming there was no true change in death rate. Therefore, you would require

$P(T \geq T^{*}│H_{0})<0.05$

to be true for you to decide to reject the null hypothesis. This is a one-tailed test. 

Consider the second case. To reject the null hypothesis, you would accept values of $T^{*}$ as significant if they were either very large or very small, and would divide the 5% critical region between the two tails

$P(T \geq T^{*}│H_{0})<0.025$

$P(T \leq T^{*}│H_{0})<0.025$

Notice that it now becomes a *more stringent test*. If $T^{*}$ is large, it now has to be even larger to qualify as "significant". This is a two-tailed test.

**<span style="color: indigo;">TWO KEY POINTS:</span>**

<span style="color: indigo;">1. If you are using a one-tailed test, you have to be willing to accept a result that is opposite in sign of what was expected as being PURELY BY CHANCE!! In other words, if traffic deaths went UP after the texting ban, you would have to be willing to accept that that was by pure chance and you would then fail to reject the null hypothesis of NO CHANGE. This is in fact what happened, by the way: Texting bans actually increase traffic deaths – WHY?</span>

<span style="color: indigo;">2. Before using the more “lenient” one-tailed test, make sure you really believe that results opposite to what you expect are only random</span>
	
You cannot do a one-tailed test, find the answer to have the wrong sign and then do a two-tailed test. While probably quite common, this is not statistically valid. You cannot use the data to generate the test!
Not all tests are created equal!! Tests differ in their power to detect differences, and their efficiency. The balance between power and efficiency depends on the specific situation; we will discuss this more next week.

We are going to introduce the idea of hypothesis testing through the practice of permutation tests, because it allows us to get into the flow of testing hypotheses without the burden of a lot of complicated mathematics. Moreover, in doing so, we introduce the more general concept of "generative models", which generate outcomes through simulation. For example, we can think of the statement $X \sim Pois(\lambda)$ as a generative model because it allows us to generate datasets that follow the distribution $Pois(\lambda)$. If we had a dataset and we wanted to know whether it came from a Poisson distribution, we could imagine generating lots of datasets using a generative model (i.e. drawn from $Pois(\lambda)$) and asking ourselves whether any of the generated datasets look anything like the dataset we have. In the same way, we can think about testing a null hypothesis $H_{0}$ by generating data under that null hypothesis, calculating some test statistics from that generated data, and asking whether our geneterated test statsitics "look like" the test statistic obtained from our real data. If not, then we can reject the null hypothesis. Simulations like this are enormously powerful tools for testing hypotheses and are often far more intuitive than the alternative "parametric" statistical tests we will learn in Weeks 3 and 4.

Permutation tests 
------------------------

<a id="permmethods"></a>Let’s say we have two random samples drawn from possibly different probability distributions F and G,

$F \rightarrow z=\{z_{1},z_{2},...,z_{n}\}$

$G \rightarrow y=\{y_{1},y_{2},...,y_{m}\}$

Having observed z and y, we wish to test the null hypothesis $H_{0}$ of no difference between F and G, $H_{0}:F=G$.
Note that the equality F=G means that the two distributions are exactly the same across their entire distribution, not just that their means are the same. If $H_{0}$ is true, than there is no probabilistic difference between drawing random values from F and drawing random values from G.

*What are some possible test statistics that we might use in this case?*

There are many test statistics that we could use to test this null hypothesis but lets use the difference in means as the test statistic. If there is a large difference in their means, than we can probably reject the null hypothesis that they represent the same underlying distribution.

$T=E[F]-E[G]=\bar{z}-\bar{y}$ 

The way to do this is to lump all the data together and to randomly permute the labels so that data are randomly assigned to a group (z vs. y). **<span style="color: orangered;">Important: We are not sampling with replacement here. We are simply permuting the labels to "erase" any possible correlation between group and the values of the data.</span>**

We then calculate the mean of the "fake z" group and the mean of the "fake y" group and take the difference. That is the result of ONE permutation. If we do that many many times (say, 10000 times) then the distribution of those differences reflects the distribution under the null hypothesis of no difference between F and G.

We will do an example like this in the problem set.

Parameter estimation
-------------------------

Hypothesis testing is a bit like the game 50-questions (Are you red? Are you blue? Each question is a null hypothesis to be rejected...).

<a id="effectsize"></a>Parameter estimation appears at first glance more direct, it just asks "What are you?", and in doing so it estimates the value of a parameter (mean growth rate of a fish population, for example) and provides a measure of uncertainty about that estimate. 

As a reminder, **estimators are tools that produce estimates of population statistics from sample statistics**. 

The basic outline of "statistical inference":
Data = sample $\rightarrow$ sample statistics $\rightarrow$ ESTIMATOR $\rightarrow$ population parameters

We generally use the word "statistic" when discussing the data, and "parameter" when discussing the underlying distribution.

**An "estimator" or "point estimate" is a statistic (that is, a function of the data) that is used to infer the value of an unknown parameter in a statistical model. If the parameter is denoted $\theta$ then the estimator is typically written by adding a "hat" over the symbol: $\hat{\theta}$. Being a function of the data, the estimator is itself a random variable; a particular realization of this random variable is called the "estimate".** Sometimes the words "estimator" and "estimate" are used interchangeably, but I will try and be consistent in using the word "estimator" for the function in the generic, and the word "estimate" for the result of applying that function to the data at hand.

Example:

$$
X \sim N(\mu,\sigma^{2})
$$

<a id="estimator"></a>We define the "estimator" for $\mu$ as

$$
\frac{1}{n}\sum_{i=1}^{n}X_{i} 
$$

Therefore, the "estimate" $\hat{\mu}$ is

$$
\hat{\mu}=\bar{X} =\frac{1}{n}\sum_{i=1}^{n}X_{i} 
$$

Estimators are imperfect tools, and they can suffer from *bias* and/or *variance* (or, equivalently, standard error). 

1. Bias: As $n \rightarrow \infty$, sample statistic does not converge to the population parameter

2. Standard error: Each individual estimate may be too low or too high from the true value (this can occur even if the long run average value is correct, i.e. unbiased). In other words, there is sample-to-sample variance in the estimates obtained from a given sample. 

It turns out that bias and variance trade-off, and this trade-off is controlled by the complexity of the model you are trying to fit. We will return to these ideas in Week 13 but I suggest reading through [this](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229) nice (and short) explanation of bias and variance (written from the perspective of machine learning, which is just another kind of model building enterprise).

**Why are estimators associated with a standard error? <span style="color: orangered;">If you were to do your experiment all over again, say 1000 times, the value of your estimate would be different each time.</span> Your 1000 estimates would have a statistical distribution with some spread, and the spread of these 1000 estimates is quantified by the standard error.**

(A good example of this from the Week 1 problem set is the question asking you the probability of getting *your* bag of M&Ms from a random sample of M&Ms. Let's say you calculated this probability by sampling 100,000 times and you got 4 "matches". This gives you an estimate of 0.00004. But if you repeated this experiement a second time, with *another* sample of 100,000 bags, you might get 7 matches or 2 matches or none at all! So 0.00004 is your estimate of the probability, but it is an estimate with some uncertainty because you don't know how variable that probability would be if you did the experiment multiple times. This "spread" of the estimates is the standard error of your estimate. Computationally, we can calculate the standard error by doing lots of experiments, but in practice, this is not always possible. Therefore, as we'll see in the next few weeks, we usually use the properties of statistical distributions to calculate the standard error of an estimate.)

How do we estimate the bias and variance (related to standard error) of an estimator?

While there are other methods that we will discuss in a few weeks, now we are going to introduce the idea through two non-parametric approaches: bootstrap and jackknife.

First we need to stop and discuss what it means to sample from an empirical distribution.

Let’s say I have a bunch of lotto balls in an urn

$X=\{X_{1},X_{2},X_{3},...,X_{n}\}$

and I want to draw sets of 5 lotto numbers from that urn. I can sample with replacement or without replacement.

If you sample with replacement, we may get some numbers more than once. It also means that if you draw n balls out of an urn with n numbers, there are some numbers you will never draw.

**STOP: Do you understand sample-with-replacement and sample-without-replacement?**
                 
Method #1: Non-parametric bootstrap
-----------------------------------

The basic idea behind bootstrap sampling is that even if we don’t know what the distribution is that underlies the data, we can “pull ourselves up by our bootstraps” and generate the distribution by resampling WITH REPLACEMENT from the data itself.

Say we have original data drawn from an unknown distribution G

$X=\{X_{1},X_{2},X_{3},...,X_{n}\}$

$$
X \sim G()
$$

We don’t know the underlying distribution, but we can substitute the empirical distribution $\hat{G}$ which is defined by $\{X_{1},X_{2},X_{3},...,X_{n}\}$. In other words, we model the underlying "true" unknown distribution as a multinomial where every value in X is given a probability $\frac{1}{n}$ of occurring.

Let’s say we want to compute a statistic of the probability distribution $\theta=f(G)$, which could be the mean or the median or the standard error of the standard deviation (anything at all!!). 

BTW: $\theta$ is analogous to the test statistic T used for hypothesis testing, and it will be used in the same way. However, I will use the symbol $\theta$ to be consistent with the Efron and Tibshirani and other literature on the bootstrap.

The "plug-in" principle states that for every parameter of the underlying distribution, we can estimate that function by simply plugging in the empirical distribution

$$
\hat{\theta}=f(\hat{G})=f(X)
$$

This is exactly what we would do intuitively. If we have a bunch of numbers and we want to know the mean of the distribution from whence they came, we would use as the best estimate the mean of those numbers. The "plug-in" principle simply formalizes the idea that these summary statistics can be used to make inference about the generating distribution.

In the development to follow, we will assume that we have NO other information about a distribution other than a single sample from that distribution.

Summary statistics are easy enough to compute, but we don’t have any way of knowing how accurate those summary statistics might be. The bootstrap gives us a way to calculate the accuracy of our summary statistics $\hat{\theta}$.

The bootstrap works NO MATTER HOW COMPLICATED THE FUNCTION, IT IS COMPLETELY AUTOMATIC, AND REQUIRES NO THEORETICAL CALCULATIONS.

First we need the idea of a bootstrap sample. A bootstrap sample is any sample drawn randomly WITH REPLACEMENT from the empirical distribution.

BOOTSTRAP = SAMPLE WITH REPLACEMENT

$X^{*}=\{\mbox{n values drawn with replacement from } X\}$

n = size of the bootstrap sample = size of the original dataset

We draw k such bootstrap samples:

$$
X_{1}^{*}=\{\mbox{n values drawn with replacement from } X\}
$$
$$
X_{2}^{*}=\{\mbox{n values drawn with replacement from } X\}
$$

etc.

$$
X_{k}^{*}=\{\mbox{n values drawn with replacement from } X\}
$$

**Important**: Because we are sampling WITH REPLACEMENT, some of the original values will be represented more than once in any given bootstrap sample and others not at all. 

We calculate our statistic of interest on each bootstrap sample:

$$
\theta_{1}^{*}=f(X_{1}^{*})
$$
$$
\theta_{2}^{*}=f(X_{2}^{*})
$$

etc.

$$
\theta_{k}^{*}=f(X_{k}^{*})
$$

We will number the different bootstrap sample statistics as

$$
\theta_{1}^{*},θ_{2}^{*},θ_{3}^{*},...,θ_{k}^{*}
$$

k = number of bootstrap samples, you can choose the number of bootstrap samples, more sample = better estimates

Now that we have our collection of k bootstrapped estimates of the statistic, what do we do with them?

Remember: The goal was to calculate the bias and standard error of our estimator.

$$
\widehat{Bias_{boot}}=\left( \frac{1}{k}\sum_{i=1}^{k}\theta_{i}^{*}\right)-\hat{\theta}
$$

In other words, the <a id="bias"></a>Bias of our estimator is simply the mean of the bootstrapped sample statistics minus the statistic as calculated for the original data. (For unbiased estimators, our estimate of bias goes to zero as the sample size n gets very large.)

We can also use these bootstrapped statistics to calculate the standard error of the estimator:

$$
\widehat{se_{boot}}=\sqrt{\frac{1}{k-1}\sum_{i=1}^{k}(\theta_{i}^{*}-\bar{\theta^{*}})^{2}}
$$

*This is just the standard deviation of the distribution of $\theta$.* This is a really important point that is worth dwelling on for a bit. Our uncertainty about the value is captured by how much variation there is when I draw a different sub-sample of the data, which mimics re-doing the experiment altogether. In this case, I call the standard deviation of those $\theta^{*}$ values a standard error, because they represent my uncertainty (my potential error) about $\hat{\theta}$. Do not confuse standard deviation and standard error. A standard deviation is a statistic (something calculated from data) about the spread of the data. <a id="se"></a>A standard error is the standard deviation of my estimates, and therefore is a measure of how uncertain I am about my estimate.   

We will work through 2 examples, one using hand calculations, and one using R pseudocode.

**Example 1**:

*Data*: 10 pennies that the students have

*Test statistic*: Median

Lets say we are trying to find the median age of all pennies in circulation. We can't figure this out exactly, because we can't collect all the pennies in circulation, but we each have a sample of 10 pennies. The median age of the pennies in our sample is a reasonable estimate for the median age of all pennies in circulation. 

What is our uncertainty about that number? How far might our estimate of the median age be from the true median age? In this case, we don't know the underlying distribution of penny ages. (Let's brainstorm this for a bit. Do we have any guesses what this distribution might look like? What might be a reasonable distribution to describe the shape of penny age?) 

Let’s use bootstrapped samples to calculate the s.e. associated with that estimate.

Procedure: 
1. Sample WITH REPLACEMENT a group of 10 pennies. (To sample with replacement you will have to sample one penny, write down the age, and repeat that 10 times.)
2. Calculate the median age from that sample of pennies.
3. Repeat

Over time you will gather a collection of median estimates, each one of which was calculated using a different bootstrapped dataset. They can be used to calculate the Bias and the Variance of the estimator.

We actually have two primary mechanisms for generating confidence intervals for the statistic.

Method #1: We can use the following normal approximation:

$$
\hat{\theta^{*}} \sim N(\hat{\theta},\hat{se}^{2})
$$

**STOP**: How do we construct a confidence interval from this?

$$
\hat{\theta}_{LL}=\hat{\theta}-1.96*\hat{se}
$$
$$
\hat{\theta}_{UL}=\hat{\theta}+1.96*\hat{se}
$$

Remember that 95$\%$ of the probability for a Standard Normal distribution lies between (-1.96$\sigma$,+1.96$\sigma$)? Here we are using the same principle, capturing 95$\%$ of the probability of the distribution by assuming the distribution of $\theta^{*}$ is Normal and pulling out the lower limit and the upper limit lying 1.95 times the standard deviation below and above (respectively) the estimate.

**OR** 

Method #2: we could do away with normal approximations altogether and simply take the quantiles directly from the distribution of $\hat{\theta}^{*}$:

$$
\theta_{LL} = \mbox{2.5th percentile of } \hat{\theta}^{*}
$$
$$
\theta_{UL} = \mbox{97.5th percentile of } \hat{\theta}^{*}
$$

Notice that (by construction) 95$%$ of the $\hat{\theta}^{*}$ values fall in the interval $(\theta_{LL},\theta_{UL})$.

NB: If you are going to go through the trouble of doing the bootstrap sampling, I don’t know why you would make a normal approximation at the very end to construct the CIs. I recommend Method #2.


**Example 2**:

Knowing how to draw bootstrap replicates gets more complicated when you have multivariate datasets.
For example, lets start with a dataset comparing average LSAT scores and GPA for the incoming classes for 15 law schools 

```{r echo=FALSE, fig.align='center', fig.cap='Source: Efron and Tibshirani (1994)', out.width='40%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/LSAT_info.png')
```


Lets say we want to estimate the true correlation coefficient between LSAT scores and GPA. We haven’t covered this yet, but one estimator for the true correlation coefficient is Pearson’s product moment correlation coefficient r

$$
r=\frac{cov(a,b)}{\sqrt{var(a)×var(b)}}
$$

Therefore, in this case

$$
\hat{r} = \frac{cov(LSAT,GPA)}{\sqrt{var(LSAT)*var(GPA)}}
$$

(In R, we would write this as r.est = cor.test(LSAT,GPA)$estimate.)

If LSAT and GPA both come from a normal distribution, then we could use the theory of normal distributions to calculate the standard error of $\hat{r}$. (We will learn this in Week 9.) But, we know LSAT and GPA can’t be from  normal distributions. At the very least, GPA is bounded on (0,4), so it cannot be Normally distributed. So, how do we calculate the standard error of $\hat{r}$?

Here we sample with replacement from the bivariate PAIRS of data. In other words, we sample

$$
X_{1}^{*}=(LSAT_{i},GPA_{i}), \mbox{where i=sample with replacement 1...n}
$$
$$
X_{2}^{*}=(LSAT_{i},GPA_{i}), \mbox{where i=sample with replacement 1...n}
$$

and so forth, and then calculate the correlation of each simulated dataset.

**Question**: Why not sample with replacement from the two datasets independently? What question would that be answering?

If we do this many times, say k=10,000 times, then we can draw a histogram of these bootstrapped correlation coefficients.

 We can calculate the standard error of our estimate for the correlation coefficient 

$$
\hat{se}_{boot} = \sqrt{\left(\frac{1}{k-1}\right)\sum_{i=1}^{k}(r_{i}^{*}-\bar{r^{*}})^{2}}
$$


Therefore, using R, we would calculate the parametric correlation coefficient as: r.est ± 1.96*s.e.boot (VERSION 1)

Even better, we can calculate the 95th percentile confidence interval of this distribution: quantile(all.cor,c(0.025,0.975)) (VERSION 2)

Note that while VERSION 1 is common, VERSION 2 is preferred because there is no guarantee that the distribution of bootstrap statistics is even vaguely Normal.

Bootstrapping can deal with even more complex cases, and is particularly useful when dealing with spatial or temporal autocorrelation. Take for instance a time series of hormone levels:

```{r echo=FALSE, fig.align='center', fig.cap='Source: Efron and Tibshirani (1994)', out.width='40%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Hormone_timeseries.png')
```

If you wanted to do some time series analysis of this data, say to calculate the correlation between each datapoint and the last datapoint, you would have a difficult time doing so because of the complex temporal autocorrelation. Bootstrap can help in this case, but its not at all obvious how to bootstrap from this time series and preserve the essential temporal autocorrelation structure of the data. One approach would be to do a “moving blocks” bootstrap. 

```{r echo=FALSE, fig.align='center', fig.cap='Source: Efron and Tibshirani (1994)', out.width='40%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Moving_blocks.png')
```

This is more advanced, but it makes the point that a) bootstrap can be enormously useful in a variety of complicated analyses and b) you have to think carefully about what to sample in order to preserve the essential element of the data.

R has numerous functions for doing bootstrapping, although bootstrapping is so easy its often just as easy (and more transparent) to simply write your own code to do it. We will go over some examples in lab.

Note that the procedure we have described is called the non-parametric bootstrap estimate because it is based only on the non-parametric empirical distribution G ̂. If we had assumed some kind of distributional form for G, it would be considered a parametric bootstrap. 

Parametric bootstrap
---------------------

The parametric bootstrap is similar to the non-parametric bootstrap except that instead of drawing our bootstrap samples from the original data, we fit a distribution to the data first, and then draw our samples from that. 
We haven’t covered how to fit a distribution to data yet, nor have we introduced any of the univariate distributions, so I won’t show you how to do a parametric bootstrap now but we’ll get some practice in the Week 3 problem set.

Why would we ever do a parametric bootstrap? We might use a parametric distribution if our original sample size was so small that we did not think it could "stand in" for the underlying parametric distribution. For example, if your dataset for coin age just so happens not to have any coins made in 1990, you may be uncomfortable having all your bootstrapped datasets also be missing coins made in 1990. (Remember: Bootstrapping is, in some way, supposed to mimic redoing your experiment. Do you really think that you'd never get a coin made in 1990?) To get around this problem, you might do a parametric bootstrap. Note that, if you use MLEs to get the parameters for the parametric bootstrap, those parameter estimates assume large sample sizes (the formula are asymptotically correct for large sample sizes) and so you have to be a little caution that your parametric bootstrap might not be capturing the true underlying distribution. While parametric bootstrap is often done when sample sizes are too small, occasionally it may also be used when you have some strong theoretical justification for a particular distribution but the statistics you are interested in have no simple formula. (In other words, maybe the distribution is known, but the statistical properties of the specific parameter you are interested in is not known but could be derived through parametric bootstrapping.)

Jackknife
---------------

<a id="jackknife"></a>Jackknifing is another method of assessing bias and standard error of sample statistics. Jackknife can also be used to establish the influence of each datapoint in your dataset. The procedure simply involves leaving out each datapoint and recalculating the statistic of interest. 

If your dataset involves the set 

$$
\{x_{1},x_{2},x_{3}\}
$$

then the jackknife samples are

$$
\{x_{1},x_{2}\},\{x_{1},x_{3}\},\{x_{2},x_{3}\}
$$

The traditional notation is that the estimate based on the dataset when the ith element is removed is ($\widehat{\theta_{(i)}}$).

The jackknife estimate of bias is given by

$$
\widehat{Bias_{jack}}=(n-1)(\hat{\theta_{(.)}}-\hat{\theta})
$$
where

$$
\hat{\theta}_{(.)}=\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(i)}
$$

You can convince yourself of this formula by working out the case where $\hat{\theta}$ is the mean. You can also see intuitively why you would have to multiply the jackknife estimate of bias by (n-1) since the deviation of the jackknifed samples from the full sample is much smaller than the standard deviation of the bootstrapped samples. (**DOES EVERYONE SEE WHY?**)

The jackknife estimate of standard error is given by

$$
\hat{se}_{jack}=\sqrt{\frac{n-1}{n}\sum_{i=1}^{n}(\hat{\theta}_{(i)}-\hat{\theta}_{(.)})^{2}}
$$

With the pennies example, we had 10 pennies and we have only 10 possible jackknifed samples. Do you see why? Note that while bootstrapping can involve simulating an arbitrarily large number of pseudosamples (k), there are only n possible jackknife replicates for a dataset of size n.

**Exercise**: Use your pennies to calculate $\widehat{Bias}_{jack}$ and $\widehat{se}_{jack}$.

Both bootstrap and jackknife can estimate the standard error of a statistic, and in this way, their use can often be interchangeable. However, the jackknife can ONLY compute the bias and standard error whereas the bootstrap calculates the entire distribution of the statistic from which the standard error can be inferred. Bootstrapping is often more computer intensive, but with modern computers this is hardly a drawback.

Jackknife-after-bootstrap 
---------------------------

Jackknife-after-bootstrap is one method of assessing the standard error of bootstrap statistics.
For example, jackknife-after-bootstrap can give us the standard error of the bootstrap standard error: 

$$
\widehat{se}_{jack}(\widehat{se}_{boot})
$$

To do this there are two steps:

1. Leave out data point i and use the remaining data in a bootstrap analysis to calculate (s.e.) ̂_(boot(i))
2. Define 

$$
\widehat{se}_{jack}(\widehat{se}_{boot})=\sqrt{\left(\frac{n-1}{n}\right)\sum_{i=1}^{n}(\hat{se}_{boot(i)}-\hat{se}_{boot(.)})^{2}}
$$

Notice that because there are always some bootstrap samples that do not include i, you do not actually have to do any extra computation to do jackknife-after-bootstrap, but the precise details of using the bootstrap samples you already have are a bit complicated. 

In R, this can be done using the ‘jack.after.boot’ function in the package “boot”.

**Discuss: Why are hypothesis testing and parameter estimation two sides of the same coin?**


Week 3 Lecture
========================================================

Overview of probability distributions
-------------------------------

We will cover 7 distributions this week, and several more next week. Please refer to the handout if univariate distributions, and note that the arrows between Normal and Standard Normal need to be reversed.

For each distribution, there are five things I want you to know.

1 - Probability density function

2 - General Shape

3 - The expected value $E[X]$

4 - The variance $E[X-E[X]]^2$

5 - Relationship to other distributions 


What is a probability density?

if $f(x \mid params)$ is a probability density function:

\ 

$$ P(a<x<b)=\int_a^bf(x \mid params)dx $$

- Probablity at a single point is always zero but probablity density is not.

- The probability density function is not restricted to being $\le1$

- The integral over it's range is always 1.


Normal (Gaussian) Distribution
---------------------------

- The outcome is produced by small number effects acting **additively** and **independently**.

- Normally distributed errors is the most common assumption of linear models.

- Central Limit theorem!


The probability density function of the Normal distribution is given by

$$ f(x \mid \mu, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} $$
$$ x \in \mathbb{R} \\ \mu \in \mathbb{R} \\ \sigma > 0 $$


The shape of the Normal distribution can be illustrated by a few examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dnorm(seq(-5,5,0.1), 0, 0.5)
y2 <- dnorm(seq(-5,5,0.1), 0, 1)
y3 <- dnorm(seq(-5,5,0.1), 0, 2)
y4 <- dnorm(seq(-5,5,0.1), -2, 0.5)

n.data <- data.frame(y = c(y1,y2,y3,y4),
                     x = rep(seq(-5,5,0.1), times = 4),
                     type = rep(c("mu = 0, sigma = 0.5",
                                  "mu = 0, sigma = 1",
                                  "mu = 0, sigma = 2",
                                  "mu = -2, sigma = 0.5"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | mu, sigma)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))
```

```{r, echo = FALSE}
knitr::include_app('https://hlynch.shinyapps.io/normaldistribution/', height = '1000px')
```

The expected value of the Normal distribution is given by

$$ 
\begin{align} E[X] &= \int_{-\infty}^{\infty}{X \cdot f(X)dX} \\
&= \int_{-\infty}^{\infty} x \cdot f(x \mid \mu, \sigma) = \int_{-\infty}^{\infty}\frac{x}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \\ &= \mu \end{align}
$$


The variance of the Normal distribution is given by
 
$$ 
\begin{align} Var[X] &= E[(X- E[X])^2] \\ 
&= E[(X - \mu)^2] \\ 
&= E[X^2] - \mu^2 \\ 
&= \left( \int_{-\infty}^{\infty} x^2 \cdot f(x \mid \mu, \sigma) = \int_{-\infty}^{\infty}\frac{x^{2}}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx \right) - \mu^2 \\ 
&= \sigma^2 \end{align} 
$$


Standard Normal Distribution
-------------------------------

- Raw data rarely fits standard normal.

- Mostly useful as a theoretical construct in hypothesis testing.


The probability density function of the Standard Normal distribution is given by

$$ Z = \frac{X-\mu}{\sigma} $$

$$ f(z \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2} $$
While the letter used to represent a random variable is usually arbitrary (X usually, maybe Y), we use Z (or its specific incarnation z) to represent a random variable drawn from a Standard Normal distribution.

The expected value and variance of the Standard Normal distribution are given by

- E[X] = 0

- Var[X] = 1

Note that the Standard Normal distribution is a linear transformation of the Normal distribution (centered on zero with variance equal to 1).

Log-Normal Distribution
---------------------------

- The outcome is produced by small number effects acting **multiplicatively** and **independently**.

- Often used for things where small grows slowly and big grows quickly, such as forest fires or insurence claims.


The probability density function of the Log-Normal distribution is given by

$$ \begin{align} log(X) &\sim N(\mu,\sigma) \\ X &\sim LN(\mu,\sigma) \end{align} $$

$$ f(x \mid \mu, \sigma) = \frac{1}{x\sqrt{2 \pi \sigma^2}}e^{-\frac{(log(x)-\mu)^2}{2\sigma^2}} \\ x \in \{0,\infty\} \\
\mu \in \mathbb{R} \\ 
\sigma > 0 $$


The shape of the Log-Normal distribution can be illustrated with a few examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dlnorm(seq(0,5,0.01), 0, 0.5)
y2 <- dlnorm(seq(0,5,0.01), 0, 1)
y3 <- dlnorm(seq(0,5,0.01), 0, 2)

n.data <- data.frame(y = c(y1,y2,y3),
                     x = rep(seq(0,5,0.01), times = 3),
                     type = rep(c("mu = 0, sigma = 0.5",
                                  "mu = 0, sigma = 1",
                                  "mu = 0, sigma = 2"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | mu, sigma)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))

```


The expected value and variance of the Log-Normal distribution is given by

- $\mu$ is no longer the mean!

$$ E[X] = e^{\mu + \frac{\sigma^2}{2}} $$

- $\sigma$ is no longer the variance!

$$ Var[X] = e^{2(\mu + \sigma^2) - (2\mu + \sigma^2)} $$

Note that if the log of a variable (X) has a Normal distribution

$$
log(X) \sim N(\mu,\sigma^{2})
$$
than the variable X follows a Log-Normal distribution.

**NB: Be careful when using the Log-Normal distribution. In particular, keep in mind that the sum of Log-normally distributed variables is NOT Log-Normally distributed.**

Intermission: Central Limit Theorem
--------------------------------

$$ X_1,X_2,X_3,...,X_k \sim N(\mu,\sigma^2) \\ 
S_n = \frac{1}{n} (X_1 + X_2 + X_3,...,X_n) \\
\lim_{n \to \infty} S_n \to N(\mu,\frac{\sigma^2}{n}) $$

- X is i.i.d

- X can be drawn from any distribution (with some very limited exceptions; distribution has to have finite moments)!


Poisson Distribution
--------------------

The Poisson distribution arises principally in 3 situations:

1 - In the description of random spatial point patterns (disease events, complete spatial randomness)

2 - As the frequency distribution of rare but independent events

3 - As the error distribution in linear models of count data


The probability mass function of the Poisson distribution is given by

$$ P(x \mid \lambda)= \frac{e^{-\lambda} \cdot \lambda^x}{x!} \\
\lambda>0 \\
x \in \mathbb{N} \cup \{0\} $$

Note that when variables are discrete (i.e. when the distribution only produces integer numbers), we call the probability density function a probability *mass* function. The PDF and PMF play the same role in both cases.

The shape of the Poisson distribution is illustrated by a few examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dpois(seq(0,20,1), 1)
y2 <- dpois(seq(0,20,1), 5)
y3 <- dpois(seq(0,20,1), 10)

n.data <- data.frame(y = c(y1,y2,y3),
                     x = rep(seq(0,20,1), times = 3),
                     type = rep(c("Lambda = 1",
                                  "Lambda = 5",
                                  "Lambda = 10"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  geom_point(aes(color = type)) +
  labs(y = "P (x | Lambda)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))

```


The expected value and variance of the Poisson distribution is given by

$$ \begin{align} E[X] &= \sum_{x=1}^{\infty} x \frac{e^{-\lambda} \cdot \lambda^x}{x!} \\
&= \lambda \cdot e^{-\lambda} \cdot \sum_{x=1}^{\infty} x \frac{\lambda^{x-1}}{x!} \\
&= \lambda \cdot e^{-\lambda} \cdot \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!}\\ 
&\mbox{define } y = x-1 \\
&= \lambda \cdot e^{-\lambda} \cdot \sum_{y=0}^{\infty} \frac{\lambda^{y}}{y!} \mbox{ (the sum is now the expansion of the exponential)}\\
&= \lambda \cdot e^{-\lambda} \cdot e^{\lambda} \\
&= \lambda\end{align} $$

$$ Var[X] = \lambda $$


The Poisson distribution has the following relationship to the Normal distribution:

$$ \lim_{\lambda \to \infty} Pois(\lambda) \to N(\lambda, \lambda)  $$

The above limit can be understood in terms of 'moment matching'. What are moments? Think of moments as being like characteristics of the distribution, but in decreasing order of importance. The first moment in the mean of the distribution (or, equivalently, the expected value E[X]). The second moment is the variance of the distribution, or Var[X]. The third moment is the skew of a distribution (assemmetry of left and right tails), the fourth momnent is the kurtosis (fatness of the tails), etc.  Consider this, if I wanted to construct a Normal distribution that "looked" like a Poisson, I would want a Normal distribution that had the same moments as possible to the Poisson, so at the very least I would want that Normal distribution to have the same mean and variance. This is easy done with the Normal distribution because the mean and variance are two parameters that can be controlled independently, so we can simply set these memonts to match one another (just like the Poisson) and now we have a Normal distribution that has the key characteristic of the Poisson which is E[X]=Var[X]. Since we use the Greek letter $\lambda$ to represent this parameter for the Poisson, we can use that same letter in the Normal to emphasize how these two distributions relate as $\lambda \to \infty$. Note also that as $\lambda \to \infty$, the values of the draws from Pois($\lambda$) get very large and so the discreteness of the values approximates a continuous distribution like the Normal.

Note that the sum of Poisson distributed variables is itself Poisson distributed.

$$ \begin{align} X &\sim Pois(1) \\ Y &= \sum_{i=1}^{\lambda} X_i \\
Y &\sim Pois(\lambda) \end{align} $$

Applying the Central Limit Theorem

$$\begin{align} \bar{X} = \frac{Y}{\lambda} &\sim N(1,\frac{1}{\lambda}) \\
Y &\sim N(\lambda, \lambda) \end{align} $$

Binomial Distribution
-------------------------

The probability mass function of the Binomial distribution is given by

$$
P(x \mid p,n) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\
n \in \mathbb{N} \cup \{0\} \\ 
x \in \{1,2,3,...,n\} \\
p \in [0,1] 
$$


The shape of the Binomial distribution is illustrated by the following examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dbinom(0:10, 10, 0.8)
y2 <- dbinom(0:10, 10, 0.4)
y3 <- dbinom(0:10, 10, 0.1)
y4 <- dbinom(0:10, 5, 0.4)

n.data <- data.frame(y = c(y1,y2,y3,y4),
                     x = rep(0:10, times = 4),
                     type = rep(c("p = 0.8, n = 10",
                                  "p = 0.4, n = 10",
                                  "p = 0.1, n = 10",
                                  "p = 0.4, n = 5"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  geom_point(aes(color = type)) +
  labs(y = "P (x | n, p)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.5, 0.8)) +
  scale_x_continuous(breaks=0:10)
```


The expected value and variance of the Binomial distribution is given by

$$
\begin{align} E[X] &= \sum_{x=1}^n x \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} \\ 
&=np \\
Var[X] &= np(1-p) \end{align} 
$$


The Binomial distribution has the following relationship to the Normal distribution

$$ \lim_{n \to \infty} Binom(n,p) \to N(np,np(1-p)) $$

Beta Distribution
---------------------

- One of the few distributions that is restricted to a finite interval (0 and 1).

- Can be used to model proportions.

The probability density function of the Beta distribution is given by

$$ f(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1} \\ 
 \alpha>0 \\ 
 \beta>0 \\
 x \in (0,1) $$

Gamma Function: if n is a positive integer $\Gamma(n)=(n-1)!$ 

The shape of the Beta distribution is illustrated by the following examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dbeta(seq(0,1,0.01), 0.5, 0.5)
y2 <- dbeta(seq(0,1,0.01), 2, 0.5)
y3 <- dbeta(seq(0,1,0.01), 0.5, 2)
y4 <- dbeta(seq(0,1,0.01), 2, 2)

n.data <- data.frame(y = c(y1,y2,y3,y4),
                     x = rep(seq(0,1,0.01), times = 4),
                     type = rep(c("alpha = 0.5, beta = 0.5",
                                  "alpha = 2, beta = 0.5",
                                  "alpha = 0.5, beta = 2",
                                  "alpha = 2, beta = 2"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | alpha, beta)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.5, 0.8))
```

```{r, echo = FALSE}
knitr::include_app('https://hlynch.shinyapps.io/betadistribution/', height = '1000px')
```

The expected value and variance of the Beta distribution is given by

$$ \begin{align} E[X] &= \int_0^1x\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) + \Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}dx \\
&= \frac{\alpha}{\alpha + \beta}  \end{align} $$

$$ Var[X] = \frac{\alpha\beta}{(\alpha+\beta)^2 \cdot(\alpha + \beta + 1)} $$


The Beta Distribution's relationship to the Normal and Uniform distributions are given by

$Beta(1,1)$ is the same as $Uniform(0,1)$

$$ f(x \mid 1,1) = \frac{\Gamma(2)}{\Gamma(1) + \Gamma(1)}x^{0}(1-x)^{0} = 1 $$
In the limit that $\alpha$ and $\beta$ are the same and growing large, the Beta distribution has the following relationship to the Normal distribution.

$$ \lim_{\alpha=\beta \to \infty} \to N(\frac{1}{2}, \frac{1}{8\alpha + 4}) $$

Gamma Distribution
-----------------------

- Useful for variables that have a positive skew. 

- it is often used to model “waiting times”, such as the time before a device or machine fails.

The probability density function of the Gamma distribution (not to be confused with the Gamma Function) is given by

$$ f(x \mid \alpha, \beta) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{(\alpha-1)} e^\frac{-x}{\beta} \\
\alpha>0 \\
\beta >0 \\
x>0$$

The shape of the Gamma distribution is illustrated by the following examples

```{r, echo=FALSE}

library(ggplot2)

y1 <- dgamma(seq(0,20,0.1), 1, scale = 2)
y2 <- dgamma(seq(0,20,0.1), 2, scale = 2)
y3 <- dgamma(seq(0,20,0.1), 9, scale = 0.5)
y4 <- dgamma(seq(0,20,0.1), 7.5, scale = 1)

n.data <- data.frame(y = c(y1,y2,y3,y4),
                     x = rep(seq(0,20,0.1), times = 4),
                     type = rep(c("alpha = 1, beta = 2",
                                  "alpha = 2, beta = 2",
                                  "alpha = 9, beta = 0.5",
                                  "alpha = 7.5, beta = 1"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | alpha, beta)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.5, 0.8))
```


The expected value and variance of the Gamma distribution is given by

$$ \begin{align} E[X] &= \int_0^\infty x\frac{1}{\beta^\alpha \Gamma(\alpha)} x^{(\alpha-1)} e^\frac{-x}{\beta} \\
&= \alpha\beta \end{align} $$


$$ Var[X] = \alpha\beta^2 $$


The relationship between the Gamma distribution and the Normal distribution is

$$ \lim_{\alpha \to \infty} Gamma(\alpha,\beta) \to N(\alpha \beta,\alpha\beta^2) $$


Some additional notes:
-------------------------

Please skim through the reading that has been posted "The Algebra of Expectations". Focus on the "rules".

I want to make sure everyone is clear on the distribution of Normal random variables.

If 

$$
X\sim N(\mu,\sigma^{2})
$$

then the distribution of a new variable c*X (where c is a constant) is given by

$$
cX\sim N(c\mu,c^{2}\sigma^{2})
$$

The mean is simply multiplied by $c$, and the variance is multiplied by $c^{2}$.

To understand why this is, let's first go into a more detailed derivation of 
the variance of the distribution of Normal random variables than we did in class.
To begin with, you will need to know that one property of expectations is the following:

$$
E[ g(x) ] = \int g(x) f(x) \ dx
$$

With this knowledge, let's approach the derivation of the variance, where 
we are essentially defining $g(x) = (X-E[X])^2$ and using some basic properties
of algebra involving integrals.  Note that in this derivation we are treating
$E[X]$ as a constant, and **not** as a function of $x$.

$$
\begin{eqnarray}
Var[X]&=&E[(X-E[X])^{2}] \\
&=&\int (x - E(X))^2 f(x) \ dx \\
&=&\int (x^2 -2xE(X) + E(X)^2 ) f(x) \ dx \\
&=&\int x^2 f(x) \ dx - 2E(X) \int x f(x) \ dx + \int E(X)^2 f(x) \ dx \\
&=& E(X^2) -2 E(X)^2 + E(X)^2 \\
&=& E(X^2) - E(X)^2 
\end{eqnarray}
$$

Week 4 Lecture
========================================================

Short digression: Degrees of freedom
------------

What is meant by “degrees of freedom”?

Draw five boxes on the board:

We want the mean to be 4 so the sum has to be 20. Let’s start filling in boxes...

Do we get to choose the last number? No! The last number is constrained by the fact that we already fixed the mean to be 4.

So we only have 4 degrees of freedom, the last box is prescribed by the mean. So if we know the mean of a sample of size n, we only have n-1 remaining degrees of freedom in the sample.

degrees of freedom (dof)=sample size (n)-number of parameters already estimated from the data (p)

If we go back for a second, it turns out that the real definition of the sample variance is

$$
\mbox{variance} =  \frac{\mbox{sum of squares}}{\mbox{degree of freedom}}
$$

What is the sum of squares?

$$
SS = \sum(Y-\bar{Y})^2
$$
How many degrees of freedom did I start with? (n)

How many did I lose in the calculation of the SS? (1, for $\bar{Y}$)

Therefore, an unbiased estimate of the population variance is:

$$
SS = \frac{\sum(Y-\bar{Y})^{2}}{n-1}
$$

Now that we have that out of the way, we can get back to univariate distributions. The three distributions we will learn about today all share one thing, which is that their parameters represent some kind of a number. The most difficult part of these distributions is keeping separate in your mind the number associated with the parameter and the number associated with the number of samples drawn from the distribution. 

t-distribution
-------------

Let $X_{1},X_{2},...,X_{n}$ be independently and identically distributed random variables with mean $\mu$ and finite, non-zero variance $\sigma^{2}$, and the average of these variable $\bar{X}$ be defined as

$$
\bar{X} = \frac{1}{n}(X_{1}+X_{2}+X_{3}+...+X_{n})
$$

Then the Central Limit Theorem states:

$$
lim_{n \rightarrow \infty} \bar{X} \rightarrow N\left(\mu,\frac{\sigma^{2}}{n}\right)
$$
and therefore that

$$
lim_{n \rightarrow \infty} \frac{\bar{X}-\mu}{\sqrt{\sigma^{2}/n}} \rightarrow N(0,1)
$$

(Note that I could use $s^{2}$ synonymously with $\sigma^{2}$ here because $s^{2}$ is an unbiased estimator of $\sigma^{2}$, which means that $lim_{n \rightarrow \infty} s^{2}\rightarrow \sigma^{2}$.)

**What is the distribution of $\bar{X}$ for finite n when we don’t know what the population variance $\sigma^{2}$ is and have to substitute $s^{2}$ instead? The t-distribution.**

The t-distribution has one parameter, n-1, where n=the number of degrees of freedom.

(Just like $\mu$ and $\sigma^{2}$ are the parameters of the Normal, the parameter of the t is called the “degrees of freedom”. Do not let the name confuse you. Also, keep in mind that there is no relationship between the *number* of draws from the t distribution and the parameter for the t distribution. You can have n=100 draws from a $t_{n-1=10}$ or n=10 draws from a $t_{n-1=100}$. The two "n"s here are different and independent of one another.)

Notice that as n gets large, both the numerator and the denominator get small. The ratio of these two small numbers is the t-distribution, which is symmetric about 0 because the sample mean could end up either slightly too low or slightly too high.

**Where did we lose the degree of freedom?** 

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
We lost one degree of freedom because we had to use the sample mean in the estimation of the sample variance.
</span>
</details> 

Sometimes $s^{2}$ is smaller than $\sigma^{2}$, and sometimes it is larger, which explains why **the t-distribution is flatter in the middle but fatter in the tails**.

```{r, echo=FALSE}
library(ggplot2)

y1 <- dt(seq(-5,5,0.1), 2)
y2 <- dt(seq(-5,5,0.1), 5)
y3 <- dt(seq(-5,5,0.1), 10)

n.data <- data.frame(y = c(y1,y2,y3),
                     x = rep(seq(-5,5,0.1), times = 3),
                     type = rep(c("df=2",
                                  "df=5",
                                  "df=10"), each = length(y1)))

theme_set(theme_bw())
ggplot(n.data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | df)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))
```

For large sample sizes, the t-distribution is indistinguishable from the normal distribution, but for small sample sizes, the t-distribution is flatter in the middle and fatter in the tails. 

The p.d.f. looks like

$$
p.d.f. = f(x|p=n-1=d.o.f.) \sim \frac{\Gamma(\frac{p+1}{2})}{\Gamma(\frac{p}{2})}\frac{1}{(p\pi)^{1/2}}\frac{1}{(1+\frac{x^{2}}{p})^{(p+1)/2}}
$$

Side note: I mentioned that there were some distributions for which I was not requiring you to memorize the pdf. Note that for some of the remaining pdfs (like the t, F, and chi-squared) I may give you the pdf and ask you to identify it. Fair warning!!

The expected value is given by

$$
E[X]=0 \mbox{ if p}>1
$$

The variance is given by

$$
Var[X]=\frac{p}{p-2} \mbox{ if p}>2
$$

Chi-squared distribution
---------------

The chi-squared distribution is related to the standard normal distribution as follows:
Draw

$$
X_{1},X_{2},X_{3},...,X_{n}  \sim N(0,1)
$$

Create a new quantity

$$
Y= X_{1}^{2}+X_{2}^{2}+X_{3}^{2}+...+X_{n}^{2}
$$
$$
Y \sim χ_{n}^{2}
$$

In other words, the sum of squared standard deviates is distributed as a $\chi^{2}$ distribution with n degrees of freedom.

If we construct standard normal by transforming normal distributions with mean $\mu$ and variance $\sigma^{2}$, we can re-write the sum above as

$$
Y = \frac{1}{\sigma^{2}}(X_{1}-\mu)^{2}+\frac{1}{\sigma^{2}}(X_{2}-\mu)^{2}+\frac{1}{\sigma^{2}}(X_{3}-\mu)^{2}+...+\frac{1}{\sigma^{2}}(X_{n}-\mu)^{2}
$$

$$
Y = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}{(X_{i}-\mu)^{2}} \sim \chi_{n}^{2}
$$

where $\mu$ is the population mean and $\sigma^{2}$ is the population variance.

To reiterate, it is important to remember the following properties of the chi-squared distribution:

1. If $X \sim N(0,1)$, then $X^{2} \sim \chi_{1}^{2}$. In other words, the square of a standard normal random variable is a chi-squared random variable.

```{r, echo=TRUE}
chisqhist<-hist((rnorm(1000,0,1))^2,freq=F,breaks=30,main="Chi-squared with df=1")
lines(chisqhist$mids,dchisq(chisqhist$mids,1),col="red",lwd=2)
```

2. If $X_{p1},X_{p2},X_{p3}... \sim \chi^{2}$ distribution, then $\sum_{i}{X_{pi}} \sim \chi^{2}_{\sum{p_{i}}}$. In other words, independent chi-squared random variables sum to a chi-squared random variable, and the degrees of freedom also add.

```{r, echo=TRUE}
df1<-5 
df2<-2
df3<-3
chisqhist2<-hist(rchisq(1000,df1)+rchisq(1000,df2)+rchisq(1000,df3),freq=F,breaks=30,main="Sum of chi-squared distributions")
lines(chisqhist2$mids,dchisq(chisqhist2$mids,df1+df2+df3),col="red",lwd=2)
```

While above I used the symbol "n" to designate the parameter of the $\chi^{2}$ distribution because it is intuitive how that parameter is equal to the number of (squared) random variables are being summed, here I will switch to the letter $\nu$, which is slightly more traditional. The chi-squared distribution has an ugly pdf, which I include here for completeness.

$$ 
f(x \mid \nu=n) = \frac{1}{2^\frac{\nu}{2} \Gamma(\frac{\nu}{2})} x^{(\frac{\nu}{2}-1)} e^\frac{-x}{2}
$$
Remembering that the Gamma distribution is given by

$$
f(x|\alpha,\beta) = \frac{1}{\beta^{\alpha}\Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta}
$$
With this we can see that the $\chi^{2}$ distribution is a special case of the gamma distribution with $\alpha = \nu/2$ and $\beta=2$.

The shape of the $\chi^{2}$ distribution can be seen with a few examples:

```{r, echo=FALSE}
chi1 <- dchisq(seq(0,8,0.1), 1)
chi2 <- dchisq(seq(0,8,0.1), 3)
chi3 <- dchisq(seq(0,8,0.1), 6)
chi4 <- dchisq(seq(0,8,0.1), 9)

n_data <- data.frame(y = c(chi1, chi2, chi3, chi4),
                     x = rep(seq(0,8,0.1), times = 4),
                     type = rep(c("d.o.f = 1", "d.o.f = 3","d.o.f = 6","d.o.f = 9"), 
                                each = length(chi1)))

theme_set(theme_bw())
ggplot(n_data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | nu = n-1)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))
```

The expected value and variance are given by:

$$ E[X] = \nu \\ Var[X] =2\nu  $$

I don’t derive these, but they follow from the E[X] and Var[X] of the gamma distribution.

F distribution
-------------

As you will see in detail throughout the semester, the F distribution is critical to modeling variances. The F distribution is related to the $\chi^{2}$ distribution, in fact the ratio of two chi-squared distributions is the F-distribution.

$$
\frac{\chi^{2}_{n-1}/(n-1)}{\chi^{2}_{m-1}/(m-1)} \sim F_{n-1,m-1}
$$

The F distribution has two parameters: the d.o.f. of both samples being compared (n-1 and m-1).

The pdf of the F distribution is given by

$$
f(x|r=n-1,s=m-1)=\frac{r\Gamma(\frac{1}{2}(r+s))}{s\Gamma(\frac{1}{2}r)\Gamma(\frac{1}{2}s)}\frac{\frac{rx}{s}^{\frac{r}{2}-1}}{(1+\frac{rx}{s})^{\frac{r+s}{2}}}
$$

(noting that I have used r=n-1 and s=m-1 to make the equation slightly more readable) and the shape of the F distribution looks like

```{r, echo=FALSE}
F1 <- df(seq(0,5,0.01), 1, 2)
F2 <- df(seq(0,5,0.01), 2, 1)
F3 <- df(seq(0,5,0.01), 5, 2)
F4 <- df(seq(0,5,0.01), 100, 2)
F5 <- df(seq(0,5,0.01), 100, 100)

n_data <- data.frame(y = c(F1, F2, F3, F4, F5),
                     x = rep(seq(0,5,0.01), times = 5),
                     type = rep(c("d1 = 1, d2 = 2", "d1 = 2, d2 = 1",
                                  "d1 = 5, d2 = 2", "d1 = 100, d2 = 2",
                                  "d1 = 100, d2 = 100"), 
                                each = length(F1)))

theme_set(theme_bw())
ggplot(n_data, aes(x=x, y=y)) + 
  geom_line(aes(color = type)) +
  labs(y = "f (x | n-1,m-1)", x = "x") +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.55))
```

$$ E[X] = \frac{m-1}{m-3} $$

and the variance is just ugly.

Why would we ever need to use the F-distribution? It turns out that the F-distribution is critical for assessing variances. Let’s say you have two datasets...

Data set A, with n data points:

$$
X_{1},X_{2},X_{3},...,X_{n} \sim N(\mu_{A},\sigma_{A}^{2})
$$

and data set B, with m data points:

$$
Y_{1},Y_{2},Y_{3},...,Y_{m} \sim N(\mu_{B},\sigma_{B}^{2})
$$

Now you have two sets of data, both normally distributed but with different means and variances. You want to know whether the variance of data set A is bigger or smaller than the variance for data set B.

We *want* to know the ratio $\sigma_{A}^{2}/\sigma_{B}^{2}$ but all we *have* at our disposal is the ratio of the sample variances $s_{A}^{2}/s_{B}^{2}$. It turns out the F-distribution can help us. 

We start with the fact that you know from our discussion a few minutes ago that

$$ 
\frac{1}{\sigma^2}\sum_{i=1}^n{(X_i-\mu)^2} \sim \chi^2_n
$$

If we have to estimate $\mu$ using $\bar{X}$ than we lose one degree of freedom

$$
\frac{1}{\sigma^2}\sum_{i=1}^n{(X_i-\bar{X})^2} \sim \chi^2_{n-1} 
$$
but we know that

$$
s^2 = \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{n-1}
$$

Therefore

$$
s^2(n-1) = \sum_{i=1}^n(X_i-\bar{X})^2
$$

So we can write

$$
\frac{1}{\sigma^2}\sum_{i=1}^n(X_i-\bar{X})^2 =  \frac{s^2(n-1)}{\sigma^2} \sim \chi^2_{n-1}
$$


Therefore, 

$$ 
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim \frac{\chi^2_{n-1}/(n-1)}{\chi^2_{m-1}/(m-1)} 
$$

is a ratio of scaled chi-squared distributions which is, as you now know, also an F-distribution:

$$ 
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim F_{n-1,m-1}
$$

So, if we know the population variances ($\sigma^2$) and we calculate the sample variances ($s^2$), than the ratio above has the F-distribution. While we don’t usually know the population variances, we are often testing a hypothesis that the two population variances are equal. If we *assume* that $\sigma_{A}^{2}=\sigma_{B}^{2}$, than the expression above becomes

$$ 
\frac{s^2_A}{s^2_B} \sim F_{n-1,m-1}
$$
This fact will be used in a few weeks when we discuss F-tests. (Stay tuned…)

The t-, F-, and chi-squared distributions are all closely related. I will not work through the proofs here, but I will highlight a couple important relationships that will come in handy later.

*Relationship 1*: We’ve already shown that the F distribution is a ratio of scaled chi-squared distributions.

$$ 
\frac{\chi^2_{n-1}/(n-1)}{\chi^2_{m-1}/(m-1)} \sim F_{n-1,m-1}
$$
*Relationship 2*: If $X \sim t_{n-1}$, then $X^2 \sim F_{1,n-1}$. In words, this means that if you square a t-distributed variable, you get an F distributed variable. 

Once you decide on a distribution for your data, you need some way of estimating the best-fit parameters. This brings us to the next major topic of Biometry.

Estimating confidence intervals - 5 special cases
----------------

We will now learn how to fit a model to data, that is, to estimate the parameter values for the distribution being used to model the data. Since parameter estimates are useless without their corresponding confidence intervals, we will also learn out to estimate confidence intervals for these parameters.

Let's review for a second what is meant by a confidence interval. **<span style="color: orangered;">A 95th percentile confidence interval is an interval estimated from the data that we are 95$\%$ certain contains the true but unknown value for the parameter.</span>** Because this interval is estimated from data, it has its own confidence interval. While we usually we don't worry too much about this, keep in mind that **a different dataset would also produce a different parameter estimate and a different confidence interval**.

How do we calculate confidence intervals when we are using a parametric distribution to model some data? We are going to use what we know about these distributions to derive an analytical expression for the distribution of the parameter of interest, and then use the quantiles of that distribution to calculate the appropriate confidence intervals (typically we use the $\alpha⁄2$ and 1-$\alpha⁄2$ confidence intervals, where $\alpha$=0.05).

The following five examples use the same methodology:

*Step #1*: Start with an expression involving the parameter of interest that you know is true based on what we already know about the properties of the univariate distributions

*Step #2*: Algebraically re-arrange that expression to isolate the parameter of interest on one side of the equation. You now have an expression for the statistical distribution describing the estimate of that parameter

*Step #3*: Replace the distribution with the appropriate quantiles to generate the lower bound and upper bound of interest.

The first four examples involve the following model

$$
X \sim N(\mu,\sigma^{2})
$$
If we want to find confidence intervals for $\mu$, we can do so in the case where $\sigma$ is known (unrealistic) or where $\sigma$ is unknown. If we want to find confidence intervals for $\sigma$, we can do so in cases where $\mu$ is known (unrealistic) or where $\mu$ is unknown. These form the first four of the five examples we will cover.

**Example 1: Confidence intervals for $\mu$ assuming $\sigma$ is known.** Let's say that we have some data $X_{1},X_{2},X_{3},....,X_{n}$. To use a concrete example, let's say that these data represent the population growth rate of some bird colonies that I am monitoring. I am going to model growth rate by a Normal distribution:

$$
X \sim N(\mu,\sigma^{2})
$$
Let's assume for the moment that I already know what the variance of growth rate is, so **$\sigma$ is already known and does not have to be estimated from the data**, but I do need to use the data to estimate the mean of the distribution $\mu$. 

Let's start with the model we have for the data

$$
X \sim N(\mu,\sigma^{2})
$$

From this, follows

$$
\bar{X} \sim N(\mu,\sigma^{2}/n)
$$
and

$$
\bar{X}-\mu \sim N(0,\sigma^{2}/n)
$$

and

$$
\bar{X}-\mu \sim \sqrt{\frac{\sigma^{2}}{n}}N(0,1)
$$
We can re-arrange this expression to get the distribution for $\mu$

$$
\mu-\bar{X} \sim \sqrt{\frac{\sigma^{2}}{n}}N(0,1)
$$
(Why can I just reverse the signs on the left hand side?)

$$
\mu \sim \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}N(0,1)
$$

We now have the distribution for the parameter $\mu$ in terms of quantities we already know ($\sigma$, which is assumed known, sample size n, and the average of the data $\bar{X}$).

We can use this expression to get confidence intervals for $\mu$, by plugging in the quantiles of the standard Normal distribution on the right hand side. The lower limit is defined by the [$\alpha$/2] quantile of N(0,1), and the upper limit is defined by 1-[$\alpha$/2] quantile of N(0,1). By tradition, we call the quantiles of the standard normal ''z''.

$$
P(\bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{\alpha/2} \leq \mu \leq \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2}) = 0.95
$$

Because the N(0,1) is symmetric about zero,

$$
z_{\alpha/2}= -z_{1-\alpha/2}
$$

```{r echo=FALSE, fig.align='center', fig.cap='Standard normal diagram illustrating the symmetry of the distribution and the quantiles for the left and right tails.', out.width='75%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/StandardNormalTails.png')
```

this is the same as

$$
P(\bar{X}-\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2} \leq \mu \leq \bar{X}+\sqrt{\frac{\sigma^{2}}{n}}z_{1-\alpha/2}) = 0.95 = 1-\alpha
$$

This second version makes it easier to see that there is a quantity that is subtracted off $\bar{X}$ for the lower limit, and added to $\bar{X}$ for the upper limit. The confidence intervals in this case are symmetric about $\bar{X}$.

Side note: This is called the “equal-tails” method, because we have constructed the confidence interval using equal amounts in each tail of the distribution. It is the most common way of constructing a confidence interval, but not the only way. 

Notice that

$$
z_{1-\alpha/2}=qnorm(0.975)=1.96
$$

from which we arrive at a form for the confidence interval that might look familiar, that is, estimate $\pm$ 2 $\times$ SE (the standard error).

*Note*: We often use this approach for the population mean even if we do not know the exact distribution because the CLT tells us that for large sample sizes, the mean of the distribution is normally distributed. But this approach is quite limited because it doesn’t tell us how to get estimates and CIs for other parameters, nor does it address the problem of estimates when the sample size is small and the CLT doesn’t apply.

**Example #2: Confidence intervals for $\mu$ assuming $\sigma$ is unknown.** What about a (much more common) situation where you need to use the data to estimate both the mean and the variance? In this case we have to use the sample variance $s^{2}$ to estimate the parametric variance $\sigma^{2}$. We start with the following fact

$$
\frac{\bar{X}-\mu}{\sqrt{\frac{s^{2}}{n}}} \sim t_{n-1}
$$

Therefore

$$
\bar{X}-\mu \sim \sqrt{\frac{s^{2}}{n}} t_{n-1}
$$

$$
P(\bar{X}-\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[n-1]} \leq \mu \leq \bar{X}+\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[n-1]}) = 1-\alpha
$$

It is almost never the case that you know $\sigma^{2}$, so you should always use the quantiles of the t-distribution for building confidence intervals for $\bar{X}$. People incorrectly use the normal approximation because back in the day of tables, the normal distribution was easier. *In the age of computers, no excuse – use the t-distribution.*

One final note - next week we will learn about t-tests and depending on the assumptions being made, the appropriate t-distribution may have a degree-of-freedom different from n-1, so I'm including the most general statement about the confidence interval for the mean of a Normally distriuted population here:

$$
P(\bar{X}-\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]} \leq \mu \leq \bar{X}+\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]}) = 1-\alpha
$$

**Example #3: Estimating the confidence intervals for the variance $\sigma^{2}$ assuming $\mu$ is known.**

Based on the definition of a $\chi^{2}$ distribution, we know that if we have data $X \sim N(\mu,\sigma^{2})$, than 

$$
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2} \sim \chi^{2}_{n}
$$

here we will use the fact that if $\mu$ is *known*, than the sample variance is calculated as

$$
\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\mu)^{2} = s^{2}
$$
Notice that because $\mu$ is known and does not have to be estimated from the data, we retain all n degrees of freedom in the denominator.

We can re-arrange this equation to get

$$
\sum_{i=1}^{n}(X_{i}-\mu)^{2} = ns^{2}
$$
which we plug into our $\chi^{2}$ expression to yield

$$
\frac{1}{\sigma^{2}}ns^{2} \sim \chi^{2}_{n}
$$
We then re-arrange a bit further to isolate $\sigma^{2}$

$$
\sigma^{2} \sim \frac{ns^{2}}{\chi^{2}_{n}}
$$

Now that we have the sampling distribution for $\sigma^{2}$ we can simply insert the appropriate quantiles to get the lower and upper limits of the confidence interval. Note that because the $\chi^{2}$ is in the denominator, the *larger* quantile is associated with the lower limit, and vice versa.

$$
P\left(\frac{ns^{2}}{\chi^{2}_{(1-\alpha/2)[n]}} \leq \sigma^{2} \leq \frac{ns^{2}}{\chi^{2}_{(\alpha/2)[n]}}\right) = 1-\alpha
$$

**Example #4: Estimating the confidence intervals for the variance $\sigma^{2}$ assuming $\mu$ is unknown.**

This example proceeds similarly to the one above, except we now need to estimate $\mu$ from the data, and this means we lose of degree of freedom both in the $\chi^{2}$ distribution and in the estimate of the sample variance.

$$
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2} \sim \chi^{2}_{n-1}
$$

Why n-1? Because we lost a degree of freedom when we had to estimate $\bar{X}$.

As before (except with $\mu$ unknown, and estimated by $\bar{X}$)

$$
\frac{1}{(n-1)}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2} = s^{2}
$$

so we can rearrange to show that

$$
\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}=\frac{(n-1)s^{2}}{\sigma^{2}} \sim \chi^{2}_{n-1}
$$
Using the above expression, we re-arrange to get the sampling distribution of $\sigma^{2}$

$$
\sigma^{2} \sim \frac{(n-1)s^{2}}{\chi^{2}_{n-1}}
$$

from which we get the confidence intervals

$$
P(\frac{(n-1)s^{2}}{\chi^{2}_{(1-\alpha/2)[n-1]}} \leq \sigma^{2} \leq \frac{(n-1)s^{2}}{\chi^{2}_{(\alpha/2)[n-1]}}) = 1-\alpha
$$

**Example #5: Estimating the ratio of two variances $\sigma^{2}_{A}/\sigma^{2}_{B}$**

We start with this expression, which we derived above:

$$ 
\frac{s^2_A/\sigma^2_A}{s^2_B/\sigma^2_B} \sim F_{n-1,m-1}
$$
We can rewrite this equation as

$$ 
\frac{s^2_A/s^2_B}{\sigma_{A}^{2}/\sigma^2_B} \sim F_{n-1,m-1}
$$
Now we are going to invert the fraction on the left hand side in order to get the $\sigma$s in the numerator. The right hand side is still an F-distribution, but because we have flipped numerator and denominator, we need to switch the order of the two parameters

$$ 
\frac{\sigma_{A}^{2}/\sigma^2_B}{s^2_A/s^2_B} \sim F_{m-1,n-1}
$$

We isolate the ratio of $\sigma$s in the numerator by multiplying the sample variances across, as follows

$$ 
\frac{\sigma_{A}^{2}}{\sigma^2_B} \sim \frac{s^2_A}{s^2_B} F_{m-1,n-1}
$$

Now, as before, we have the sampling distribution for the quantity we want, and we obtain the confidence intervals by substituting in the appropriate quantiles. 

$$
\frac{s_{A}^{2}}{s_{B}^{2}}F_{(\alpha/2)[m-1,n-1]} \leq \frac{\sigma_{A}^{2}}{\sigma_{B}^{2}} \leq \frac{s_{A}^{2}}{s_{B}^{2}}F_{(1-\alpha/2)[m-1,n-1]}
$$

To recap
-----------

We used the Normal distribution to derive the confidence interval for the parametric mean $\mu$ when $\sigma$ is known. 

We used the t-distribution to derive the confidence interval for the parametric mean $\mu$ when $\sigma$ is unknown (much more common).

We used the $\chi^{2}$ distribution to derive the confidence interval for the parametric variance $\sigma^{2}$ when $\mu$ is known.

We used the $\chi^{2}$ distribution to derive the confidence interval for the parametric variance $\sigma^{2}$ when $\mu$ is unknown (much more common).

We used the F distribution to derive the confidence interval for the ratio of two variances.

Week 4 Lab
=============

On Tuesday we discussed a few ways of getting confidence intervals for parameters under special cases where you have a limiting distribution that allows you to solve for it. Another, much more general, way of obtaining parameter estimates and confidence intervals is to use maximum likelihood.

There are few more important subjects in applied statistics. Maximum likelihood and probability distributions are intimately related, for reasons that will become apparent. To serve as an example, we'll use the Normal Distribution $N(\mu,\sigma^{2})$:

The probability density of the normal distribution is given by

$$
f(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(x-\mu)^{2}}{\sigma^{2}}\right)}
$$

Remember that for variables that are i.i.d., the joint probability $(X_{1},X_{2},X_{3})$ is simply the product of the three p.d.f.s

$$
P(X_{1}\cap X_{2} \cap X_{3})=P(X_{1})\times P(X_{2})\times P(X_{3})
$$

$$
f(X_{1},X_{2},...,X_{n}|\mu, \sigma) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

Taken as a probability density, this equation denotes the probability of getting unknown data ${X_{1},X_{2},...,X_{n}}$ given (|) the known distribution parameters $\mu$ and $\sigma$. However, it can be rewritten as a likelihood simply by reversing the conditionality:

$$
L(\mu,\sigma|X_{1},X_{2},...,X_{n}) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

The likelihood specifies the probability of obtaining the known data ${X_{1},X_{2},...,X_{n}}$ by a certain combination of the unknown parameters $\mu$ and $\sigma$.

pdf: parameters known, data varies            
likelihood: data known, parameters vary

In this way, the relationship between the joint probability density and the likelihood function is a bit like the relationship between the young woman and the old maid in this famous optical illusion:

```{r echo=FALSE, fig.align='center', fig.cap='Optical illusion known as "My Wife and my Mother-in-Law". Source: Wikimedia Commons', out.width='25%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Optical_illusion.png')
```

Parameter estimates may be found by maximum likelihood simply by finding those parameters that make your data most likely (among all possible data sets).

Conceptually, it helps to remember the Week #1 problem set. The likelihood of obtaining your exact set of colors was very small even when using the true underlying probabilities of each color. **<span style="color: orangered;">Likelihoods are always very small numbers - even the maximum likelihood estimates (MLEs) are very unlikely to produce your dataset, simply because there are so many possible datasets that could be produced.</span>** The MLEs are simply those parameters that make your dataset more likely than any other dataset.

The magnitude of the likelihood means **nothing**. The actual value of the likelihood depends on the size of the "sample space" (how many possible datasets could you imagine getting?), so we can only assign meaning to the relative size of likelihoods among different combinations of parameter values. We can say whether one set of parameter values is more likely to be the "true" population values than other possible sets of parameter values.

We will now discuss how to go about finding MLEs.

First we will calculate the MLE for the normal parameters by hand, and then we will use two different methods of calculating the maximum likelihood estimators using R. First, we are going to do it manually.

The likelihood function for X drawn from $N(\mu,\sigma^{2})$ is

$$
L(\mu,\sigma|X_{1},X_{2},...,X_{n})= \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

Because likelihoods are very small, and we are only interested in relative values, we use the log-likelihood values which are easier to work with (for reasons that will become clear)

The log-likelihood (LL) is

$$
LL = \sum_{i}\left(-\frac{1}{2}log(2\pi\sigma^{2})-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)
$$

We want to maximize the LL, which is usually done by minimizing the negative-LL (NLL).To make the algebra easier, I will define $A=\sigma^{2}$.

$$
NLL=\sum_{i}\left(\frac{1}{2}log(2\pi A)+\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{A}\right)
$$

$$
\frac{\partial NLL}{\partial \mu} = \sum_{i}\left(\frac{-(X_{i}-\hat{\mu})}{A}\right)=0
$$

Notice that when I set the left-hand side to 0, the notation changes from $\mu$ to $\hat{\mu}$ because the MLE $\hat{\mu}$ is that value that makes that statement true.

$$
\frac{\partial NLL}{\partial \mu} =\sum_{i}-(X_{i}-\hat{\mu})=0=\Sigma_{i}(X_{i}-\hat{\mu})
$$

$$
n\hat{\mu}-\sum_{i}X_{i}=0
$$

$$
\hat{\mu}=\frac{1}{n}\sum_{i}X_{i}
$$

Now we do the same for $A=\sigma^{2}$

$$
NLL=\sum_{i}\left(\frac{1}{2}log(2\pi A)+\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{A}\right)
$$

$$
\frac{\partial NLL}{\partial A} = \sum_{i}\left(\frac{1}{2}\frac{2\pi}{2\pi\hat{A}}-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\hat{A}^{2}}\right)=0
$$

$$
\sum_{i}\left(1-\frac{(X_{i}-\mu)^{2}}{\hat{A}}\right)=0
$$

$$
n-\frac{1}{\hat{A}}\sum_{i}\left((X_{i}-\mu)^{2}\right)=0
$$

$$
\hat{A}=\hat{\sigma^{2}}=\frac{1}{n}\sum_{i}(X_{i}-\mu)^{2}
$$

The MLEs are not necessarily the best estimates, or even unbised estimates. In fact, the MLE for $\sigma^{2}$ is biased (the unbiased estimator replaces n with n-1).

To do this in R, we have to write a function to define the NLL:

```{r}
neg.ll<-function(x,mu,sigma2)
 {
sum(0.5*log(2*pi*sigma2)+0.5*((x-mu)^2)/sigma2)
}
```

For the purposes of a simple example, lets generate some fake "data" by drawing random samples from a $N(\mu=1,\sigma=2)$.

```{r}
x<-rnorm(1000,mean=1,sd=2)
mu.test.values<-seq(-2,4,0.1)
sigma2.test.values<-seq(1,11,0.1)
```

Next, we will make a matrix to store the values of the likelihood for a grid of potential $\mu$ and $\sigma^{2}$ values.

```{r}
likelihood.matrix<-matrix(nrow=length(mu.test.values),ncol=length(sigma2.test.values))
```

Now we will search parameter space by brute force, calculating the likelihood on a grid of potential $\mu$ and $\sigma^{2}$ values.

```{r}
for (i in 1:length(mu.test.values))
 {
  for (j in 1:length(sigma2.test.values))
   {
    likelihood.matrix[i,j]<-neg.ll(x,mu.test.values[i],sigma2.test.values[j])
   }
 }
```

We can plot the results using the functions 'image' and 'contour', and place on top of this plot the maximum likelihood as found by the grid search as well as the known parameter values.

```{r}
image(mu.test.values,sigma2.test.values,likelihood.matrix,col=topo.colors(100))
contour(mu.test.values,sigma2.test.values,likelihood.matrix,nlevels=30,add=T)

max.element<-which(likelihood.matrix==min(likelihood.matrix),arr.ind=T)
points(mu.test.values[max.element[1]],sigma2.test.values[max.element[2]],pch=16,cex=2)
points(1,4,pch='x',cex=2)
```

Now we can plot the likelihood "slices", which show cross sections across the search grid for fixed values of $\mu$ or $\sigma^{2}$.

```{r}
par(mfrow=c(1,2))
plot(mu.test.values,likelihood.matrix[,max.element[2]],typ="b")
plot(sigma2.test.values,likelihood.matrix[max.element[1],],typ="b")
```

Notice how the likelihood curve for $\sigma^{2}$ is not symmetric. While a horizontal line drawn at some higher value (which represents the likelihood of an alternative hypothesis) yields a fairly symmetric confidence interval for $\mu$, the assymetry of the likelihood surface yields a highly assymetric confidence interval for $\sigma^{2}$. Confidence intervals do not have to be symmetric! **However**, immediately in the vicinity of the minimum, the likelihood surface **is** approximately quadratic and symmetric. We will come back to this in a second.

In this case, the bivariate likelihood surface shows no correlation between $\mu$ and $\sigma^{2}$, but this is not always the case. Sometimes you get strong correlations among parameter estimates and get diagonal "ridges" in parameter space. In this case, it is important to distinguish between the likelihood profile and likellihood slices. (see Bolker!) The likelihood surface need not even have a single maximum; there could be several peaks which makes it difficult to define the MLE or its confidence intervals. If there are strong tradeoffs between parameter values, it is often better to discuss the MLEs in terms of a confidence region, which is the envelop of parameter space that you are [insert confidence limit here] percent certain contains the true combination of population parameter values.

R has a function 'optim' which optimizes functions (and is thus much better than a simple grid search 'brute force' approach we just did) and is very handy for minimizing the LL. We take advantage of the R function that gives us the probability density function, which saves us having to hard code that into R. Make sure the use of 'dnorm' in the code below makes sense!


```{r}
neg.ll.v2<-function(x,params)
{
mu=params[1]
sigma=params[2]
-sum(dnorm(x,mean=mu,sd=sigma,log=TRUE))
}
```

Notice that I used the "log-TRUE" option to take the log inside the dnorm command, which saves me taking it later. I also had to pass the parameters as one variable since that is what 'optim' is expecting. Take a second to convince yourself that the neg.ll and neg.ll.v2 functions give the same answer. 

We still need a way to maximize the log-likelihood and for this we use the function 'optim':

```{r}
opt1<-optim(par=c(1,1),fn=neg.ll.v2,x=x)
opt1
```

An even easier way is to use the 'fitdistr' command we already learned about, but the 'optim' function comes in handy all the time and is the only option you have when fitting non-tranditional distributions not covered by 'fitdistr'.

```{r}
library(MASS)
fitdistr(x,"normal")
```

Notice that this function outputs the SE as well, whereas our function and 'optim' only give the MLE. 

The likelihood is a relative concept that only makes sense relative to other possible datasets. The absolute magnitude depends on the "sample space" of the data and sometimes even the maximum likelihood is a very small value. So all we can do is compare relative likelihoods. 

We now know how to use maximum likelihood to calculate the "best" parameter value (in the sense that it is the parameter value that maximizes the likelihood, or minimizes the negative log-likelihood.) But we know that parameter estimates by themselves are useless. We need to somehow calculate the uncertainty in our maximum likelihood estimate, i.e. the confidence interval.

For example, if I have a one-parameter model (let's call the parameter $\theta$) and I want to find a confidence interval on $\theta$, then I want to ask the question: What values of estimated theta are reasonable under the null hypothesis that $\hat{\theta}$ is the true value? We want to find a window of $\theta$ values around $\hat{\theta}$ that are similar enough to $\hat{\theta}$ that I would *not* reject the null hypothesis (that $\hat{\theta}$ is the true value). Since $\hat{\theta}$ is the one that minimizes the NLL surface, any other $\theta$ value would have a higher NLL.

It turns out that, if one model represents a special case of another likelihood, the ratio between two likelihoods is related to a $\chi^{2}$-distribution. 

$$
-2log\left(\frac{\widehat{\mbox{L}_{r}}}{\widehat{\mbox{L}}}\right) \sim \chi^{2}_{r}
$$

where $r$ is the number of "constrained" parameters for the smaller model. So if I compare a model where $\theta$ is allowed to vary (the original model) to one in which I fix $\theta$ arbitrarily (somewhere higher on the NLL surface), then the likelihood ratio ($\times$ -2) follows a $\chi^{2}_{1}$.

Therefore, we can set a cut-off for the difference in log-likelihoods based on the 95th percentile of the $\chi^{2}$ distribution.

```{r}
qchisq(0.95,1)/2
```

which equals 1.92 log-likelihood units if you are looking at only one parameter to be estimated. Therefore, if we have only one parameter, then we simply calculate the NLL over a range of parameter values, and find the CIs representing those parameter estimates which have <1.92 increase in NLL from the MLE.

You will explore this more in the problem set. There is also more discussion of this in Bolker's Chapter #6. 

While the above method (finding the bounds on the parameter within which the increase in the NLL is less than or equal to the threshold determined by the number of parameters and the desired critical value $\alpha$) is the best method of finding the confidence intervals for a parameter, it's worth coming back to the idea that "fitdistr" returns an estimate of the parameter standard errors. How does "fitdistr" do that? It turns out that the standard error is related to the inverse of the second derivative of the NLL right in the vicinity of the minimum where the curve is approximately quadratic. (The second derivative of the NLL has to be positive, do you see why?) 

```{r echo=FALSE, fig.align='center', fig.cap='Comparing steep and shallow NLL functions, and its impact on the estimated standard errors.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/FisherInformationFigure.png')
```

A large second derivative is associated with a steep curve in the NLL, and this results in a small standard error (does this make sense?). Conversely, a very flat NLL that gently slopes up would be associated with a large standard error. So, if "fitdistr" estimates the standard error, why not just create the confidence intervals using [MLE-1.96$\times$SE, MLE+1.96$\times$SE]? Well, you *could*, but this assumes that the NLL is symmetric around its minimum and that the confidence interval is correspondingly symmetric around the maximum likelihood estimate. For some parameters for some distributions, this assumption will be fine, but for other parameters this may be a poor assumption and so the confidence intervals created using the standard error will be only approximate (and the approximation may not be that accurate, especially if you want 90th or 50th percentile confidence intervals [do you see why the approximation gets worse as you estimate larger CI intervals?]). Using the NLL function directly in the manner described about involves no approximation and will always be more correct.

Week 5 Lecture
========================================================
Statistical power
----------

In any hypothesis test, there are 4 possible outcomes.

```{r echo=FALSE, fig.align='center', fig.cap='Type I and Type II errors', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Power.png')
```

B and C are the **correct answer**. If $H_{0}$ is false, we want to reject the null hypothesis.

A = Probability of rejecting $H_{0}$ when $H_{0}$ is true = Type I error = This is your $\alpha$!

D = Probability of not rejecting $H_{0}$ when $H_{0}$ is false = Type II error

The power of a statistical test is defined as

$$
\mbox{Power} = \frac{C}{C+D}
$$
In words, **power is the probability of correctly rejecting the null hypothesis**.

Power calculations boil down to this unavoidable fact: When variance is high, you need larger samples. When the differences are small, you need larger samples. There is a tradeoff between Type I and Type II errors. As a general rule of thumb, people aim for Type I errors of 0.05 and Power = 0.80.
Question for the class: Why do we worry more about Type I errors than Type II errors? When would a Type II error be really serious? (For example, failing to detect a disease agent...)

Power calculations only make sense before an experiment, not after. If you found a significant effect, then clearly you had enough power, and if no significant effect, you clearly do not enough power. The main utility of power calculations is to get some intuition for the necessary sample size required while designing an experiement.

In order to plan a study, you need to know how many samples you need to detect as significant difference of a certain magnitude. For a single sample comparison (against a fixed value)

$$
n = \left(\frac{\sigma(z_{1-\alpha}+z_{1-\beta})}{\delta}\right)^{2}
$$

You will be derive this as a group in lab this week but for now its enough to note that in the very common case that $\alpha$=0.05 and power=1-$\beta$=0.80, this can be approximated by

$$
n = \frac{8\sigma^{2}}{\delta^{2}}
$$
Note that some authors use the symbol $\sigma^{2}$ under the assumption that this is known (or assumed) prior to calculation, whereas others substitute $s^{2}$ for $\sigma^{2}$ in recognition that this is calculated from the data, although they leave the formula unchanged. As long as you understand what is meant by the symbol, than it is not important which you use here.

*Example*: If you want to be able to detect a change of 2.0 in a population with variance $\sigma^{2}$=10.0, then we would need 8x10/$2^{2}$ = 20 replicates.

For a two sample comparison, the sample size (for each group!) required is simply twice

$$
n = 2\left(\frac{\sigma(z_{1-\alpha}+z_{1-\beta})}{\delta}\right)^{2}
$$
or

$$
n = \frac{16\sigma^{2}}{\delta^{2}}
$$
I won’t say a lot more about power calculations because I think that these calculations typify the old way of thinking because they depend so heavily on the arbitrary cut-off of “significance” that is so problematic in null hypothesis testing. Such power calculations can give you some general rules of thumb for whether an experiment is well designed, but even then it required some major assumptions about the size of the unknown variance that I’m not sure how helpful they are.

**A bit of review: What have we done so far this semester?**

1.	We learned a bit about probability theory (joint probability, union and intersection, etc.).
2.	We learned about hypothesis testing and parameter estimation, using two randomization-based procedures that gave us the main idea behind these two key concepts
3.	We learned a suite of discrete and continuous probability distributions. These probability distributions allow us to describe the relative probability of various outcomes. We often use probability distributions to model our data. We learned various ways of expressing probability distributions (e.g., cumulative distributions, quantiles, etc.), and we learned how to calculate various properties of a distribution (e.g., the expected value E[X] and the variance Var[X]).
4. We learned two different ways to estimate the parameters of a parametric distribution given data: A) Using the Central Limit Theorem and other "theory" about distributions; B) Using maximum likelihood. While we discussed four special cases where we can use Method A, Method B is MUCH more general, and can be used under all circumstances. Maximum likelihood is the "go-to" approach for fitting models to data if you wish to fit a parametric distribution to your data.

*An important sidebar*: Now that we’ve covered maximum likelihood estimation, and we know how to use ML to estimate a parameter and its confidence intervals, lets cycle back to what we learned about randomization-based procedures in Week 2. In Week 2, we use bootstrap and jackknife to estimate the confidence intervals (or the standard error, if you prefer to think of it that way) for a parameter estimate. So why might we prefer one approach (ML or bootstrap/jackknife) over the other? ML is the standard approach if you know the joint likelihood of your data, as it is computationally much more efficient and it has well known properties because you have specified the distribution of your data using well described distributions. However, sometimes you don’t know the joint distribution of your dataset? Why not?

a)	It may be that each individual data point does not come from a known distribution (or, put another way, none of the known paramteric distributions fit your data well)
b)	It may be that each individual data point does come from a known distribution but your data are not independent and you are not able to describe the JOINT distribution of your data
In these cases, we tend to fall back on non-parametric methods like bootstrap and jackknife, such as what was covered in Week 2. 

We will now learn how to combine our knowledge of all the univariate distributions with our understanding of hypothesis testing to test hypotheses about the parameters of a distribution. (In other words, we will learn how to use statistics to pose and answer quantitatively rigorous questions about our data.) 

*First, a few reminders about statistical hypothesis testing...*

We frame decision-making in terms of a null and an alternative hypothesis: $H_{0}$  vs. $H_{A}$. Let's say that we are measuring the growth rates of bird colonies, and as before, we use a Normal distribution $N(\mu,\sigma^2)$. One reasonable null hypothesis might be $H_{0}:\mu=0$. So we collect data on several bird colonies, and we might find that our confidence interval for $\mu$ contains 0. In this case, we cannot reject the null hypothesis that $\mu=0$. But we also cannot affirmatively prove the null hypothesis. We simply "cannot reject" the null hypothesis. There are two reasons we "cannot reject" the null hypothesis. It might be that $\mu$ really is equal to 0. It is also possible that $\mu \neq 0$ but we did not have enough data to shrink the confidence intervals sufficiently to reject the null hypothesis. In this latter scenario, we would say that we did not have enough "statistical power" to reject the null hypothesis. 

**The six steps of null hypothesis testing are**:

**Step #1**: Specify a null hypothesis $H_{0}$.

**Step #2**: Specific an appropriate test statistic T. A test statistic is some summary of your data that pertains to the null hypothesis. For testing simple hypotheses, there are test statistics known to be ideal in certain situations. However, even in these simple cases, there are other test statistics that could be used. In more complex situations, YOU will have to determine the most appropriate test statistic. 

generic=$T$=f(X)
specific=$T^{*}$=$T(X_{1},X_{2},...,X_{n})$

We are familiar with some test statistics already, for example the use of $\bar{X}$ as a measure of the mean of a normally distributed population.

**Step #3**: Determine the distribution of the test statistic under the null hypothesis $H_{0}$. A test statistic is a statistical quantity that has a statistical distribution ($f(T│H_{0})$). Remember that this is the probability of obtaining the test statistic T GIVEN the null distribution, it is NOT
$f(H_{0}│T)$. The test statistic and its distribution under the null hypothesis form the statistical test. Test = Test statistic + Distribution of test statistic under $H_{0}$.

**Step #4**: Collect data and calculate $T^{*}$. Collect data by taking random samples from your population and calculate the test statistic from the sample data.

**Step #5**: Calculate a p-value. Calculate the probability that you would get a value for the test statistic as large or larger than that obtained with the data under the null hypothesis 
$P(T^{*}│H_{0})$=p-value.

**Step #6**: Interpret the p-value. Use the p-value to determine whether to reject the null hypothesis (or, alternatively, to decide that the null hypothesis cannot be rejected).

Note that these steps apply for both parametric and non-parametric statistics. The same basic steps also apply whether the test statistic follows a known distribution under the null hypothesis or whether the distribution under the null hypothesis needs to be generated by randomization (randomization test).

The basic idea underlying all statistical tests: What is the probability that I would get a test statistic as large or larger (as produced by the data) if the null hypothesis was true (this is the ''p-value''). To answer this question we need (1) a test statistic and (2) a distribution under the null hypothesis.

p-value = P(data|H0)

**Remember – the p-value is a statement about the probability of getting your data if the null hypothesis were true. It is NOT a statement about the probability that the null hypothesis is true.** 

Not all tests are created equal!! Tests differ in their power to detect differences and their "efficiency". The balance between power and efficiency depends on the specific situation; we will discuss this more next week. 

To get some practice in constructing and executing hypothesis tests, we are going to go over 4 classic and frequently used hypothesis tests:

1.	The t-test
2.	The F-test
3.	Test of binomial proportions
4.	Test of two distributions

The t-test is used to make inference on the means of normally distributed variables. There are three varieties of the t-test – one to test whether the mean of some normally distributed variable is equal to some hypothesized value, one to test whether the means of two unpaired samples are equal, and one to test whether the means of two paired samples are equal.

The single sample t test
---------------

(The t-test is often called the Student's t-test, as it is named after the pseudonym under which the original paper was submitted. However, I will refer to it as just the t-test for simplicity.)

The single sample t-test is used when you want to compare the mean of a distribution to a prescribed value. $H_{0}: \mu = c$

Let’s say we have normally distributed data:

$$
X \sim N(\mu,\sigma^{2})
$$

The Central Limit Theorem says:

$$
\bar{X} \sim N(\mu,\sigma^{2}/n)
$$

which means that

$$
\frac{\bar{X}-\mu}{\sqrt{\sigma^{2}/n}} \sim N(0,1)
$$

and if we do not know $\sigma^{2}$, that

$$
\frac{\bar{X}-\mu}{\sqrt{s^{2}/n}} \sim t_{n-1}
$$

Remember that:

$$
H_{0}: \bar{X}=c
$$
$$
H_{A}: \bar{X} \neq c
$$
Therefore, under the NULL HYPOTHESIS

$$
\frac{\bar{X}-c}{\sqrt{s^{2}/n}} \sim t_{n-1}
$$

**<span style="color: orangered;">In this case, our “test statistic” is $T=\frac{\bar{X}-c}{\sqrt{s^{2}/n}}$ and this test statistics follow the $t_{n-1}$ distribution. In other words, under the null hypothesis (if $\mu$ really is c), the value of the test statistic for any particular dataset of size n will be drawn from this distribution.</span>**

Let’s say when I actually calculate T for the data I have, I get $T^{*}$.

$$
P(T \leq (-|T^{*}|) \mbox{ OR } T \geq (|T^{*}|)|H_{0}) = p-value
$$

If p<0.05, we say that there is $<5\%$ probability that we would obtain something as or more ‘extreme’ than $T^{*}$ if the null hypothesis was true, we therefore REJECT the null hypothesis

If p>0.05, we say that there is a $>5\%$ probability we would obtain something as or more ‘extreme’ than $T^{*}$ if the null hypothesis is true, and therefore we DO NOT REJECT the null hypothesis.

Note that the t-test assumes that the data are Normally distributed, but it turns out that the t-test is fairly robust to violations of this assumption. In fact, the Central Limit Theorem says that (with a few minor requirements) the means of data have as their limit (for large sample sizes) the Normal distribution, so the t-test is often valid even if the original data is not Normally distributed.

Keep in mind that the t-test is intimately connected to the idea of calculating a confidence interval for the mean of a Normally distributed population. Last week, we derived this formula:

$$
P(\bar{X}-\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]} \leq \mu \leq \bar{X}+\sqrt{\frac{s^{2}}{n}}t_{(1-\alpha/2)[dof]}) = 1-\alpha
$$

So we now can test a hypothesis regarding the value of the mean *and* we can generate confidence intervals on the true (but unknown) mean of the population we are studying.

The unpaired two sample t test
---------------

The unpaired two-sample t test is used when you have data from two groups and you want to test whether these groups have the same mean value (i.e. $H_{0}: \mu_{A} = \mu_{B}$. As before, we will assume that the data are at least approximately Normally distributed. 

$$
X_{A} \sim N(\mu_{A},\sigma_{A}^{2})
$$

$$
X_{B} \sim N(\mu_{B},\sigma_{B}^{2})
$$

If datasets A and B are independent (in other words, each draw from A is not correlated to a corresponding draw from B), then the difference between these two datasets is given by

$$
X_{A}-X_{B} \sim N(\mu_{A}-\mu_{B},\sigma_{A}^{2}+\sigma_{B}^{2})
$$
Why? It's worth going back to review the Algebra of Expectations, but in brief, when you add or subtract independent variables, their variances add.

It follows (but I leave the few steps of algebra for you):

$$
\bar{X_{A}}-\bar{X_{B}} \sim N(\mu_{A}-\mu_{B},\frac{\sigma_{A}^{2}}{n_{A}}+\frac{\sigma_{B}^{2}}{n_{B}})
$$

Note that in addition to making no assumption about sample variances, we make no assumption of equal sample sizes between the two datasets being compared.

Therefore,the standard error of the difference between means is given by

$$
SE = \sqrt{\frac{\sigma_{A}^{2}}{n_{A}}+\frac{\sigma_{B}^{2}}{n_{B}}}
$$

which we have to estimate from the data using the sample variances $s^{2}$:

$$
SE = \sqrt{\frac{s_{A}^{2}}{n_{A}}+\frac{s_{B}^{2}}{n_{B}}}
$$

So our test statistic in this case is

$$
T = \frac{\bar{X_{A}}-\bar{X_{B}}}{\sqrt{\frac{s_{A}^{2}}{n_{A}}+\frac{s_{B}^{2}}{n_{B}}}}
$$

Now all we need is the distribution of the test statistics under the null hypothesis which is, by definition (this is a t-test after all) the t distribution. The degress of freedom for this distribution is a bit complicated:

$$
dof = \frac{\left(\frac{s_{A}^{2}}{n_{A}}+\frac{s_{B}^{2}}{n_{B}}\right)^{2}}{\frac{\left[\frac{s_{A}^{2}}{n_{A}}\right]^2}{n_A-1}+\frac{\left[\frac{s_{B}^{2}}{n_{B}}\right]^2}{n_B-1}}
$$

Note that there are some simpler formulas that apply if you assume equal sample size and/or equal variances. In the case of equal variances, you can pool the data to find a pooled estimate of the common variance $s^{2}$, but I will not go into the details here. Note that the default for R is to assume unequal sample sizes and unequal variances.

If the variances are assumed equal, this simplifies somewhat

$$
T = \frac{\bar{X_{A}}-\bar{X_{B}}}{SE_{diff}} = \frac{\bar{X_{A}}-\bar{X_{B}}}{\sqrt{s^{2}_{pooled}\left(\frac{1}{n_{A}}+\frac{1}{n_{B}}\right)}}
$$

How do we calculate $s^{2}_{pooled}$?

$$
s^{2}_{pooled} = \frac{1}{n_{A}+n_{B}-2}(SS_{A}+SS_{B})
$$

where $SS_{A}$ is the sums-of-squares for dataset A and $SS_{B}$ is the sums-of-squares for dataset B.

Why bother assuming the variances are equal? By combining the data in the estimate of their pooled variance, we get a better estimate of $s_{pooled}^{2}$ than either $s_{A}^{2}$  or $s_{B}^{2}$.

The paired two sample t test
-----------------

Paired data occurs when each datapoint in set A corresponds to a datapoint in set B. Examples might be the strength of the left vs. right leg in a sample of individuals, or the blood sugar of husbands vs. wifes in a sample of married couples. In these cases, the question at hand is whether the difference between the two datasets is equal to some value or not. In other words, the null hypothesis is $H_{0}: X_{A}-X_{B} = c$. The test statistic is the same as with the unpaired test

$$
T = \frac{\bar{X_{A}}-\bar{X_{B}}}{SE_{diff}}
$$
but now our calculation of the $SE_{diff}$ changes because we are no longer assuming the two datasests are independent. In this case, the variances do not simply add, and the correct expression is

$$
X_{A}-X_{B} \sim N\left(\mu_{A}-\mu_{B},\sigma^{2}_{A}+\sigma^{2}_{B}-2Cov(A,B)\right)
$$
(Remember that $\sigma^{2}_{A}$ is just Var(A) or, put another way, the Cov(A,A). So this is the more general formula for the difference between two random Normally distributed variables, because if the two datasests are in fact independent, Cov(A,B)=0 and we end up with the simpler formula we introduced earlier.

Working through the algebra a little

$$
\bar{X_{A}}-\bar{X_{B}} \sim N(\mu_{A}-\mu_{B},\frac{\sigma^{2}_{A}+\sigma^{2}_{B}-2Cov(A,B)}{n})
$$

So now the test statistic looks like

$$
T = \frac{\bar{X_{A}}-\bar{X_{B}}}{SE_{diff}}
$$

as before, but the $SE_{diff}$ is given by

$$
SE_{diff} = \sqrt{\frac{\sigma^{2}_{A}+\sigma^{2}_{B}-2Cov(A,B)}{n}}
$$

The last term represents the covariance between sample A and sample B. When this covariance is positive, the variance of the difference is reduced, which means that any given difference found between the two samples is actually *more significant*. Therefore, if the data are paired, a paired t-test will yield more significant results because it is a *more powerful test* for paired data. We will see this in action in lab on Wednesday.

Note that a paired two-sample t-test is equivalent to a one-sample t-test where you create the one sample dataset by subtracting the paired data. In other words, 

$$
Y = X_{A}-X_{B}
$$
Now you can do a one-sample t-test on Y just as we did before. This is usually the easiest way to deal with paired data.

The t-test does assume that the data are Normally distributed and, depending on the form we choose to use, we may be assuming that the variances are the same. Given a real dataset, you wouldn't know for sure whether the variance are the same, and so you would need to first test whether the variances are the same (or, rather, whether you can reject the null hypothesis that they are the same). How do we test whether two datasests come from populations with the same variance - the F-test! (That's coming now...)

The F test
---------------

(This is often referred to as Fisher's F test, but I will just stick with F test.)

Its fairly obvious why someone would want to compare two means, but less obvious why you would want to compare two variances. As mentioned just a second ago, one of the biggest uses of the F-test is to determine whether two samples violate the equal-variances assumption underlying the t-test. Another major use is when comparing two nested models to determine which model fits a dataset better. (We will see this again when we cover ANOVA.)

The null hypothesis for the F test is $H_{0}: \sigma^{2}_{A} = \sigma^{2}_{B}$. Here I assume the dataset with the larger sample variance is sample A, so the implied alternative is $H_{A}: \sigma^{2}_{A} > \sigma^{2}_{B}$. In this case, I am testing whether we can reject the null hypothesis that the two parametric variances are actually the same (even if the sample variance of A is larger) [this is the one-tailed test, we will discuss the two-tailed test at the end].

Remember from last week:

$$
\frac{s_{A}^{2}/\sigma_{A}^{2}}{s_{B}^{2}/\sigma_{B}^{2}} \sim \frac{\chi^{2}_{n-1}/(n-1)}{\chi^{2}_{m-1}/(m-1)} \sim F_{n-1,m-1}
$$

Therefore, under the null hypothesis, we get

$$
\frac{s_{A}^{2}}{s_{B}^{2}} \sim F_{n-1,m-1}
$$
The left hand side of this equation, the ratio of the sample variances, is the test statistic for the F test and we call it the F statistic. Under the null distribution

$$
(F_{s}=\frac{s_{A}^{2}}{s_{B}^{2}}|H_{0}) \sim F_{n-1,m-1}
$$
That that (by convention) the larger variance is placed in the numerator.

Under the null hypothesis that the two samples are drawn from populations with the same variance, the F-statistic $F_{s}$ is distributed as the F-distribution. The F-distribution is peaked around 1.0 because the two variances are samples estimates of the same quantity. The only difference between $s_{1}^{2}$ and $s_{2}^{2}$ is the sample size. The F-distribution depends on two degrees of freedom, n-1 and m-1. There exists a separate F-distribution for each combination of n and m.

As before, we would reject the null hypothesis is our test statistic is an ‘extreme’ (and hence unlikely) value to get from the null distribution. What should we use as the critical value for this test? If you are testing 

Case 1: $H_{0}:  \sigma_{A}^{2}= \sigma_{B}^{2}$ vs. $H_{A}:\sigma_{A}^{2} \neq \sigma_{B}^{2}$

then you need a two-tailed test and you use the $\alpha/2$ quantile for the F distribution.

If you are testing

Case 2: $H_{0}:  \sigma_{A}^{2}= \sigma_{B}^{2}$ vs. $H_{A}:\sigma_{A}^{2} > \sigma_{B}^{2}$

then you need a one-tailed test and you use the $\alpha$ quantile for the F distribution.

The R function for testing whether the variances of two samples are different is ''var.test''. We will be using this function in lab this week.

Comparing two proportions
---------------

There are a number of different ways to do the proportion test, but I will only expect you to know one. I will call the true underlying proportion $\theta$. In this case, $H_{0}: \theta = \theta_{0}$, and for the two-tailed test, $H_{A}:\theta \neq \theta_{0}$.

The approach we will go over is called the Wald test (see also Handout), and it uses the large sample normal approximation to the Binomial distribution.

Recall that

$$
\mbox{lim}_{n \rightarrow \infty} Binom(n,\theta) \rightarrow N(n\theta,n\theta(1-\theta))
$$
from which it follows ([details below](#binomialproof))

$$
\mbox{lim}_{n \rightarrow \infty} \hat{p} \rightarrow N(\theta,\frac{\theta(1-\theta)}{n})
$$

So under the null hypothesis that $\theta=\theta_{0}$

$$
\frac{\hat{p}-\theta_{0}}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}}
$$

(Technically, since we estimated the s.e. using the data, this is t-distributed, but since we are already assuming $n \rightarrow \infty$, then we typically use the standard normal here.)

Therefore, we can compare out test statistic against the standard normal to decide if the observed value is EXTREME, i.e. (if $T^{*}$ is positive [see handout])

$$
P(T \geq T^{*}│H_{0})+P(T \leq (-T^{*}) │H_{0}) = \mbox{p-value for 2-tailed test}
$$

Correspondingly, we can derived confidence intervals on the true proportion $\theta$.

$$
P\left(\hat{p}-z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \leq \theta \leq  \hat{p}+z_{1-\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\right) = 1-\alpha
$$

This approximation works if $\theta \sim 0.5$, and sample size is large. 

Side note: Another test for proportions is called the “score” test, defined as

$$
\frac{\hat{p}-\theta_{0}}{\sqrt{\frac{\theta_{0}(1-\theta_{0})}{n}}} \sim N(0,1)
$$

Notice that the s.e. here is a function of value $\theta_{0}$ in our null hypothesis. It turns out this is a “better test” (coverage more closely 1-$\theta$) but it is less commonly used because the expression for the confidence interval is very complicated.

Question: What can go wrong with the Wald test if $\theta \sim$ 0 or 1?

Answer: We can easily get confidence intervals that extend beyond (0,1). We can truncate the CIs at 0 or 1, but the “coverage” of the CI is no longer 1-$\alpha$.

There are MANY methods statisticians have devised to obtain true 1-$\alpha$ CIs that do not go outside of (0,1) and which work for all $\theta$. I have posted a paper under “Interesting statistics papers” that address 7 methods, including the Wald approximation, but I will only expect you to know the Wald approximation.

Comparing two distributions
--------------

There are many ways that we might compare two distributions. Many of these methods focus on the difference between the empirical cumulative density functions (i.e. $P(X \leq X^{*})$), and they differ in the loss function (that is, the function used to weight differences of different magnitudes) used when comparing the CDFs.

Here I introduce the Kolmogorov-Smirnov test (or the K-S test), which is one of the most common. The K-S test can be used to compare two empirical distributions OR an empirical distribution against a parametric distribution. The K-S statistic D is the *maximum* difference between the two CDFs or, more formally, 

$$
D=sup_{x} |CDF_{1}(x)-CDF_{2}(x)|
$$

```{r echo=FALSE, fig.align='center', fig.cap='Illustration of the empirical CDF (blue) and the CDF of the distribution being tested (red). Source: Wikimedia Commons', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/KS_Example.png')
```

The expected distribution of D under the null hypothesis is complicated and not one you need to know. We will go over R’s functions to do these tests on Thursday.

A bit more detail on the Binomial
--------------

<a id="binomialproof"></a> Above I skipped over some details about deriving the Binomial from the Bernoulli. Here I fill in those details.

Let's assume X follows a Bernoulli distribution:

$$
X \sim Bernoulli(\theta)
$$
and that Y represents the sum of multiple Xs.

$$
Y = \sum_{i=1}^{n}X_{i} \sim Binomial
$$
The Central Limit Theorem states that

$$
\sum_{i=1}^{n}X_{i} \rightarrow N(mean=\sum_{i=1}^{n}E[X_{i}],variance = \sum_{i=1}^{n}Var[X_{i}],)
$$
If we add up all the coin flips (i.e., all the $X_{i}$) than get $n\hat{p}$ because the empirical probability (what we actually get out of the coin flip experiment, which we use to estimate the theoretical population parameter $\theta$) of getting $X_{i}=1$ is just $\hat{p}$ and we flip n coins.

$$
n\hat{p} \rightarrow N(mean=n\theta,var=n\theta(1-\theta))
$$
So if we divide through by n

$$
\hat{p} \rightarrow N(mean=\theta,var=\theta(1-\theta)/n)
$$
in the limit that $n \rightarrow \infty$.

Side-note about the Wald test
-------------

We introduced the Wald test in the context of the binomial test, but the Wald test is a much more general test about the statistical significance of a maximum likelihood estimate. We often use the Wald test even when we haven’t proven that the estimate in question is actually the MLE. In the case of the proportion test, the MLE for the binomial parameter p is just $\hat{p}$ (=# heads)⁄n.

The Wald test states that if you have a parameter estimate $\hat{\theta}$ and you want to test it against the null hypothesis value $\theta_{0}$, you can use the following (approximate) relationship

$$
\frac{\hat{\theta}-\theta_{0}}{se(\hat{\theta})} \sim N(0,1)
$$

or, equivalently,

$$
\frac{(\hat{\theta}-\theta_{0})^{2}}{var(\hat{\theta})} \sim \chi^{2}_{1}
$$

The standard error of a maximum likelihood estimate $se(\hat{\theta})$ is usually approximated using the inverse of the second derivative of the log-likelihood (if you are interested, this is called the Fisher Information matrix). This makes intuitive sense because if the negative log-likelihood surface is steep around the minimum (and the second derivative large), the uncertainty about the MLE is small (narrow CI). I won’t get into detail about this, because often you have some knowledge of the parameter’s variance and can use the Wald test with little fuss or calculation.

Chi-squared goodness-of-fit test 
-------------

$H_{0}$ is a table (need not be 2 $\times$ 2) of predicted probabilities such as

```{r echo=FALSE, fig.align='center', fig.cap='2 x 2 contingency table', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Chisq1.png')
```

and you want to test whether the data are consistent with the null hypothesis of known probabilities.

The chi-squared test statistic is

$$
X^{2} = \sum_{\mbox{cell} i}\frac{O_{i}-E_{i}}{E_{i}}
$$
$$
X^{2}|H_{0} \sim \chi^{2}_{(r \times c) -1}
$$


We have lost a single degree of freedom because the total sample size of the observed data constrains the value of one of the cells given the other three. When we have a single proportion (e.g., percentage of men vs. women in class) we can use this as an alternative to the binomial test we discussed in class. (When the expected frequencies are small, the binomial test is preferred over the chi-squared goodness of fit test.)

If all you are given are marginal probabilities, you have to assume independence to get the probabilities for individual cells.

Chi-squared test of independence
-------------

$H_{0}$ is a table (need not be 2 $\times$ 2) of marginal probabilities 

```{r echo=FALSE, fig.align='center', fig.cap='2 x 2 contingency table with marginal probabilities only', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Chisq2.png')
```

**or** a table of observed data from which marginal probabilities can be calculated


```{r echo=FALSE, fig.align='center', fig.cap='2 x 2 contingency table', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Chisq3.png')
```

The chi-squared test statistic is

$$
X^{2} = \sum_{\mbox{cell} i}\frac{O_{i}-E_{i}}{E_{i}}
$$


$$
X^{2}|H_{0} \sim \chi^{2}_{(r-1) \times (c-1)}
$$
We have one degree of freedom from each row and column because the total sample size in each row and column is fixed by the marginal totals. 
This test is used to test whether the characters are independent.

Week 6 Lecture
========================================================

The ''family-wise error rate'' is

$$
\alpha = 1-(1-\alpha^{’})^k
$$

where $\alpha^{'}$ is the ''per-comparison error rate''. This is an important formula, don’t just memorize it - make sure you understand it!

Solution #1: One solution is to narrow down the model a priori to a single model that you think best captures the relevant biology of the system. 

Solution #2: Another work-around is to lower the threshold for significance so that the Type I error rate for the whole family of comparisons is still 5$\%$. There are several ways to do this, of which we will discuss two:

Method #1: The first method simply involves rearranging the formula above

$$
\alpha=1-(1-\alpha^{'})^{k}
$$
$$
\alpha^{'}=1-(1-\alpha)^{1/k}
$$

This method, called the Dunn-Šidák method, simply sets the per comparison error rate as above. This assumes that all the comparisons are independent.

Method #2: If the comparisons are not independent, then it is better to use a more conservative method, called Bonferroni’s method (of the Bonferroni correction)

$$
\alpha^{'}=\alpha⁄k
$$

However, this can set the per-comparison Type I error rate so low that it severely inflates the probability of a Type II error.

Solution #3: Sequential Bonferroni’s test – 

Let’s say you start with 10 comparisons. You would test each comparison, rank them in order of their p-values, and discard the least significant (highest p value) if it was not significant at the $\alpha$/10 level, the next least significant if it was not significant at the $\alpha$/9 level, and so forth until all remaining comparisons were significant using a Bonferroni adjusted critical value for the number of remaining comparisons.

Solution #4: Resampling-based corrected p-values – 

$$
P_{corrected} = P(min(p) \leq p-observed|H_{0})
$$

In other words, if you were to simulate data under the null hypothesis, what is the probability that the smallest p-value among the set of comparisons made is smaller than or equal to the p-value that you actually obtained. (In other words, the adjusted p-value asks “how extreme is my most extreme p-value when compared against the most extreme p-values I would expect under the null hypothesis?)

Week 7 Lecture/Lab
=============

We have already covered most of the basic elements of plotting, but here I'll go over some elements of plotting that you may not have learned already this semester.

You will need the packages 'ggplot2','gplots', and 'ade4' so you might as well install them now.

```{r}
library(ggplot2)
library(gplots)
library(ade4)
```

Box plots
------------------

A boxplot is a convenient way of summarizing univariate data; the top and bottom of the rectangle represent the upper and lower quartiles of the data, the median by a horizontal line in the rectangle. There are several conventions for the whiskers, so the meaning of the whiskers should be explained clearly in the legend. One convention holds that the whisker extend to the further point that is no more than 1.5 times the interquartile range (75th-25th). In this case, outlying datapoints are shown with their own dot or star. You can also draw whiskers that extend out to the furthest datapoint.

Let's illustrate this using some randomly drawn data.

```{r}
boxplot(rnorm(1000,mean=0,sd=1),rnorm(1000,mean=0,sd=3))
```

We can play around with some of the options, such as naming the boxes and coloring the borders.

```{r}
boxplot(rnorm(1000,mean=0,sd=1),rnorm(1000,mean=0,sd=3),names=c("cats","dogs"), border=c("red","green"))
```

Two-dimensional data
-------------------------

Read in the following dataset describing [fox fur production as a function of year](https://github.com/hlynch/Biometry2021/tree/master/_data/FoxFurProduction.csv)

```{r}
fox.data<-read.csv("/Users/meganwyatt/Desktop/Biometry2021-master/_data/FoxFurProduction.csv", header=F)
```

We could do our entire analysis referring to the two columns as [,1] and [,2] but to make our code more readable, lets add some column names

```{r}
colnames(fox.data)<-c("year","production")
```

Now we can refer to the two columns of data as $year and $production.

Now let's plot the data:

```{r}
plot(fox.data$year,fox.data$production)
abline(a=200,b=0)
abline(h=200)
points(fox.data$year[fox.data$production>200],fox.data$production[fox.data$production>200],cex=2,pch=16)
large.production<- fox.data$production>200
points(fox.data$year[large.production],fox.data$production[large.production],cex=2,pch=16)
```

Notice that with this code I am highlighting all the points where fur production is >200, and I added a horizontal bar at 200 using the function "abline". (Stop: Discuss the options for abline.)

All software packages make horrible default plots and R is no exception. Make the following changes to make a more information plot and to explore what R can do.

1. Label the x and y axes.
2. Try plotting as a line using "typ="l"""
3. Plot both points and lines suing "typ="b""
4. Change the plotting symbol using "pch=XX" where XX is a number between 1 and 25. 
5. While black is the logical default color, play around with color using the option "col=XX" where XX can be either a number or a name.

We can get R to list all of the named colors using

```{r}
colors()
```

but we can also explicitly type in RGB values using the rgb() function and values (from 0 to MaxColorValue, which is 1 by default) for each of the red, green, and blue components. 
 
```{r}
plot(fox.data$year,fox.data$production,col=rgb(red=0,green=1,blue=0))
``` 
 
You can extract rgb values from a named color using the col2rgb() function

```{r}
col2rgb("peachpuff")
```

For complex plots which require a suite of colors, I highly recommend using the R package RColorBrewer. This package has a companion website for choosing color schemes (for mapping, for example) www.colorbrewer2.org. RColorBrewer allows you to pick color schemes with certain characteristics (diverging, sequential, etc) and to select a certain number of colors within that color scheme. The website also suggests color schemes that are color-blind friendly (often required for publication).

```{r}
library(RColorBrewer)
mypalette<-brewer.pal(7,"Greens")
image(1:7,1,as.matrix(1:7),col=mypalette,xlab="Greens (sequential)",
      ylab="",xaxt="n",yaxt="n",bty="n")
```

This is a sequential palette. ColorBrewer also offers up a variety of divergent and qualitative palettes such as

```{r}
display.brewer.pal(7,"BrBG")
```

and

```{r}
display.brewer.pal(7,"Accent")
```

There are many other options. You can extract the actual hex codes for these colors using

```{r}
brewer.pal(7,"Accent")
```

To play around with a few more graphics options, we will use one of the built-in datasets that has R called "mtcars". I will break one of my own rules here and attach the dataset.

```{r}
attach(mtcars)
plot(wt, mpg, main="Milage vs. Car Weight",xlab="Weight", ylab="Mileage", pch=18, col="blue")
text(wt, mpg, row.names(mtcars), cex=0.6, pos=4, col="red")
```

Note that we can add a legend:

```{r}
# Legend Example
boxplot(mpg~cyl, main="Milage by Car Weight",yaxt="n", xlab="Milage", horizontal=TRUE,col=terrain.colors(3))
legend("topright", inset=.05, title="Number of Cylinders",c("4","6","8"), fill=terrain.colors(3), horiz=TRUE)
```

Be aware of overplotting (points that overlap) especially when data are discrete or rounded. One strategy for overcoming this is jittering the points so the density of points can be displayed without overly distorting the underlying relationships. You can use the 'jitter' command in the base package.

```{r}
jitter(rep(0, 7))
```

Before we launch into three-dimensional plotting, we should introduce 'ggplot2' which is quickly becoming "industry standard" for making plots in R. There is *so* much that can be done with ggplot2 that we will only scratch the surface today, but at least this introduction will illustrate some of the things that ggplot2 can do.

The basic function to make plots using ggplot2 is 'qplot'. We can recreate the scatterplot of the mtcars data using ggplot2 as follows

```{r}
library(ggplot2)
qplot(wt, mpg, main="Milage vs. Car Weight",xlab="Weight", ylab="Mileage", data=mtcars)
```

We can also do fancier things, such as

```{r}
mtcars$gear <- factor(mtcars$gear,levels=c(3,4,5),
   labels=c("3gears","4gears","5gears"))
mtcars$am <- factor(mtcars$am,levels=c(0,1),
   labels=c("Automatic","Manual"))
mtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8),
   labels=c("4cyl","6cyl","8cyl"))

# Kernel density plots for mpg
# grouped by number of gears (indicated by color)
qplot(mpg, data=mtcars, geom="density", fill=gear, alpha=I(.5),
   main="Distribution of Gas Milage", xlab="Miles Per Gallon",
   ylab="Density")
```


There is more to ggplot2 than I can cover today, but if you are serious about making nice plots, its worth investing some time in learning this package.


Three-dimensional data
------------------------

There are almost an infinite variety of graphical options for R, most of which are available through the contributed R packages. We will use one of them now to demonstrate some of R's other graphical options.

You should already have installed the R package 'gplots' but if not, do so now. 

```{r}
library(gplots,quietly=TRUE,warn.conflicts=F)
```

For data, we will simply draw samples from the normal distribution. Feel free to choose any two distributions of your own liking.

```{r}
x <- rnorm(2000, sd=4)
y <- rnorm(2000, sd=1)
hist2d(x,y, same.scale=TRUE)
```

Notice that we had to use "same.scale=T" to make sure that distances along the x and y axes were comparable.

Now we will use the hisr2d function to create inputs for a three-dimensional perspective plot.

```{r}
h2d <- hist2d(x,y,show=FALSE, same.scale=TRUE, nbins=c(20,30))
persp( h2d$x, h2d$y, h2d$counts, ticktype="detailed", theta=30, phi=30, expand=0.5, shade=0.5, col="cyan", ltheta=-30,xlab="",ylab="",zlab="")
```

Make sure you know what these options all mean! Play around with theta and phi and see how that changes the perspective.

Let's use the same data and make a contour plot.

```{r}
contour( h2d$x, h2d$y, h2d$counts, nlevels=10)
```

We can also make a filled contour plot:

```{r}
filled.contour( h2d$x, h2d$y, h2d$counts, nlevels=10, col=gray((10:0)/10))
```

Play around with "col=rainbow(10)" or "col=topo.colors(10)".

Go back and play around with some of the other distributions from the last few weeks. It is particularly useful to use the 2-dimensional histograms to get a sense for what changing a distribution parameter does to the distribution.

We can demonstrate some other kinds of plots R can make using some of the built in datasets. Here we demonstrate both a color plot and a contour plot:

```{r}
x <- 10*(1:nrow(volcano))
x.at <- seq(100, 800, by=100)
y <- 10*(1:ncol(volcano))
y.at <- seq(100, 600, by=100)
image(x, y, volcano, col=terrain.colors(100),axes=FALSE)
contour(x, y, volcano, levels=seq(90, 200, by=5), add=TRUE, col="brown")
axis(1, at=x.at)
axis(2, at=y.at)
box()
title(main="Maunga Whau Volcano", sub = "col=terrain.colors(100)", font.main=4)
```

Multiple plots
---------------------

Now we will discuss how to arrange multiple plots of the same page.

With the par() function, you can include the option mfrow=c(nrows,ncols) to make a matrix of nrowsxncols plots that are filled by row. mfcol=c(nrows,ncols) would fill the matrix by columns.

```{r}
par(mfrow=c(2,2))
plot(mtcars$wt,mtcars$mpg, main="Scatterplot of wt vs. mpg")
plot(mtcars$wt,mtcars$disp, main="Scatterplot of wt vs disp")
hist(mtcars$wt, main="Histogram of wt")
boxplot(mtcars$wt, main="Boxplot of wt")
```

```{r}
# 3 figures arranged in 3 rows and 1 column
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))
hist(mtcars$wt)
hist(mtcars$mpg)
hist(mtcars$disp)
```

Now for a slightly complicated example that uses much of what we have learned. Childrens IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What proportion of children are expected to have an IQ between 80 and 120?

```{r}
mean<-100
sd<-15
lb<-80
ub<-120
x<-seq(-4,4,length=100)*sd+mean
hx<-dnorm(x,mean,sd)
plot(x,hx,typ="n",xlab="IQ Values",ylab="Density",main="Normal Distribution",axes=F)
i<-x>=lb & x<=ub
lines(x,hx)
polygon(c(lb,x[i],ub),c(0,hx[i],0),col="red")
```

As one final example, we will plot scatter plots with marginal histograms. For this we need to install the R package "ade4". This package has a huge number of useful functions, which we can look at here:

<a href="http://pbil.univ-lyon1.fr/ADE-4/" target="_blank">The ade4 webpage</a>

For now, I just want to use some of its plotting functionality to illustrate the kinds of sophisticated graphics R is capable of.

```{r}
library(ade4)
data(rpjdl)
```

First, lets just look at the data using "?rpjdl".

```{r}
coa1<-dudi.coa(rpjdl$fau,scannf=FALSE,nf=4)
s.hist(coa1$li)
```

Finally, just a word of advice about making figures for publication. While R is very flexible, making publication ready figures can be very time consuming in R. If you have any possibility of using Adobe Illustrator, I *highly* suggest using R to get your figure 90% correct, and then exporting it as an .eps (Encapsulated Postscript) to Illustrator. 

Except is very rare cases where you have an actual photo or image in your figure, you want to keep your figures as vector graphics throughout the entire process. Postscript (.ps) and Encapsulated Postscript (.eps) are vector file formats, but .tiff, .jpeg, and .png are not. Vector file formats will preserve the clarity pf your figure at any size scale, so your figure remains crisp and clear throughout the creation process. 

In Illustrator, you can relabel everything (in whatever font the editor prefers), fix the line widths, tweak the colors as needed, arrange multiple panels as needed, add arrows and legends or other details, make one figure an inset of another, or do anything else you could possibly want to do. Illustrator has a learning curve of its own but is well worth the investment.

Week 8 Lecture
=============

Warm-up
-----------

Which of these is the best model? Why?

```{r echo=FALSE, fig.align='center', fig.cap='Models of three different complexities. Figure adapted from [Grunwald (2007) The Minimum Description Length principle](https://mitpress.mit.edu/books/minimum-description-length-principle)', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/ModelComplexity.png')
```

The aims of modelling -- A discussion of Shmueli (2010)
------------------

Before we launch into the technical details of building linear models, let’s discuss the Shmueli (2010) paper. Shmeuli makes the distinction between three kinds of models

1.	Explanatory models 
2.	Predictive models
3.	Descriptive models

**Explanatory models** fit data with the goal of testing causal hypotheses about theoretical constructs. Explanatory models can therefore never be confirmed and are harder to contradict than predictive models. These are models used for *inference* (inferential models).

**Predictive models** fit data solely to predict new outcomes. Models either work for prediction or don't. Predictive models may involve parameters that are uninterpretable (e.g., Generalized Additive Models, spline fits, etc.). These models are used for *prediction.*

**Descriptive models** summarize data, and that's the only goal. These models are used to *describe* data.

**Question: What are the differences among these three types of model?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
    "Explanatory modeling refers…to the application of statistical models to data for testing causal hypotheses about theoretical constructs." (Shmueli 2010) Using this definition, explanatory modeling is testing a hypothesis or a theory and therefore can never be confirmed and are also harder (than predictive models) to contradict. We have been referring to these models as “inferential models” or models used for “inference”.

Predictive models are concerned only with the ability to predict new (measurable) outcomes. Either the models works for prediction or it doesn’t.  Predictive models are therefore more utilitarian than explanatory models (use whatever works!). Predictive models may involve predictors that are uninterpretable (more on this later when we discuss Generalized Additive Models and other curve fitting techniques).

Example: We can think back to our discussion of population statistics and sample statistics. If we are measuring every individual of a population and we want to summarize the data, we are doing *descriptive* statistics. Descriptive modeling is aimed at succinctly summarizing a body of data. If we want to use that data to make some inference about a larger population, we are doing *inferential* statistics. If we want to use our sample to predict a new sample from that larger population, we are doing *predictive* statistics. 
</span>
</details> 

As noted by Shmeuli, prediction and explanation are conflated in the hypothetical-deductive paradigm of null hypothesis significance testing, because we are saying in essence "If it predicts the data, then it explains the data". Does everyone see why this is the case?

The critical distinction between explanation and prediction can be seen by looking at the Expected Prediction Error (Shmeuli 2010; page 293), which we will discuss in more detail below.

This distinction is at the core of model selection because, when faced with a number of competing models, we must decide whether we prefer the most accurate model (in terms of capturing the underlying mechanism) or the one that will yield the best predictions. It is also important to note that predictive models are restricted to variables which can actually be measured ahead of time. For example, a model to predict wine quality at the time of harvest cannot depend on the type of cask because the type of cask hasn’t been measured at the time the model is to be run. Likewise, the cost of running an aircraft may depend on the cost of fuel on the day of the flight, but fuel costs on the day of flight cannot be used in a predictive model because it is not possible to know what the cost of fuel will be on that day.

**Question: If you have to choose one or the other, do you want the most accurate model that captures the underlying mechanism, or the model that will give the best predictions?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
If we describe "real life" as, $\mathscr{Y} = \mathscr{F}(\mathscr{X})$, where $\mathscr{F}$ is the *mechanism*, then in model space, we write, $Y = F(X)$, where $F$ is the *model*. With explanatory modeling, the goal is $F = \mathscr{F}$, while with predictive modeling, the goal is $Y = \mathscr{Y}$. 

The expected prediction error ($EPE$)

$$\text{EPE} = \mathrm{E} [ (Y - \hat{f} (X))^2 ]$$

can be broken down into three components:

$$
\text{EPE} = \mathrm{Var}(Y) + \text{bias}^2 + \mathrm{Var} (\hat{f} (X))
$$

1.The variance in the response, or the underlying stochasticity in the process ($\mathrm{Var} (Y)$), 2. the bias, which describes to what degree the model is misspecified ($\text{bias}^2$), and 
3. the estimation variance, which arises because the model is developed from a sample of the data ($\mathrm{Var} (\hat{f} (X))$).

While the goal of predictive modeling is to minimize EPE, the goal of explanatory modeling is to minimize bias.
</span>
</details> 

**Question: Shmeuli makes the case that predictive modeling requires a larger sample size than explanatory modeling. Why?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
In explanatory modeling, you only need enough data to "see whats going on" – in essence, you just need to understand the mean behavior. On the other hand, in predictive modeling, you actually want to be able to make good predictions, which means you need to understand the mean behavior AND the variability around the mean. Understanding the variability requires more data. Also, if the method for testing predictive models is to withhold some of the data at the model building stage, then you need more data.
</span>
</details> 

**Question: What is data partitioning? When/why is it used?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Data partitioning is when you partition the data into a "training" dataset and a "validation" dataset. The model is fit on the "training dataset" and its predictive accuracy tested on the "validation dataset". Data partitioning is less used in explanatory modeling because by reducing the sample size, it reduces the power of the test.
</span>
</details> 

**Question: How do explanatory and predictive models differ in their use of “dimension reduction”. (First, what is meant by “dimension reduction”.)**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Sometimes we want to combine a number of related predictors into a smaller set of predictors that are uncorrelated. The classic example of this would be Principal Components Analysis (PCA). Dimension reduction is often used for predictive modeling, because we can reduce the variance of our predictions by reducing the number of predictors used in the model. However, we do so at the expense of interpretability, since the new predictors are linear combinations of the original variables and can be difficult to interpret. (More on this in the last week of the semester.) The important point here is that in predictive modeling, we don’t care what goes into the model, only that what comes out of the model is optimal is terms of the accuracy of its predictions for new data.
</span>
</details> 

**Question: Shmeuli discusses “algorithmic modeling”. What is it?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
There are many data mining techniques which can produce predictive models that are useless for explanatory models. One example would be "neural networks". 
</span>
</details> 

**Question: What are ensemble methods?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Ensemble models exploit multiple models simultaneously to achieve predictive accuracy. These models might be completely incompatible (using different predictors for example) but their predictions can be merged into a distribution of predictions, the ensemble of which is better than any of the individual models. Examples include climate change models in which predictions from multiple climate models are combined, and hurricane models where you can look at multiple predicted storm tracks to get an idea of where the storm is most likely to go.

Shmueli distinguishes between three types of model checking:

"Model validation": How well does the model fit the underlying causal relationships?

"Model fit": How well does the model fit the data you have?

"Generalization": How well does the model fit new data?
</span>
</details> 

**Question: This brings us to the problem of overfitting – what is overfitting?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Overfitting is when models are fit to the noise in the data as well as the underlying "signal". This can happen when too many predictor variables are used and is the basis for the idea behind model "parsimony" (use the simplest model that works).
</span>
</details> 
  
Model selection depends on a clear idea of purpose – i.e. whether the model is explanatory or predictive. In explanatory models, predictors that have a theoretical justification may be left in the model even if they are not statistically significant because they are thought to be part of the true underlying model F. On the other hand, predictive models may drop predictors even if they are statistically significant, if their effect size is so small than leaving them out of the model actually increases predictive accuracy.

Shmeuli illustrates these ideas with two examples:

Example #1: The Netflix prize

*Question: What was the Netflix prize about?*

*Question: What were some of the main conclusions?*

1.	Missingness is predictively informative. In other words, it is important to know what makes a person decide to rate a movie at all.
2.	Data reduction was key, and many variables you would think would be useful in the model were not (i.e. which actors were actually in the movie).
3.	You can get better predictions by combining the predictions of several smaller models than by building a single more complex model (i.e. ensemble methods work)

Example #2: Online auctions.

*Question: What was the online auction example about?*

*Question: What were some of the main conclusions?*

1.	In this example, the goal was explanatory model, so covariates such as the number of bids were excluded because they arose from the bidding process and did not contribute to the bidding process (and therefore could not have a causal effect).

2.	R2 as a measure of explanatory power is used to compare models (this is the common metric of explanatory power, more on this next week)

3.	The authors retained insignificant variables because the model was built from theory and not from the data itself.

*Major conclusions:*

1.	Prediction, explanation, and description are three very different goals for a statistical analysis.
2.	R2 is a measure of explanatory power but does not tell you anything about predictive power because you may have fit the model to noise in the original sample. Also, explanatory models tend to be overly optimistic with regards to predictive power.
3. "Checking the model" can include three component: 1) Model validation: how well does the model fit the underlying causal mechanism?; 2) Model fit: how well does the model fit the data you have? (e.g., $R^2$); 3) Generalization: how well does the model fit new data? (not $R^2$).

*Final discussion questions:*

1. Must an explanatory model have some level of predictive power to be considered scientifically useful?
2. Must a predictive model have sufficient explanatory power to be scientifically useful?

Introduction to linear models
--------------
\subsection{Continuous covariates}

In the first half of the semester, we were discussing how to identify distributions, and how to fit distributions to data (i.e. parameter estimation). This is a very simple application of fitting a "statistical model".  For example, we would write

$$
Y \sim N(\mu,\sigma^{2})
$$
and then we would use MLE (or other methods) to estimate the two parameters of this distribution $\mu$ and $\sigma$. We can also re-write this model as

$$
Y = \mu + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2}) 
$$
Note that the equation on the left has an equal sign, since Y is strictly equal to the sum of $\mu$ and $\epsilon$ and it is $\epsilon$ that is drawn from a statistical distribution. In essence, we have decomposed Y into a component that is fixed ($\mu$) and a component that is stochastic ($\epsilon$). We read this equation as "Y is modeled as having a mean $\mu$ and a random error that is Normally distributed".

This illustrates nicely the general format of a linear model:

Response = Deterministic function + Stochastic function

Some authors will write this as:

Response = Model + Error

but I dislike this for two reasons. 1) Your "model" is not just the deterministic function. A model for your data includes the deterministic component and the stochastic component. 2) I don't like referring to the stochastic component as "error" because that implies that your model is faulty in some way. This stochasticity may be built into the system that you are trying to model, it may be irreducible variability that is not your "fault" and therefore should be considered a legitimate part of what you are trying to model rather than a mistake or an error.

Going back for a moment to our original model $Y \sim N(\mu,\sigma^{2})$, we can either leave $\mu$ as a constant to be estimated or we can try and improve our model by adding complexity to $\mu$. In other words, if Y represents the size of individuals in a collection of gentoo penguins, we might model mean size as being a function of a continuous variable like age. In that case, our model looks like:

$$
Y = \beta_{0}+\beta_{1}Age + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2})
$$

or, equivalently,

$$
Y \sim N(\beta_{0}+\beta_{1}Age,\sigma^{2})
$$
where I've used $\beta_{0}$ to represent the intercept of this linear model and $\beta_{1}$ to represent the slope (the amount by which Y increases for each year of Age).

We may choose to model $\mu$ as a function of a discrete variable like Colony. In this case, we would write the model as

$$
Y = \mu_{i} + \epsilon \mbox{ where } \epsilon \sim N(0,\sigma^{2})
$$
where each group has its own $\mu$:$\mu_{1}$,$\mu_{2}$,...,$\mu_{n}$.

When the covariates are continuous (the first case), we call this regression. When the covariates represent discrete groups or categories (the second case), we call this ANOVA. But both of these are simply different examples of linear modeling, which is it itself just a special case of fitting distributions, which is what we have been doing all along.

In this class, we are not defining linear models as models that can be described by a line. Instead, we define linear models as models in which the parameters can be arranged in a linear combination, and no parameter is a multiplier, divisor, or exponent to any other parameter. For example, $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon \text{, } \epsilon \sim N(0, \sigma^2)$, is a linear model because we can create a new variable $Z=X^{2}$, which makes it more obvious that Y is a linear function of X and this new variable Z. $Y = \beta_0 + \beta_1 X + \beta_1^2 X$ is also a linear model. An example of a nonlinearizable model is $Y = \frac{\beta_1 + \beta_2 X}{\beta_3 + \beta_4 X}$.

Linear models can be represented as a matrix equation, which we will illustrate first with an example of a linear model with a single continuous covariate.

Linear models | example with continuous covariate
---------------

Linear models can be written in matrix form (with vectors and matrices), which is more clear when you look at an example. For the model:

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
$$

We can expand this model for every data point ($i$), for a total of five data points:

$$
Y_1 = \beta_0 + \beta_1 X_1 + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
$$

$$
Y_2 = \beta_0 + \beta_1 X_2 + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
$$

$$
Y_3 = \beta_0 + \beta_1 X_3 + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
$$
$$
Y_4 = \beta_0 + \beta_1 X_4 + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
$$

$$
Y_5 = \beta_0 + \beta_1 X_5 + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
$$

Note that $\epsilon_{1}$, $\epsilon_{2}$,...,$\epsilon_{5}$ are i.i.d. draws from the same error distribution.

In matrix form the model can be written as:

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 & X_1 \\
  1 & X_2 \\
  1 & X_3 \\
  1 & X_4 \\
  1 & X_5
\end{bmatrix}
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}$$

where $Y$ is the response vector, $X$ is the design matrix, $\beta$ is the vector of parameters, and $\epsilon$ is the error vector. The design matrix tells us which combination of parameters are used to predict each data point. We can simply this notation even further by writing it as

$$
\overrightarrow{Y} = \overleftrightarrow{X}\overrightarrow{\alpha} + \overrightarrow{\epsilon}
$$

where $\overrightarrow{Y}$ is the response vector, $\overleftrightarrow{X}$ is design matrix, $\overrightarrow{\alpha}$ is the vector of parameters (coefficients), $\overrightarrow{\epsilon}$ is the vector of residuals (i.e. the remaining stochastic component). (Note: Keeping with traditional mathematical notation, vectors have a single headed arrow whereas as matrices have a double headed arrow. Also, here I use $\overrightarrow{\alpha}$ to represent the vector of model coefficients. More traditional would be to use the Greek letter $\overrightarrow{\beta}$, but in keeping with my notation above I will stick with $\overrightarrow{\alpha}$.)

The design matrix is the crucial element here. It tells us what combination of parameters are used to predict each data point (the Ys).

In the above example we had one continuous predictor, and the interpretation in this case is fairly straightforward. The interpretation and construction of the design matrix gets more complicated when we have discrete or categorical variables because there are many ways in which to parametrize the design matrix. 

\subsection{Discrete covariates}
Rewriting what we had above for a categorical predictor

$$
Y_i = \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
$$
where $\alpha_{j(i)}$ is the mean of the group $j$ to which data point $i$ belongs (before we had used the variable $\mu$ to represent the mean but here we will use $\alpha$ as the mean for each of the $j$ discrete groups). We can expand this model for every data point (let's say there are 5 data points and 3 groups):

$$
Y_1 = \alpha_{1(1)} + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
$$

$$
Y_2 = \alpha_{3(2)} + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
$$

$$
Y_3 = \alpha_{1(3)} + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
$$

$$
Y_4 = \alpha_{2(4)} + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
$$

$$
Y_5 = \alpha_{2(5)} + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
$$

Here, data point 1 is in group 1, 2 is in group 3, 3 is in group 1, 4 is in group 2, and 5 is in group 2. Both $Y_1$ and $Y_3$ are in group 1, for example, so will have the same predicted value. However, the error ($\epsilon$) for each is different.

This is called the "group means", "cell means", or "dummy" coding. Think of it like representing each group with a binary indicator telling you if each point is in the group (a "dummy" variable). 

We can illustrate dummy coding with an example:

```{r include=FALSE, echo=FALSE}
library(ggplot2)
library(plyr)
font.size <- 16
```

```{r}
iris.sub <- iris[sample(x = 1:nrow(iris), size = 12), ]
model.matrix( ~ -1 + iris.sub$Species)
iris.means <- ddply(iris.sub, .(Species), summarise, Sepal.Length = mean(Sepal.Length))
```

```{r, fig.width=5, fig.height=4}
dummy <- summary(lm(iris.sub$Sepal.Length ~ -1 + iris.sub$Species))
dummy$coefficients
iris.fig <- ggplot(iris.sub, aes(x = Species, y = Sepal.Length)) + geom_point() +
  geom_point(data = iris.means, aes(x = Species, y = Sepal.Length, col = Species), size = 3) + 
  labs(y = "Sepal length") + theme_classic() + theme(text = element_text(size = font.size))
iris.fig
```

Don't worry about the syntax, but look closely at the figure. The figure is illustrating the values for each group, and the red, green, and blue filled circles represent the mean within each group. Our simple "dummy coding" model simply states that each value (the individual black dots) can be modelled as the mean for that group PLUS the residual difference (i.e. $\epsilon$) between the group mean and that individual value. Note that the function we use to fit the linear model 'lm' is one that we haven't formally introduced yet (but that will come soon). For now, just observe how the output of the model represents the parameters we are interested in (in this case, the group means).

We write the equation in matrix form as

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 0 & 1 \\
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}$$

Like with the continuous example, $Y$ is the response vector of length $n$, the design matrix is an $n \times p$ matrix of ones and zeroes, the vector of parameters has length $p$, and $\epsilon$ is the error vector of length $n$.

Note that we can still write the equation in the compact form:

$$
\overrightarrow{Y} = \overleftrightarrow{X}\overrightarrow{\beta} + \overrightarrow{\epsilon}
$$

Notice that we have a system of equations, one for each group. (We may have several data points in each group – so our model wont fit each data point exactly; the error term takes care of this...) We also have three unknowns, which are the three group means. The three model parameters $\alpha_{1}$, $\alpha_{2}$, and $\alpha_{3}$ are all uniquely specified by the data.

However, there are other equivalent ways of writing the same model. For example, we could write

$$
Y_{i} \sim \bar{\alpha} + \alpha_{j(i)} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
$$

where **now $\bar{\alpha}$ represents the average of all the group means and $\alpha_{j(i)}$ is the DIFFERENCE between the mean of group j and the mean of all the groups**. (If the number of data points within each group are equal, than the mean of the group means is simply the mean of all the data, and we often refer to this as the 'grand' mean. However, if the groups are unbalanced, this may not be the case.) 

We can explicitly write out what this model means for each and every data point $Y_{i}$:

$$
Y_1 = \bar{\alpha} + \alpha_{1(1)} + \epsilon_1 \text{, where } \epsilon_1 \sim N(0, \sigma^2)
$$

$$
Y_2 = \bar{\alpha} + \alpha_{3(2)} + \epsilon_2 \text{, where } \epsilon_2 \sim N(0, \sigma^2)
$$

$$
Y_3 = \bar{\alpha} + \alpha_{1(3)} + \epsilon_3 \text{, where } \epsilon_3 \sim N(0, \sigma^2)
$$

$$
Y_4 = \bar{\alpha} + \alpha_{2(4)} + \epsilon_4 \text{, where } \epsilon_4 \sim N(0, \sigma^2)
$$

$$
Y_5 = \bar{\alpha} + \alpha_{2(5)} + \epsilon_5 \text{, where } \epsilon_5 \sim N(0, \sigma^2)
$$

We write the equation in matrix form as

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 \\
  Y_5
\end{bmatrix} =
\begin{bmatrix}
  1 & 1 & 0 & 0 \\
  1 & 0 & 0 & 1 \\
  1 & 1 & 0 & 0 \\
  1 & 0 & 1 & 0 \\
  1 & 0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
  \bar{\alpha} \\
  \alpha_1 \\
  \alpha_2 \\
  \alpha_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4 \\
  \epsilon_5
\end{bmatrix}$$

We have rewritten the model in terms of an overall mean (the mean of all the group means) and the effect of membership in each of the groups is added to this. Because the differences of each group to the overall mean sum to zero (do you see why this is?)

Notice that now we have introduced a fourth parameter without changing the data. We now have three group means and four parameters, so we cannot uniquely estimate all four parameters independently. If we estimate the overall global mean $\bar{\alpha}$, then we can only estimate two of the three group differences. The difference associated with the last group is completely constrained. This is analogous to our discussion of the number of degrees of freedom. We say that this model is **over parametrized**. We can solve this problem is a number of ways. We can remove a parameter altogether (in our first example, we implicitly removed the global mean $\bar{\alpha}$), or we can create new groups (linear combinations of the old groups) that are uniquely parameterized. Parameters that are not estimated because they are fully constrained by the other parameters are said to be **aliased**.

**Question:** What parameter was removed/aliased in the dummy coding scheme? In that case, $\bar{\alpha}$ was removed/aliased.

We have now introduced two perfectly "correct" ways of writing this linear model with different design matrices and correspondingly different interpretations of the model coefficients. These different ways of parametrizing the model for categorical variables are called "coding schemes". While the models represent the same math, the different ways of writing the models down permits different hypothesis tests about the model parameters.

In the case where we are looking at group means

$$
Y_{i} \sim \alpha_{j(i)} + \epsilon \mbox{, where } \epsilon \sim N(0,\sigma^{2})
$$
the (implicit) null hypothesis is that the group means are zero. Mathematically,

$$
H_{0}: \alpha_{j} = 0
$$

However, in the second case, 

$$
Y_{i} \sim \bar{\alpha} + \alpha_{j(i)} + \epsilon \mbox{, where } \epsilon \sim N(0,\sigma^{2})
$$
the same null hypothesis

$$
H_{0}: \alpha_{j} = 0
$$
means something very different. Therefore, we need to make sure to use coding schemes that will allow us to most directly test the hypothesis of interest. (This is why there is no one "correct" approach.)

Resolving overparameterization using contrasts
--------------

We can reduce the number of parameters in our model by creating a new set of parameters, each of which is a linear combination of groups, rather than a single group effect. Each parameter can include contributions from any number of the groups. We will now introduce of series of potential options for doing this, each of which is designed to test a specific hypothesis about how groups may differ. It is occassionally the case that none of these "off the shelf" options are appropriate for your analysis and you may need to design a design matrix that is tailored specifically for your research question, so the options we introduce here are just a few of the options and meant to illustrate the overall idea. The following decision tree is designed to help you choose the correct coding scheme for your specific analysis.

We know now that there are many ways in which to write a linear model with categorical predictors, and that the choices made about how to contrast the various levels of the factors is important for interpreting the results. In addition to the "dummy" or "cell means" coding scheme introduced above, we will go over 4 common "off-the-shelf" coding schemes that are used. Keep in mind that these are not all the possible coding schemes that could be used to write a model. Many others exist, and often it is necessary to write your own custom coding schemes to answer a particular question of interest. 

Before getting into the math that describes the various options, we'll work through a simple example. Lets say we have a study in which we measure the average lifespan of four species of penguins. For arguments sake, lets say we track 10 individuals in each of four species (40 penguins total). The mean lifespan in each group is 13, 12, 9, and 6. One way to write this model would be to simply estimate the mean of each group. This is the 'cell means' coding we described above. Nothing is being contrasted here. In other words, no group is being compared to any other group.

| Parameter          | Method$\#1$     | Method$\#2$    | Method$\#3$    |Method$\#4$  | 
| ------------------ |:---------------:| :------------: | :------------: |:-----------:|
| $\mu$              | aliased         |                |                |             |
| $\alpha_{1}$       | 13              |                |                |             |
| $\alpha_{2}$       | 12              |                |                |             |
| $\alpha_{3}$       | 9               |                |                |             |
| $\alpha_{4}$       | 6               |                |                |             |

Now let's say we want to write the model differently. Now we want to explicitly compare the lifespan of penguins in spp 2, 3, and 4 to the lifespan of penguins of spp 1. In other words, we are contrasting spp 2 vs. spp 1, spp 3 vs. spp 1, and spp 4 vs. spp 1. The estimates we get for these contrasts will allow us to test hypotheses about how each group compares to the first. 

| Parameter          | Method$\#1$     | Method$\#2$    | Method$\#3$    |Method$\#4$  | 
| ------------------ |:---------------:|:--------------:|:--------------:|:-----------:|
| $\mu$              | aliased         |  aliased       |                |             |
| $\alpha_{1}$       | 13              |  13            |                |             |
| $\alpha_{2}$       | 12              |  -1            |                |             |
| $\alpha_{3}$       | 9               |  -4            |                |             |
| $\alpha_{4}$       | 6               |  -7            |                |             |

Let's try another set of contrasts. In this case, let's say that, for whatever reason, we want to compare the Mean(spp 1,spp 2) vs. Mean(spp 1), as well as Mean(spp 1, spp 2, spp 3) vs.  Mean(spp 1,spp 2), and the Mean(spp 1, spp 2, spp 3, spp 4) vs. Mean(spp 1, spp 2, spp 3). That would yield the following parameter estimates. (Note that in this case, we have pulled out the group mean as a parameter, and the mean of spp 1 has been aliased, so its not being estimated.)

| Parameter          | Method$\#1$     | Method$\#2$    | Method$\#3$    |Method$\#4$  | 
| ------------------ |:---------------:|:--------------:|:--------------:|:-----------:|
| $\mu$              | aliased         |  aliased       |    10          |             |
| $\alpha_{1}$       | 13              |  13            |   aliased      |             |
| $\alpha_{2}$       | 12              |  -1            |   -0.5         |             |
| $\alpha_{3}$       | 9               |  -4            |   -1.1667      |             |
| $\alpha_{4}$       | 6               |  -7            |   -1.3333      |             |

Finally, imagine we want to compare each group to the grand mean ($\mu$). In this case we pull out the $\mu$ and estimate it. Than we compare spp 1 vs. $\mu$, and spp 2 vs. $\mu$, and spp 3 vs. $\mu$. We can't also do spp 4 vs. $\mu$ because once we've calculated the other four quantities ($\mu$, and the three contrasts with $\mu$), the final quantity (spp 4 vs. $\mu$) is completely determined and therefore is aliased.

| Parameter          | Method$\#1$     | Method$\#2$    | Method$\#3$    |Method$\#4$  | 
| ------------------ |:---------------:|:--------------:|:--------------:|:-----------:|
| $\mu$              | aliased         |  aliased       |    10          |     10      |
| $\alpha_{1}$       | 13              |  13            |   aliased      |      3      |
| $\alpha_{2}$       | 12              |  -1            |   -0.5         |      2      |
| $\alpha_{3}$       | 9               |  -4            |   -1.1667      |     -1      |
| $\alpha_{4}$       | 6               |  -7            |   -1.3333      |    aliased  |

As a review, the first way of writing the model does not contrast anything. It simply estimates the group means, and the model for group $j$ is simply the group mean for $j$ plus an error term. The other three model parameterizations explicitly contrast the groups, but in different ways.

Hopefully, by working through a few examples, you can see that there are many ways we can write a model, and that each way contrasts different things and allows for an explicit hypothesis test on differences that are of interest. In this example, Method #1 is the 'cell means' coding that we started with, Method #2 is called 'effect coding' or 'treatment contrasts', Method #3 is called Helmert contrasts, and Method #4 is called the 'sum-to-zero' contrast we already briefly introduced. We'll walk through a more formal introduction of these contrasts now.

Effect coding/Treatment constrast
------------

In effect coding or treatment contrasts, you set aside one group as the "control" and estimate the difference of every other group from that control group. The estimated parameters therefore represent the "effect" of moving from the control group to any other group (which often represents the impact of a "treatment" relative to the control group).

$$
Y_i = \alpha_{\text{control}} + \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
$$

where $\alpha_{control}$ is the mean of the control group, and $\alpha_{j>1}$ is the difference between group $j$ and the control group. If we have $k$ groups, we only have $k-1$ $\alpha_{j}$ to estimate since we use one group as the control. We can think of $\alpha_{control}$ as an intercept.

We can summary this as

| Parameter          | Interpretation           | Null hypothesis  |
| ------------------ |:------------------------:| ----------------:|
| $\alpha_{control}$ | mean of control group ($\mu_1$) | $H_0: \mu_1 = 0$ |
| $\alpha_2$         | difference between mean of group 2 and mean of control group ($\mu_2 - \mu_1$) | $H_0: \mu_2 - \mu_1 = 0$ |
| $\alpha_3$         | difference between mean of group 3 and mean of control group ($\mu_3 - \mu_1$) |    $H_0: \mu_3 - \mu_1 = 0$ |


This basically uses the first group as the "intercept" of the model, so instead of this

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 
\end{bmatrix} =
\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
  0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
  \mu_1 \\
  \mu_2 \\
  \mu_3
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}$$

we have

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 
\end{bmatrix} =
\begin{bmatrix}
  1 & 0 & 0 \\
  1 & 1 & 0 \\
  1 & 0 & 1 \\
  1 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
  \alpha_{control} = \mu_1 \\
  \alpha_{2} = \mu_2 - \mu_{1}\\
  \alpha_{3} = \mu_3 - \mu_{1}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}$$

Note that the last two data points come from the same group, and so those two rows will always be the same. In theory, there are many many rows NOT shown.

The first version specifies the model in terms of the group means. The second version specifies the model in terms of linear combinations of the group means. The latter approach permits a straightforward way of estimating quantities of biological interest (usually we are interested in testing hypotheses about differences).

```{r, fig.width=4, fig.height=3}
contrasts(iris.sub$Species) <- "contr.treatment"
model.matrix(~ iris.sub$Species)
```

```{r}
treatment <- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species))
treatment$coefficients
```

In this case, you can see that the model is estimating the mean of the first group (setosa) and then the difference between the second and first group and the difference between the third and first groups. This allows you to test hypotheses about the differences, which is often more meaningful than testing hypotheses about the group means themselves.

Helmert contrasts
---------------

Helmert contrasts include a mean treatment effect and then the remaining parameters estimate the group mean relative to the mean treatment effect of the groups before it.

$$
Y_i = \bar{\alpha}_{< j} + \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
$$
where $\alpha_{mean}$ is the mean treatment effect for all groups prior to group $j$, and $\alpha_{j}$ is the difference between group $j$ and $\alpha_{mean}$. This is usually only sensible when the categories have some natural order to them, and are not very common in ecology but they are the default in SPLUS (but not R fortunately). (There are many different versions, but as long as you keep the interpretation of each coefficient straight, it doesn’t matter which you use. I think this version is the clearest.)

| Parameter          | Interpretation           | Null hypothesis  |
| ------------------ |:------------------------:| ----------------:|
| $\bar{\alpha}$ | mean of group means | $H_0: \frac{\mu_1 + \mu_2 + \mu_3}{3} = 0$ |
| $\alpha_2$         | difference between mean of groups 1 and 2 and mean of group 1 | $H_0: \frac{\mu_1 + \mu_2}{2} - \mu_1 = 0$ |
| $\alpha_3$         | difference between mean of groups 1-3 and mean of groups 1 and 2 |    $H_0: \frac{\mu_1 + \mu_2 + \mu_3}{3} - \frac{\mu_1 + \mu_2}{2} = 0$ |

This can be expressed as:

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 
\end{bmatrix} =
\begin{bmatrix}
  1 & -1 & -1 \\
  1 & 1 & -1 \\
  1 & 0 & 2 \\
  1 & 0 & 2 
\end{bmatrix}
\begin{bmatrix}
  \alpha_{\small{\mbox{mean of group means}}} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3} \\
  \alpha_{2} = \frac{\mu_{1}+\mu_{2}}{2}-\mu_{1} \\
  \alpha_{3} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3}-\frac{\mu_{1}+\mu_{2}}{2}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}$$

where I have assumed it is $\alpha_{1}$ that is aliased (although different books define this differently, so you may find versions in which it is parameterized differently).

```{r}
contrasts(iris.sub$Species) <- "contr.helmert"
model.matrix(~ iris.sub$Species)
```

```{r}
helmert <- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species))
helmert$coefficients
```

Sum-to-zero contrasts
--------------

Sum-to-zero contrast set the intercept to the mean treatment effect (the mean of the group means) and then the remaining parameters estimate the mean treatment effect of that group relative to the mean treatment effect. (We introduced this contrast briefly earlier.)

$$
Y_i = \bar{\alpha} + \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)
$$

where $\bar{\alpha}$ is the mean treatment effect for all groups, and $\alpha_{j}$ is the difference between group $j$ and $\bar{\alpha}$. 

We can summarize this as

| Parameter          | Interpretation           | Null hypothesis  |
| ------------------ |:------------------------:| ----------------:|
| $\bar{\alpha}$ | mean of group means | $H_0: \frac{\mu_q}{p} = \frac{\mu_1 + \mu_2 + \mu_3}{3} = 0$ |
| $\alpha_1$         | difference between mean of group 1 and mean of group means | $H_0: \mu_1 - \frac{\mu_q}{p} = 0$ |
| $\alpha_2$         | difference between mean of group 2 and mean of group means |    $H_0: \mu_2 - \frac{\mu_q}{p} = 0$ |

This can be expressed as:

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 
\end{bmatrix} =
\begin{bmatrix}
  1 & 1 & 0 \\
  1 & 0 & 1 \\
  1 & -1 & -1 \\
  1 & -1 & -1 
\end{bmatrix}
\begin{bmatrix}
  \bar{\alpha} = \frac{\mu_{q}}{p} = \frac{\mu_{1}+\mu_{2}+\mu_{3}}{3} \\
  \alpha_{1} = \mu_{1}-\frac{\mu_{q}}{p} \\
  \alpha_{2} = \mu_{2}-\frac{\mu_{q}}{p}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}$$

where we see that it is the last parameter ($\alpha_{3}$), the one associated with the group mean of the last group ($\mu_{3}$) which is aliased.

```{r}
contrasts(iris.sub$Species) <- "contr.sum"
model.matrix(~ iris.sub$Species)
```

```{r}
sumtozero <- summary(lm(iris.sub$Sepal.Length ~ iris.sub$Species))
sumtozero$coefficients
```

There is one final "off-the-shelf" contrast that we will learn, and that is polynomial contrasts.

Polynomial contrasts
------------

Polynomial contrasts simply involve fitting polynomial terms of increasing order; polynomial contrasts only make sense if the covariate represents an ordinal value, so that the different levels of the factor are ordered and have equal spacing (income, years of education, etc.). This is really just a form of multiple regression (so a preview of what we will be doing in a couple of weeks…)

This one is a little hard to summarize in an equation, but is easily understood by example.

| Parameter          | Interpretation           | Null hypothesis  |
| ------------------ |:------------------------:| ----------------:|
| $\beta_0$ | Y intercept | $H_0: \beta_0 = 0$ |
| $\beta_1$         | Partial slope for the linear term | $H_0: \beta_1 = 0$ |
| $\beta_2$         | Partial slope for the quadratic term |    $H_0: \beta_2 = 0$ |

This can be expressed as:

$$\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  Y_3 \\
  Y_4 
\end{bmatrix} =
\begin{bmatrix}
  0.58 & -0.71 & 0.41 \\
  0.58 & 0 & -0.82 \\
  0.58 & 0.71 & 0.41 \\
  0.58 & 0.71 & 0.41 
\end{bmatrix}
\begin{bmatrix}
  \beta_{0} = \mbox{mean} \\
  \beta_{1} = \mbox{coefficient for the linear term} \\
  \beta_{2} = \mbox{coefficient for the quadratic term}
\end{bmatrix} +
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \epsilon_3 \\
  \epsilon_4
\end{bmatrix}$$


where the coefficients ($\beta$) can be calculated using the values in the contrast matrix (we will call this $\mathbf{P}$) associated with each group. Data point 1 is in group 1, 2 in group 3, 3 in group 1, 4 in group 2, and 5 in group 2.

$$
\beta_{k - 1} = \frac{\sum_{j} P_{kj} \bar{Y}_j}{\sum_{j} P_{kj}^2}
$$

$$
\beta_0 = \frac{1 \times \bar{Y}_{grp 1} + 1 \times \bar{Y}_{grp 2} + 1 \times \bar{Y}_{grp 3}}{1^2 + 1^2 + 1^2}
$$

$$
\beta_1 = \frac{-0.71 \times \bar{Y}_{grp 1} + 0 \times \bar{Y}_{grp 2} + 0.71 \times \bar{Y}_{grp 3}}{(-0.71)^2 + 0^2 + 0.71^2}
$$
and so forth...

The values in the design matrix look complicated, but they aren’t really. Each column is scaled to have length 1 (which means all the denominators are actually just 1).

Visualizing hypotheses for different coding schemes
-------------------

The different contrasts are different ways of comparing groups to different baselines. You can either compare each group to zero, a control group, to the mean of the group means (often called the grand mean), to the average of all previous groups, etc.

```{r, fig.width=6, fig.height=6}
baselines <- data.frame(Baseline = c("Mean of group means", "Control", "Zero"), Value = c(mean(iris.means$Sepal.Length), iris.means$Sepal.Length[1], 0))
iris.fig.baselines <- ggplot(iris.sub, aes(x = Species, y = Sepal.Length)) + geom_point() +
  geom_hline(data = baselines, aes(yintercept = Value), col = "gray", linetype = "dashed") +
  geom_text(data = baselines, aes(label = Baseline, x = Inf, y = Value), hjust = 1.0, vjust = 1.0, size = 5) +
  geom_point(data = iris.means, aes(x = Species, y = Sepal.Length, col = Species), size = 3) + 
  labs(y = "Sepal length") + theme_classic() + theme(text = element_text(size = font.size))
iris.fig.baselines
```

Orthogonal vs. Non-orthogonal contrasts
---------------------


```{r echo=FALSE, fig.align='center', fig.cap='The segment AB is orthogonal to the segement CD. Source: Wikipedia', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/perpendicular.png')
```

In a broad sense, orthogonal refers to things that are perpendicular. For some cases, that's easy to think about. But what does it mean with respect to categorical comparisons in linear models? 

In a simple model with one categorical predictor with five different groups (a, b, c, d, and e): $Y_i = \alpha_{j(i)} + \epsilon_i \text{, where } \epsilon_i \sim N(0, \sigma^2)$, we may be interested in comparing a vs. b, a vs. c, or even a, b, and c vs. d and e. There are many ways in which we might want to interpret differences among the groups, and each comparison is called a **contrast**. There are a large number of contrasts, but few *orthogonal* contrasts. For example, if we compare a vs. b and a vs. c, then a third comparison, b vs. c is not orthogonal because it is implicity included in the first two contrasts. Note that treatment contrasts are not orthogonal.

**If you have $k$ levels of a factor ($k$ numbers of groups), you only have $k - 1$ orthogonal contrasts.**

Why does this matter? With non-orthogonal contrasts, the order in which you make you comparisons in an Analysis of Variance will change your results and interpretations. See Aho section 10.3.2 for details.

Error structure of linear models
----------------

Response = Deterministic function + Stochastic function

Only part of a statistical model is deterministic. It's important to understand whether the stochastic part of the model is properly specified.

We have been referring to "error" as being normally distributed, $\epsilon \sim N(0, \sigma^2)$, and this is often assumed for linear models (including regression). 

Imagine you have modeled the number of plants in quadrats. Is your error normally distributed?

Other models for error can be more appropriate, depending on the data and the model, e.g., $\epsilon \sim \text{Bernoulli}()$ or $\epsilon \sim \text{Poisson}()$.

\subsection{Independence of errors}

Everything to this point has assumed that the residuals for each data point (i.e. $\epsilon_{i}$) are independent from one another and identifcally distributed (as $N(0,\sigma^{2}$)). 

$$
Y_i = \alpha_{j(i)} + \epsilon_i \text{, where }\epsilon_i \sim N(0, \sigma^2 \mathbf{I})
$$

$$
\sigma^2 \mathbf{I} = \sigma^2 \begin{bmatrix}
  1 & 0 & 0 & 0 & 0 \\
  0 & 1 & 0 & 0 & 0 \\
  0 & 0 & 1 & 0 & 0 \\
  0 & 0 & 0 & 1 & 0 \\
  0 & 0 & 0 & 0 & 1
\end{bmatrix} = \begin{bmatrix}
  \sigma^2 & 0 & 0 & 0 & 0 \\
  0 & \sigma^2 & 0 & 0 & 0 \\
  0 & 0 & \sigma^2 & 0 & 0 \\
  0 & 0 & 0 & \sigma^2 & 0 \\
  0 & 0 & 0 & 0 & \sigma^2
\end{bmatrix}
$$

Diagonal elements represent the $\mathrm{Var}[Y_i]$ and the off-diagonal elements are $\mathrm{Cov}[Y_i, Y_j]$. Here, $\mathrm{Cov}[Y_i, Y_j] = 0$, which means that the error for data point $Y_i$ is independent of the error for data point $Y_j$. 

But, this is not always the case. Sometimes, a positive error/residual for one data point is coupled with a positive residual for another data point, or vice versa. This happens often in ecology and evolution, because we often work with data that has spatial dependence, temporal dependence, or phylogenetic dependence. Then, the error matrix is no longer diagonal, and may look like:

$$
\epsilon_i \sim N(0, \mathbf{\Sigma}) \text{, where } \mathbf{\Sigma} = \begin{bmatrix}
  4.5 & 0.3 & -4.7 & 0 & 0 \\
  0.3 & 9.0 & 0 & 0 & 0 \\
  -4.7 & 0 & 0.4 & 0.3 & 0 \\
  0 & 0 & 0.3 & 5.2 & 0.1 \\
  0 & 0 & 0 & 0.1 & 0.8
\end{bmatrix}
$$

The matrix is symmetric, because $\mathrm{Cov}[Y_i, Y_j] = \mathrm{Cov}[Y_j, Y_i]$. Thus, it is important that we think of the error term as a matrix that describes the correlation structure of the data. Correlated errors will affect model fit. This is important because we want to downweight information from highly correlated data points because the information that each contributes is not independent from one another.

Week 9 Lecture
=============

```{r include=FALSE, warning=FALSE}
library(ggplot2)
library(asbio)

text.size <- 14
```

##Correlation

Correlation and regression are two ways of looking at the relationship between two continuous variables. Correlation is testing the null hypothesis that the two variables are uncorrelated, and there is complete symmetry between the two variables. On the other hand, regression is concerned with using one variable to PREDICT another, and there is an asymmetry between the predictor and the response variable. A regression model for $Y \sim X$ is different than $X \sim Y$.

Regression and correlation are not the same.

Let’s take two paired samples A and B and introduce a quantity called the correlation coefficient r:

$$
r = \frac{Cov(A,B)}{\sqrt{Var(A) \times Var(B)}}
$$

More precisely, this is called “Pearson’s product moment correlation”.

We know from before that the sample variance of A (a.k.a. Var(A)) is given by

$$
Var(A) = \frac{1}{n-1}\sum_{i=1}^{n}(A_{i}-\bar{A})(A_{i}-\bar{A})
$$
Sample variance is the squared deviation of a random variable from its expected value, scaled by the number of data points. When we use squared deviations, our values are always positive.

Let's say $A_1 = 12$ and $\bar{A} = 14$. The squared deviation is then $(12 - 14)^2 = 4$. Now let's say $A_2 = 30$. The squared deviation is then $(30 - 14)^2 = 256$. When using squared deviations, values far away from the mean contribute greatly to the variance (they have a large influence).

By extension, the sample covariance of A and B (i.e. Cov(A,B)) is given by

$$
Cov(A,B) = \frac{1}{n-1}\sum_{i=1}^{n}(A_{i}-\bar{A})(B_{i}-\bar{B})
$$

Why does this expression make sense? The covariance measures how much two things go up and down together. 

```{r}
A <- c(1, 2, 3, 4)
B <- c(5, 6, 7, 8)
(1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) 
B <- c(5, 4, 3, 2)
(1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B)))
```

However, the covariance isn’t meaningful by itself because it depends on the absolute scale of variability in A and B, so it has to be divided by some aggregate measure of variability in the two samples (i.e. the geometric mean). We can see this by scaling everything by a factor of 10 and recalculating Covariance:

```{r}
(1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B)))
A <- A * 10
B <- B * 10
(1 / (length(A) - 1)) * sum((A - mean(A)) * (B - mean(B))) 
```

This is why to calculate $r$ we scale the covariance by the geometric mean of the product of the variances for the two samples. 

$$
r = \frac{Cov(A,B)}{\sqrt{Var(A) \times Var(B)}}
$$
Pearson's product moment correlation is bounded between -1 and 1. Correlation coefficients of 1 or -1 occur when you set deviations in sample $B$ to be exactly matched in sample $A$. These measures are unitless. 

Let's look at a few examples...

```{r fig.width=2.5, fig.height=2.5, echo=FALSE}
df <- data.frame(x = rnorm(n = 100, mean = 10, sd = 5), y = rnorm(n = 100, mean = 10, sd = 5))
ggplot(data = df, aes(x = x, y = y)) + geom_point() + theme_classic() + theme(text = element_text(size = text.size))
cor(df$x, df$y)

df <- data.frame(x = df$x, y = df$x + rnorm(n = 100, mean = 0, sd = 2))
ggplot(data = df, aes(x = x, y = y)) + geom_point() + theme_classic() + theme(text = element_text(size = text.size))
cor(df$x, df$y)
```

## Hypothesis testing - Pearson's *r*

Before getting into hypothesis testing, some terminology:

$\rho$=true (population) correlation coefficient of A and B

$r$=empirical (sample) correlation coefficient of A and B

How can we test hypotheses about correlation?

1. Parametric approach using distribution theory

2. Nonparametric approaches: bootstrap or randomization

We'll start with 1. If $\rho$ is the population (true) correlation coefficient of $A$ and $B$ and $r$ is the sample (empirical) correlation coefficient of $A$ and $B$, we will test whether $\rho$ is significantly different from zero.

How do we assess whether our estimate of $\rho$ (which, following our notation from earlier in the semester, we will call $\hat{\rho}$) is significantly different from zero? 

Note before we begin that calculating Pearson's correlation coefficient itself does not require that the data (what we've been calling *A* and *B*) are bivariate normal, but hypothesis testing/constructing CIs based on distribution theory does.

If $A$ and $B$ are from a bivariate normal distribution and $\rho = 0$, the sample correlation coefficient is normally distributed (for large sample sizes):

$$
r | (H_0: \rho = 0) \sim \mathrm{N} {\left( 0, \frac{1 - r^2}{n - 2} \right)}
$$

and therefore we use the following test statistic:

Then we convert this to the following test statistic: 
$$
T^* = r \sqrt{\frac{n - 2}{1 - r^2}} | (H_0: \rho = 0) \sim \mathrm{N}(0, 1)
$$

Actually, it is more precise to say that under the null hypothesis, 

$$
T^* = r \sqrt{\frac{n - 2}{1 - r^2}} | (H_0: \rho = 0) \sim t_{n-2}
$$

**Question: How did we generate this test statistic, $T^*$ from the original test statistic, *r*?**

By standardizing the distribution of $r | H_0$ so that the normal distribution has a standard deviation/variance of 1

**Question: Why did we use this test statistic instead of *r*?**

The standard normal distribution and the t-distribution are conventional distributions with associated tables of quantiles that were used before computing was as powerful and widespread. Also, the t-distribution is more appropriate for small sample sizes (it has fatter tails).

**Question: Where did we lose two degrees of freedom?**

To estimate *r*, we calculated the mean of sample A and sample B

The Pearson's product moment correlation assumes the following about the two samples:

1. The joint distribution $(A, B)$ is bivariate normal (necessary for hypothesis testing using distribution theory)

2. The relationship between $A$ and $B$ is linear (always necessary)

3. Data are independent samples from the joint distribution.

```{r, fig.width=3, fig.height=3}
library(ggplot2)
A <- rnorm(n = 100, mean = 5, sd = 2)
A <- A[order(A)]
df <- data.frame(A = A, B = (A - 5)^2 + rnorm(n = 100, mean = 0, sd = 1))
ggplot(data = df, aes(x = A, y = B)) + geom_point() + theme_classic() + theme(text = element_text(size = text.size))
```

```{r, fig.width=3, fig.height=3}
cor(df$A, df$B)
```

Even though *A* and *B* clearly have a strong relationship, correlation is only effective if the relationship is **linear**.

Other considerations:

What if the data are not bivariate normal? If sample size is large, you're probably okay.

Pearson's $r$ is also very sensitive to outliers, what can you do in that case? Robust correlation measures (Spearman's or Kendall's $r$) are robust to deviations from bivariate normality and outliers. Confidence intervals on $r$ can also be derived using bootstrap and permutation methods. As these are far more intuitive, require no assumptions, and (with modern computers) are fast, it is often better to use these other methods (which may be slightly less powerful, another topic to be covered later).

**Question: How could you bootstrap the correlation coefficient given that the samples (*A* and *B*) are paired?**

<details>
  <summary>Click for Answer</summary>
  <span style="color: blueviolet;">You need to sample (with replacement) A-B pairs. This will preserve the correlated structure of the data while allowing you to resample a "new" dataset with which to calculate a confidence interval. In other words, sample with replacement from the **row indices**, preserving the relationship between the two samples

```{r}
iterations <- 1000
A <- rnorm(n = 100, mean = 5, sd = 2)
B <- A - rnorm(n = 100, mean = 0, sd = 1)
cor.obs <- cor(A, B)
dat <- data.frame(A, B)
cor.boot <- c()
for (i in 1:iterations) {
  index.boot <- sample(1:nrow(dat), size = 100, replace = TRUE)
  dat.boot <- dat[index.boot, ]
  cor.boot[i] <- cor(x = dat.boot$A, y = dat.boot$B)
}
paste("The 95% confidence interval for the estimated correlation coefficient, ", round(cor.obs, digits = 3), " is (", round(quantile(cor.boot, 0.025), digits = 3), ", ", round(quantile(cor.boot, 0.975), digits = 3), ")", sep = "")
```
</span>
</details> 

**Question: How would we conduct a permutation test for the correlation coefficient (with $H_0: \rho = 0$)?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Shuffle *A* independently of *B* and calculate the correlation of the shuffled data to create a null distribution of correlations. Compare the observed correlation to this distribution.
</span>
</details> 

## Fisher's $z$

If you have large sample sizes, you can test other null hypotheses (i.e. $H_{0}: \rho=\rho_{0}$) and put confidence intervals on $\rho$ using what is called “Fisher’s transformation”. Using Fisher's $z$ transformation, we convert a statistic with a bounded distribution to an unbounded (normal) distribution. This is useful for calculating confidence intervals. Also with this method, we can test hypotheses other than $\rho = 0$. We can transform $r$ to $z$ using the the inverse hyperbolic tangent:

$$
z = 0.5 \ln \left( \frac{1 + r}{1 - r} \right) = \tanh^{-1} (r)
$$

With this transformation, $z$ is approximately normally distributed for all values of $\rho$:

$$
z \sim \mathrm{N} {\left( 0.5 \ln \left( \frac{1 + \rho}{1 - \rho} \right), \frac{1}{n - 3} \right)}
$$
Now, the test statistic that we can use to test the null hypothesis $H_0: \rho = \rho_0$ is:

$$
T^* = \frac{z - 0.5 \ln \left( \frac{1 + \rho_0}{1 - \rho_0} \right)}{\sqrt{1 / (n - 3)}} \approx \mathrm{N}(0, 1)
$$

Why did we rewrite a new test statistic (what was wrong with $z$)? The null distribution for $z$ is more difficult to reference (compared to the quantiles of the standard normal, for example). For a bit of evidence showing that both ways of writing the test statistic are fine, see the following code:

```{r}
A <- rnorm(n = 100, mean = 5, sd = 2)
B <- A - rnorm(n = 100, mean = 0, sd = 1)
r.obs <- cor(A, B)
# using z as test statistic
z.obs <- 0.5 * log((1 + r.obs) / (1 - r.obs)) # note that z under the null distribution is 0
pnorm(q = z.obs, mean = 0, sd = sqrt(1 / (100 - 3)), lower.tail = FALSE)
# using transformed test statistic, T*
test.stat <- z.obs / sqrt(1 / (100 - 3))
pnorm(q = test.stat, lower.tail = FALSE)
```

To simply the notation, let us define

$$
\zeta_{0} = \frac{1}{2}ln\left(\frac{1+\rho_{0}}{1-\rho_{0}}\right)
$$

We can also use this to construct confidence intervals for $\zeta_{0}$:

$$
P \left( z - \frac{t_{[1 - \alpha / 2](\infty)}}{\sqrt{n - 3}} \leq \zeta_{0} \leq z + \frac{t_{[1 - \alpha / 2](\infty)}}{\sqrt{n - 3}} \right) = 1 - \alpha
$$

(Note that we need to do some back-transforming to get from here to a confidence interval on $\rho$.)

Implicit in the calculation of the correlation coefficient is that the quantities being compared are either interval or ratio variables. If this is not the case, then you need to use a more general statistical test which uses only the ranks of the data. The underlying principle is that you rank the data in each sample and then compare ranks. There are two common ones:

1.	Spearmans rank correlation: Similar to above except instead of using raw values, the data are first transformed into ranks (separately for each sample) before calculating $r$.

$$
r_{s} = \frac{Cov(ranks_A,ranks_B)}{\sqrt{Var(ranks_A) \times Var(ranks_B)}}
$$

where $ranks_A$ and $ranks_B$ are the ranks of the two datasets.

$$
r_s | H_0 \sim \sqrt{\frac{1}{n - 1}} \mathrm{N} (0, 1)
$$
Ties are dealt with in a different way (see Aho 8.3.1.1 for more detail).

```{r}
A <- c(2, 3, 5, 4)
B <- c(5, 6, 2, 3)
cov(rank(A), rank(B)) / sqrt(var(rank(A)) * var(rank(B)))
cor.test(A, B, method = "spearman")
```

Q: What information is lost when using Spearman's rather than Pearson's $r$?

2.	Kendall’s $\tau$ or “Kendall’s coefficient of rank correlation”: 

The basic idea is that for sample X, you can compare all possible combinations of $X_{i}$ to see which values are larger. For a second sample Y, you can do the same. Kendall’s tau tallies how many pairs share the same relationship. If ($X_{i}$> $X_{j}$ and $Y_{i} > Y_{j}$) OR ($X_{i}< X_{j}$ and $Y_{i}< Y_{j}$), then X and Y are considered concordant for that combination. If ($X_{i}> X_{j}$ and $Y_{i}< Y_{j}$) OR ($X_{i}< X_{j}$ and $Y_{i}> Y_{j}$), then X and Y are considered disconcordant for that combination.

This is easier to describe in practice:

```{r}
set.seed(4306)
total <- sample(x = 1:15, size = 8)
dat <- data.frame(A = total[1:4], B = total[5:8])
dat
```

$$
\tau = \frac{\#\text{concordant pairs} - \#\text{discordant pairs}}{0.5 n (n - 1)}
$$
where the denominator is simply the number of possible combinations.

Once we calculate $\tau$, how do we know if that value is significant or not? One method would be to do a randomization test. We can also use the following approximation for the test statistic for large (n>10) sample sizes:

$$
\tau | H_0 \sim \mathrm{N} \left( 0, \frac{2(2n + 5)}{9n (n - 1)} \right)
$$
In other words, 

$$
\frac{\tau}{\sqrt{\frac{2(2n + 5)}{9n (n - 1)}}} \sim N(0,1)
$$
so we can use the quantiles of the standard normal we are so familiar with. In practice, Pearson’s and Kendall’s rank correlation tests often give very very similar results (if not in the actual value, certainly in the inference derived from them).

## Regression

Regression is a linear model (in the vein of those we introduced last week) in which a continuous response is modeled by one or more continuous predictors. (If we have only discrete predictors we call this ANOVA...the names are silly since they are all just linear models.)

Linear regression simply finds the straight line that “best fits” a scatterplot of (x,y) data.
We will note at the outset that there are two distinct kinds of regression. 

“Type I” regression assumes that the purpose of the analysis is to use x to predict y. Therefore, we assume x is without uncertainty and we want to minimize the error with which we predict y given x.

“Type II” regression, or major axis regression (or standardized major axis regression), assumes a symmetry between X and Y and that the goal is in finding a correlation between them. Type II regression is often used when X (as well as Y) is measured with error, although see Smith (2009) for a full discussion of this interpretation.

We will start by talking about “Type I” regression which is much more common, and come back to discussing Type II regression later. I have assigned a reading for this week that goes over all of this is some more detail. Before discussing regression, it is important to note that there is a distinction between **regression** and **correlation**. In regression, you are using one or more variables to predict another variable because you believe there is a cause-and-effect relationship (X causes Y). Correlation does not imply the same cause-and-effect type relationship, it just addresses whether two variables are associated with one another (X and Y covary).

**Remember correlation does not imply causation. To address causation, you usually need to do some kind of a manipulative experiment.**

As a first start, we will discuss linear regression (We introduced this model briefly when we introduced linear models last week.)

$$
Y_{i} = \beta_{0}+\beta_{1}X_{i} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
$$

where $\beta_{0}$ is the **intercept** and $\beta_{1}$ is the **slope**. Note that each data point $Y_{i}$ is associted with its own value of the corresponding covariate predictor ($X_{i}$) and its own sample for the residual $\epsilon_{i}$. Before we get into some more math, let's just draw some data and visualize what the slope and intercept represent. Note that we are usually just interested in the line ($\beta_{0}+\beta_{1}X$) that represents the effect of the covariate on the response variable and therefore our focus is on estimating the parameters $\beta_{0}$ and $\beta_{1}$. We are often less interested in the variance associated with $\epsilon$ but keep in mind that $\sigma$ is a third parameter of this model and also must be estimated from the data.

How do we find the line that best fits the data? It probably makes intuitive sense that the mean value of X should be associated with the mean value of Y and this is in fact true. The best fit line passes through ($\bar{X}$,$\bar{Y}$). \textcolor{red}{What are all the possible lines that could pass through this point?}

First, we will go over the notation for linear models (and regression specifically) we'll be using for the next couple of weeks.

| Notation | Meaning        |
| ----------- |:--------------------------------------------:|
| $Y$ | data/response variable/dependent variable | 
| $Y_i$ | value of $i^\text{th}$ data point | 
| $\bar{Y}$ | mean of data | 
| $\hat{Y}_i$ | estimate from the model for point $i$. In regression, this is $\hat{Y}_i = \beta_0 + \beta_1 X_i$ |

To make any further progress with this question, we need to “partition the variance”. We will not tackle this seriously until we get to ANOVA next week, but the basic idea is fairly straightforward.

![](Regression.png)

To determine how to specifically fit slope and intercept parameters, we need to talk about partitioning variance (a goal of modeling in general, and a major part of ANOVA). When modeling, variation is either explained or unexplained. 
Let's start from the beginning with this dataset of the relative length of spider webs. Of course, our data are not all exactly the same value, there is some variation:

```{r, fig.width=3, fig.height=3}
data(webs)
ggplot(data = webs, aes(x = 1, y = length)) + geom_point(col = "gray37") + labs(x = "", y = "Relative length of spider webs") + theme_classic() +  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = text.size))
```

Next, we fit a model with two parameters, one parameter to describe the mean behavior of the system and one parameter to describe the amount of variation around that mean, e.g., $Y \sim \mathrm{N}(\mu, \sigma^2)$, where we estimate $\mu$ using $\overline{Y}$ (there are two parameters, but only $\mu$ describes the mean). Using this parameter $\mu$, we have explained some of that variation in our data.

```{r, fig.width=3, fig.height=3}
y.bar <- mean(webs[, 2])
ggplot(data = webs, aes(x = 0, y = length)) + geom_point(col = "gray37") + 
  geom_hline(aes(yintercept = mean(length)), col = "dodgerblue1") + 
  annotate('text', x = 0.01, y = y.bar+0.0001, label = "bar(Y)", parse = TRUE, size = 5, col ="dodgerblue1") +
  labs(x = "", y = "Relative length of spider webs") + theme_classic() +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), text = element_text(size = text.size))
```

Now, we might be able to explain more of this variation in the data if we add a covariate to the mean, so as to allow the mean to vary. This will "capture" some of the variability in the data and will leave less unexplained variation to be assigned to $\sigma$. There are three total parameters in this more complicated model, two that describe the mean and one to describe the **residual variation**. (The parameter $\sigma^2$ is playing the same role in the model as before, but because we hope that at least some of the variation is explained by the covariate, we now refer to whats left as **residual variation**.)

```{r, fig.width=5, fig.height=4}
data(webs)
text.size <- 14
y.bar <- mean(webs[, 2])
web.fit <- lm(formula = length ~ temp.C, data = webs)
ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
  geom_hline(aes(yintercept = mean(length)), col = "dodgerblue1") + 
  geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = "palegreen1") +
  annotate('text', x = 12, y = y.bar, label = "bar(Y)", parse = TRUE, size = 5, col ="dodgerblue1") +
  labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
```

The total amount of variation to be explained is called the "SST" for "sums of squares total". 

$$
\text{Sums of squares total} = \sum_{i = 1}^n (Y_i - \bar{Y})^2
$$

($n$ is the total number of data points.)

We can partition this total sums of squares into the component explain by the regression model and that which is left over as unexplained variation (which we often refer to as "error", but it is "error" in the sense that it is not explained in the model and not "error" in the sense that we have done something wrong). Note that different sources/books will use different acronyms for partitioning variation and you should not be too invested in the notation I am presenting here. The important thing is to remember the equations and the idea behind partitioning variation. The **sum of squares regression** is the amount of variation expained by the regression line, or the squared deviations from the points estimated in the regression $\hat{Y}_i$, which is described by $\hat{Y}_i = \hat{\beta_0} + \hat{\beta_1} X_i$, and the estimated mean $\bar{Y}$:

$$
\text{SSR} = \sum_{i = 1}^n{(\hat{Y}_i - \bar{Y})^2}
$$

You can think of the sum of squares regression as the amount of variation explained when going from a one parameter model describing the mean behavior ($\mu$ only) to the regression model (here, two parameters describing the mean behavior of the model, $\beta_0$ and $\beta_1$).

```{r, fig.width=5, fig.height=4}
x.i <- webs[36, 3]
y.i.hat <- web.fit$coefficients[1] + web.fit$coefficients[2] * x.i
ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
  geom_point(x = x.i, y = y.i.hat, col = "palegreen3", size = 3) + 
  annotate(geom = "text", x = x.i + 1.5, y = y.i.hat, label = "hat(Y[i])", parse = TRUE, col = "palegreen3", size = 5) +
  geom_hline(aes(yintercept = mean(length)), col = "dodgerblue1") + 
  geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = "palegreen1") +
  annotate("text", x = 12, y = 0.9987, label = "bar(Y)", parse = TRUE, size = 5, col ="dodgerblue1") +
  labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
```

The **sum of squares error** is the amount of variation *not* expained by the regression line, or the squared deviations from the actual data points themselves, $Y_i$, and the points estimated by the regression $\hat{Y}_i$:

$$
\text{SSE} = \sum_{i = 1}^n{(Y_i - \hat{Y}_i)^2}
$$

The sum of squares error is the remaining (unexplained, residual) error after modeling.

Again, $\hat{Y}_i = \beta_0 + \beta_1 X_i$, or the *Y* value predicted by the regression model. The best fit line minimizes the sum of squares error. All errors (residuals) are the **vertical** distances from the data points to the points estimated in the regression. This is because all error exists with respect to the response variable (here, $Y$) only. *Note that there all alternatives to minimizing the sum of squares error that we will briefly discuss later.*

```{r, fig.width=5, fig.height=4, echo=FALSE}
y.i <- webs[36, 2]
ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
  geom_point(x = x.i, y = y.i, col = "firebrick1", size = 3) + 
    geom_point(x = x.i, y = y.i.hat, col = "palegreen3", size = 3) + 
  annotate(geom = "text", x = x.i + 1.5, y = y.i.hat, label = "hat(Y[i])", parse = TRUE, col = "palegreen3", size = 5) +
  annotate(geom = "text", x = x.i + 1, y = y.i - 0.0001, label = "Y[i]", parse = TRUE, col = "firebrick1", size = 5) +
  geom_hline(aes(yintercept = mean(length)), col = "dodgerblue1") + 
  geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = "palegreen1") +
  annotate("text", x = 12, y = 0.9987, label = "bar(Y)", parse = TRUE, size = 5, col ="dodgerblue1") +
  labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
```

To estimate $\beta_0$ and $\beta_1$, we could use multiple approaches. We could use the method we have been discussing, minimizing the sums of squares error, which is called **ordinary least squares** (OLS). We could also use maximum likelihood. As long as the assumptions of regression are met, these two methods are equivalent.

The best fit line will pass through $(\bar{X}, \bar{Y})$ because this is the "center mass" of the data and because we assumed the expected value of the errors is zero. We use this fact (and some algebra) to show:

$$
\bar{Y} = \hat{\beta_0} + \hat{\beta_1} \bar{X}
$$

$$
\hat{\beta_0} = \bar{Y} - \hat{\beta_1} \bar{X}
$$

Then, we solve for $\hat{\beta_1}$ by plugging in what we know back into the equation for SSE:

$$
\text{SSE} = \sum_{i = 1}^n{(Y_i - \hat{Y}_i)^2} = \sum_{i = 1}^n{(Y_i - \hat{\beta_0} - \hat{\beta_1} X_i)^2} = \sum_{i = 1}^n{(Y_i - \bar{Y} + \hat{\beta_1} \bar{X} - \hat{\beta_1} X_i)^2}
$$

Then we only have one unknown left $\text{SSE} = f(\hat{\beta_1})$, so we can minimize the function by taking the derivative with respect to $\hat{\beta_1}$ and set it equal to 0.

$$
\frac{\partial \text{SSE}}{\partial \hat{\beta_1}} = 0
$$

After working this out,

$$
\hat{\beta_1} = \frac{\sum_{i = 1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i = 1}^n (X_i - \bar{X})^2}
$$

Try this derivation yourself and then check your work [here](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf).

##But wait, couldn't we have used maximum likelihood? Yes!
The explanation provided above is the standard "non-calculus" explanation for how to find the best fit OLS regression line. But we could have found the MLE for slope and intercept using the same methods as were learned before the midterm.

Remember back to the Week 4 lab, the joint likelihood for $n$ i.i.d. Normally distributed data $Y \sim N(\mu,\sigma^{2})$ was given by:

$$
L(\mu,\sigma|Y_{1},Y_{2},...,Y_{n}) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(Y_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$
(Before, we were calling the response variable X, but now we will switch to calling the response variable Y since we will use X for the covariates.) Here we are making one tiny change, we are replacing $\mu$ with $\beta_{0}+\beta_{1}X_{i}$ to fit the model $Y \sim N(\beta_{0}+\beta_{1}X_{i},\sigma^{2})$.

$$
L(\beta_{0},\beta_{1},\sigma|Y_{1},Y_{2},...,Y_{n}) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(Y_{i}-(\beta_{0}+\beta_{1}X_{i}))^{2}}{\sigma^{2}}\right)}
$$

Great! Now we have a joint likelihood for the new linear regression model, and we can find the MLE for $\beta_{0}$ and $\beta_{1}$ just like we did before, by setting

$$
\frac{\partial NLL}{\partial \beta_{0}} =0
$$
and

$$
\frac{\partial NLL}{\partial \beta_{1}} =0
$$
The values of $\beta_{0}$ and $\beta_{1}$ that satisfy this equation are the maximum likelihood estimators for these two parameters.

We will use this approach (writing down the joint likelihood and then minimizing with respect to the parameters) when we come to generalized linear models in another week.

## Assumptions of regression:

$$
Y_{i} = \beta_{0}+\beta_{1}X_{i} + \epsilon_{i} \mbox{, where } \epsilon_{i} \sim N(0,\sigma^{2})
$$

1.	A linear model appropriately described the relationship between X and Y.
2.	For any given value of X, the sampled Y values are independent with normally distributed errors.
We can express this assumption as follows:

$E[\epsilon_{i}]=0$

$E[(\epsilon_{i})^{2}]=\sigma_{\epsilon}^{2}$

$E[(\epsilon_{i})(\epsilon_{j})]=0 \mbox{, where } i \neq j$

3.	Variances are constant along the regression line.  (Non-constant variances are an example of heteroskedacity. More on this in a second.)
4.	An implicit assumption is that the regression fit is only valid over the range of X represented in the original data. It is very dangerous to extrapolate outside the range over which the regression line was originally fit.

**If** these conditions are met, then the OLS estimators we just derived are the best linear unbiased estimators (BLUE) and the sampling distributions for the slope and intercept are as follows:

$$
\hat{\beta_0} \sim \mathrm{N}\left( \beta_0, \frac{\sigma_\epsilon^2 \sum_{i = 1}^n X_i^2}{n \sum_{i = 1}^n (X_i - \bar{X})^2}\right) \text{, where } \sigma_\epsilon^2 = \mathrm{E}(\epsilon_i^2)
$$
$$
\hat{\beta_1} \sim \mathrm{N}\left( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right) \text{, where } \sigma_\epsilon^2 = \mathrm{E}(\epsilon_i^2)
$$
*FYI:* these distributions for $\beta$ can be derived using the second derivatives of the NLL (but you don't need to be able to do this).

These equations may look complex, but there are some features that should make sense. One: The expected value of $\hat{\beta_{0}}$ is $\beta_{0}$, as you would expect for an unbiased estimator. Moreover, if $\sigma_\epsilon^2$ goes to zero, the uncertainty in the estimates go to zero. In other words, if there is no variation in the data (all data points lie exactly on the regression line), than there is no uncertainty in the value of the parameters. Also, note that as $n \rightarrow \infty$, the uncertainty goes to zero as well (again, as we would expect).

Because we do not know the population (true) error variance $\sigma_\epsilon^2$, we estimate it from the data using:

$$
s_\epsilon^2 = \frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
$$
We substitute $s_\epsilon^2$ for $\sigma_\epsilon^2$. This is an unbiased and maximally efficient estimator for $\sigma_\epsilon^2$. $p$ is the number of parameters required to estimate $\sigma_\epsilon^2$. This is sometimes called **mean squared error (MSE)** (as it is in Aho).

**Question: How many degrees of freedom do we have for $s_\epsilon^2$ in a simple linear regression?**

$n - p = n - 2$. This is because $\hat{Y}$ involves $\bar{Y}$ and $\bar{X}$. This will be different with multiple regression (regression with more than one covariate).

**Question: What is our null hypothesis for the parameter $\beta_0$? What does it mean?**

$\beta_0 | H_0 = 0$. When $X = 0$ the estimated value for $Y$ is 0.


**Question: What about for $\beta_1$?**

$\beta_1 | H_0 = 0$. The slope of the regression equals 0, $X$ has no linear effect on $Y$.



The test statistic and standard error for estimating $\beta_1$:

$$
T^* = \frac{\hat{\beta_1} - \beta_{1 | H_0}}{\text{SE}_{\hat{\beta_1}}} \sim t_{n - p}
$$

$$
\text{SE}_{\hat{\beta_1}} = \sqrt{\frac{\frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}} = \sqrt{\frac{s_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}}
$$
Note that usually we are interested in $\beta_{1|H_{0}}=0$.

We construct confidence intervals as we always have

$$
P {\left( \hat{\beta_1 } - t_{(1 - \frac{\alpha}{2}) [n - p]} \text{SE}_{\hat{\beta_1}} \leq \beta_1 \leq \hat{\beta_1 } + t_{(1 - \frac{\alpha}{2}) [n - p]} \text{SE}_{\hat{\beta_1}} \right)} = 1 - \alpha
$$

You can use similar methods to estimate the CI for $\beta_0$. We can use bootstrapping to calculate the standard error of the regression slope as well. We demonstrate this with an example on Thursday.

**Question: I've called this value $\text{SE}_{\hat{\beta_1}}$. Aho refers to it as ${\hat{\sigma}_{\hat{\beta_1}}}$. Why are these names interchangeable?**

The standard deviation of an estimated parameter (here, $\hat{\beta}_1$) is equal to the standard error of the parameter.


Our model allows us to make a prediction about the response expected for any given value of X, which may be a value of X in the original dataset or it may be a value of X **not** in the original dataset. (The model is only valid for X values contained within the range of the original data; more on this later.) We will refer to a new X value as $X^{*}$ and the corresponding estimates for Y as $\hat{Y}$. Don't forget that the fitted $\hat{Y}$ values are also estimated quantities, and as such come with uncertainty (i.e. standard errors and confidence intervals). It turns out there are really two ways to express “confidence”: one of which we call a "confidence interval", the other we call a "prediction interval". 

##Confidence vs. Prediction intervals

One way is to ask about variability in our predicted values is: “What are the 1-$\alpha$ confidence intervals on the **mean** value of Y given $X^{*}$”? Equivalently, confidence intervals for $\hat{Y}$ mean, "there is a 95% chance that my 95th percentile CIs include the true underlying population parameter value, which is the **mean value** of $\hat{Y}$ ($\mathrm{E}[Y|X^{*}]$)."

We calculate the CIs using

$$
\hat{Y^*} \pm t_{(1 - \frac{\alpha}{2}) [n - p]} \sqrt{s_\epsilon^2 {\left(\frac{1}{n} + \frac{(X^* - \bar{X})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right)}}
$$

Note the notation here. The above equation represents the confidence intervals on $Y$ associated with the value $X^*$. The "hat" on Y reflects the fact that Y is an estimate based on the data, and the "star" is just meant to remind you that this interval is associated with the specific X value $X^*$.

However, there is a second way to think about uncertainty in the response value. In stead of wanting to know our uncertainty in the MEAN predicted value, we may want an estimate for the variation expected for a future value of $Y$ for a given $X^{*}$. Our estimates of uncertainty for the predicted values take into account both the mean of $\hat{Y_h}$ (uncertainty in the regression line itself) as well as the variance in $\hat{Y_h}$. These are called **prediction intervals** (PIs).

We calculate PIs using

$$
\hat{Y^*} \pm t_{(1 - \frac{\alpha}{2}) [n - p]} \sqrt{s_\epsilon^2 {\left( 1 + \frac{1}{n} + \frac{(X^* - \bar{X})^2}{\sum_{i = 1}^n (X_i - \bar{X})^2}\right)}}
$$

**Question: What is the difference between these two formulas? Does it make sense why they are different?**

The formula for the prediction interval has a 1 in the parenthesis, which is associated with the addition of $s^{2}_{\epsilon}$. This should make sense because it is the residual uncertianty that is being added to the uncertainty in the best fitting line itself. Prediction intervals are always wider than confidence intervals.


```{r warning=FALSE, fig.width=6, fig.height=5}
library(ggplot2)
data(webs)
text.size <- 14
y.bar <- mean(webs[, 2])
web.fit <- lm(formula = length ~ temp.C, data = webs)
web.fit <- lm(formula = length ~ temp.C, data = webs)
webs$CIl <- predict(web.fit, interval = "confidence")[, 2]
webs$CIu <- predict(web.fit, interval = "confidence")[, 3]
webs$PIl <- predict(web.fit, interval = "prediction")[, 2]
webs$PIu <- predict(web.fit, interval = "prediction")[, 3]
ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
  geom_line(aes(x = temp.C, y = CIl), linetype = "dashed", col = "dodgerblue1") +
  geom_line(aes(x = temp.C, y = CIu), linetype = "dashed", col = "dodgerblue1") +
  geom_line(aes(x = temp.C, y = PIl), linetype = "dotted", col = "slateblue1") +
  geom_line(aes(x = temp.C, y = PIu), linetype = "dotted", col = "slateblue1") +
  geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = "palegreen1") +
  labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
```

Remember: Confidence intervals represent our uncertainty about where the line should be (the mean behavior). Prediction intervals represent our certainty about the predicted values you could expect (the mean behavior + the variance).

We can confirm the interpretation of the confidence interval by bootstrapping our data and refitting a line each time.

```{r warning=FALSE, fig.width=6, fig.height=5}
webs$CIl <- predict(web.fit, interval = "confidence")[, 2]
webs$CIu <- predict(web.fit, interval = "confidence")[, 3]
webs$PIl <- predict(web.fit, interval = "prediction")[, 2]
webs$PIu <- predict(web.fit, interval = "prediction")[, 3]
p<-ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
    geom_line(aes(x = temp.C, y = CIl), linetype = "dashed", col = "dodgerblue1") +
    geom_line(aes(x = temp.C, y = CIu), linetype = "dashed", col = "dodgerblue1") +
    geom_line(aes(x = temp.C, y = PIl), linetype = "dotted", col = "slateblue1") +
    geom_line(aes(x = temp.C, y = PIu), linetype = "dotted", col = "slateblue1") +
    labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
for (i in 1:100)
{
  selection<-sample(1:nrow(webs),replace=T)
  webs.bootstrapped<-webs[selection,]
  web.fit.new <- lm(formula = length ~ temp.C, data = webs.bootstrapped)
  p<-p+geom_abline(intercept = web.fit.new$coefficients[1], slope = web.fit.new$coefficients[2], col = "palegreen1")
}
p<-p+ geom_line(aes(x = temp.C, y = CIl), linetype = "dashed", col = "dodgerblue1") +
  geom_line(aes(x = temp.C, y = CIu), linetype = "dashed", col = "dodgerblue1")  +
  geom_line(aes(x = temp.C, y = PIl), linetype = "dotted", col = "slateblue1") +
  geom_line(aes(x = temp.C, y = PIu), linetype = "dotted", col = "slateblue1") 
plot(p)
```

Notice that the confidence intervals "capture" the range of possible regression lines created by fitting a linear model to the bootstrapped datasets (each "playing the role"" of a re-do of the original data collection.)


**Question: Why are confidence and prediction intervals "bow-tie" shaped, i.e. wider further away from the middle of the line?**

Mathematically, the squared deviation from $\bar{X}$ is greater as you move away from $\bar{X}$, which increases $\text{SE}_{\hat{Y}}$. Intuitively, we can think about it this way: Because the best fitting line *has* to go through ($\bar{X},\bar{Y}$), the confidence interval is narrowest at this point. Away from ($\bar{X},\bar{Y}$), we have the additional uncertainty about what the slope should be moving away from ($\bar{X},\bar{Y}$), and so the intervals "grow" as you move away from this point.


##How do we know if our model is any good?

Earlier we determined that we fit our regression by minimizing the sum of squares error (SSE). However, SSE depends on the units of the dependent variable, so that doesn't help us decide if the model fits "well" (e.g., the model could be a best fit line, but still not very good). However, we can use the total variation, partitioned into 1) the explained variation in the regression, SSR, and 2) the unexplained residual variance, SSE, to create a useful metric of model fit.

$$
\sum_{i = 1}^n (Y_i - \bar{Y})^2 = \sum_{i = 1}^n (\hat{Y_i} - \bar{Y})^2 + \sum_{i = 1}^n (Y_i - \hat{Y}_i)^2
$$
On the left side is the total sum of squares (SST), or how much each data point differs from the mean. The first term on the right is the sum of squares regression (SSR), or the amount of variation explained by the regression. The final term on the right is the sum of squares error (SSE), or how much each data point deviates from the best fit line.

The **coefficient of determination** $R^2$ is the proportion of variation in $Y$ explained by the regression model. Think of it like the ratio of variance explained relative to the total amount of variation in the data.

$$
R^2 = \frac{\text{SSR}}{\text{SST}}
$$

The coefficient of determination ranges from 0 to 1. It is equivalent to the square of Pearson's $r$. High $R^2$ means that the model has explanatory power. However, it is not evidence that the relationship is causal.

Lets go back to our calculations, and think about how many degrees of freedom we had for each

1.	Total sum of squares = SST = $\sum_{i=1}^{n}(Y_{i}-\bar{Y})(Y_{i}-\bar{Y})$. This required one parameter $\bar{Y}$, be estimated from the data, so there remain n-1 degrees of freedom.

2. Sum of squared error = SSE = $\sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})(Y_{i}-\hat{Y_{i}})$. This required estimation of $\hat{Y_{i}}$, which used two estimated parameters (slope and intercept) $\rightarrow$ n-2 d.o.f. remain.

3. Regression model sum of squares SSR: The regression d.o.f. reflect the number of *extra* parameters involved in going from the null model ($\bar{Y}$) to the regression model, which in this case is 1. 

##Robust regression

Ordinary Least Squares regression as defined above is fairly sensitive to outliers, which can be seen in the definition of SSE:

$$
SSE = \sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
$$
Because we square the residuals, points that are far off the regression line have more influence on the estimates of intercept and slope than points that are much closer to the best fit line. There are several methods that are more robust against outliers. Not surprisingly, these fall under the heading of “robust regression”.

The most common robust regression approach is called M-estimation. **M-estimation** minimizes any monotonic function of the error, allowing for methods other than squared error. These functions may associate less of a "cost" with having a large residual, or they may even have a saturation point which defines the maximum influence any point can have.

In other words, the metric that is being minimized can be any function f() of the difference between each point and the best-fit line:

$$
\sum_{i = 1}^n f(Y_i - \hat{Y_i})
$$
Visualizing M-estimation (these are four functions, $\rho$, that can be used to model the error, $x$):

<img src="https://upload.wikimedia.org/wikipedia/commons/c/c1/RhoFunctions.png"; style="max-width:500px;">

We won't get into the mathematics, because they are complex (another benefit of least squares is simpler math) but it's an avenue you may use in the future and is good to know about.

##Robust regression
Sometimes the relationship between X and Y is not linear and cannot be made linear using a transformation of the variables. In this case, you can carry out a non-parametric test for regression. Here we will discuss Kendall’s robust line fitting method.

Step 1 – Order the x and y values in order of increasing $X$ value
Step 2 – Compute the slope between every pair of $X$ values

$$
\mbox{Slope between i and j} = \frac{Y_{j}-Y_{i}}{X_{j}-X_{i}}
$$
Step 3 –

$$
\widehat{\mbox{Slope}}_{non-parameteric} = \mbox{median of slopes}
$$
$$
\widehat{\mbox{Intercept}}_{non-parameteric} = \mbox{median of } Y_{i}-\widehat{\mbox{Slope}}_{non-parametric}X_{i}
$$
**Question: How to know if the slope is significant?** 

Use Kendall’s rank correlation test.


## Type I and Type II Regression

We have only discussed Type I regression up to this point. With Type I regression, we use $X$ to predict $Y$, and have only minimized the error in $Y$. This means that we have assumed that there is no error in $X$ ($X$ is fixed and not random). 

However, in many cases there could be error in $X$.  **Major axis regression** and **standardized major axis regression** (Type II or Model II regressions) model variance in both the explanatory and response variables. Importantly, however, these methods to not strictly predict $Y$ from $X$, but rather calculate the major axis of variation in the bivariate dataset $X$ and $Y$. This means Type II regression models are not suggested for prediction.

Major axis regression places equal weight on deviations in the $X$ and $Y$ directions and minimizes the sum of squared **perpendicular distances** to the best fit line. The major axis regression line is the first principal component from PCA (stay tuned). This requires that $X$ and $Y$ are measured on the same scale, using the same units. 

Standardized major axis regression standardizes each variable by its own variance and then fits a best fit line, so that deviations in $X$ and $Y$ are weighted accordingly.

When is MA and SMA used? (1) In tests of isometry, allometry, stoichiometry (is the relationship between X and Y 1? Is the ratio of C:N 1?), (2) Testing if two methods of measurement agree.

Week 10 Lecture
=============

```{r include=FALSE, warning=FALSE}
text.size <- 14
```

This week we will cover my favorite topic of all: Generalized Linear Regression and Multiple Regression!

An outline for this week's materials:

1. Basic idea behind GLM

2. Logistic regression

3. Poisson regression

4. Deviance

5. LOESS/spline smoothing

6. Generalized additive models

7. Multiple regression

An example
-----------

Let's start with a model of the presence or absence of a wood-boring beetle as a function of the wood density of decaying aspen trees in Quebec. The data look like this:

```{r echo=FALSE, warning=FALSE, fig.height=5, fig.width=6}
library(ggplot2)
library(asbio)

text.size <- 16
data(beetle)
ggplot(beetle, aes(x = Wood.density, y = ANAT)) + geom_point() + 
  labs(x = expression(paste("Wood density (g cm" ^ "-3", ")")), y = "Presence of wood-boring beetle", parse = TRUE) + 
  theme_classic() + theme(text = element_text(size = text.size))
```

So, let's do what we know, and we'll fit a linear regression to the data using `lm()` (like we did last week). Then we'll plot the model fit.

```{r fig.height=3, fig.width=4}
beetle.fit <- lm(formula = ANAT ~ Wood.density, data = beetle)
new.predictor <- list(Wood.density = seq(min(beetle$Wood.density), max(beetle$Wood.density), length.out = 24))
beetle.vals <- predict(beetle.fit, newdata = new.predictor)
beetle.predict <- data.frame(WoodDensity = new.predictor[[1]], Presence = beetle.vals)

ggplot(data = beetle, aes(x = Wood.density, y = ANAT)) + geom_point(col = "gray55") + 
  geom_line(data = beetle.predict, aes(x = WoodDensity, y = Presence)) +
  labs(x = expression(paste("Wood density (g cm" ^ "-3", ")")), y = "Presence of wood-boring beetle", parse = TRUE) + 
  theme_classic() + theme(text = element_text(size = 10))
```

How well does this model fit, and how well does it represent the data? Let's predict beetle presence for woody density of 0.1 g/cm<sup>3</sup> and then 0.4 g/cm<sup>3</sup>.

```{r}
unname(beetle.fit$coefficients[1] + beetle.fit$coefficients[2] * 0.1)

unname(beetle.fit$coefficients[1] + beetle.fit$coefficients[2] * 0.4)
```

Last, we'll look at the residuals for our model fit.

```{r fig.height=4, fig.width=5}
beetle.resid <- data.frame(PredictedValues = predict(beetle.fit), Residuals = residuals(beetle.fit))
ggplot(data = beetle.resid, aes(x = PredictedValues, y = Residuals)) + geom_point(col = "gray55") +
  labs(x = "Predicted values from linear regression", y = "Residuals", parse = TRUE) + 
  theme_classic() + theme(text = element_text(size = text.size))
```


Generalized linear models
--------------

The regression models that we have been introduced to thus far all assume two things:

1. A **linear** relationship between the independent variable and the dependent variables

2. Normally distributed errors

Linear regression can be thought of as

$$
Y_i \sim \mathrm{N}(\beta_0 + \beta_1 X_i, \sigma^2)
$$

or, equivalently,

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \text{, where } \epsilon_i \sim \mathrm{N}(0, \sigma^2)
$$
Generalized linear models are extensions of what we have been talking about that free us from these two constraints.  We can think of the above case as having two parts, the first is a model for the mean (or the expected value $E[Y_{i}]$).

$$
\mathrm{E}[Y_i] = \mu_i = \beta_0 + \beta_1 X_i
$$
and the second being a model for the variance

$$
\epsilon_i \sim \mathrm{N}(0, \sigma^2)
$$

The Normal distribution is "special". The mean ($\mathrm{E}(X)$) and the variance ($\mathrm{Var}(X)$) of the distribution are directly related to the two parameters of the distribution ($\mu$ and $\sigma^2$, respectively). This means the mean and variance are decoupled.

For other distributions, the parameters of the distribution often do not map uniquely to the mean and variance. GLMs allow us to model the response as some distribution (no longer has to be a Normal), and to model the parameters of that distribution (or a function of those parameters) using a linear model. In other words,  

<span style="color: orangered;">
$$
\text{Response} \sim \text{Distribution}(\text{parameters})
$$

$$
g(\text{parameters}) = \beta_0 + \beta_1 X_i
$$
</span>

$g()$ is a function of the parameters. For most distributions other than the Normal, the mean and variance are linked.

$$
\epsilon_i \sim \text{Variance function}(\text{usually determined by the distribution for the mean})
$$

The function that relates the parameters of the distribution to the parameters of the linear model is called a **link function** (here, $g()$). Not all functions are acceptable, there are a few frequently used functions that serve this role (depends on the distribution of our data).

Traditional linear regression may fail to represent our data (predicting nonsensical data, etc.) in many cases in ecology and evolution. Think about the error structure you'd expect with each of these types of data:

* Count data

* Binary data

* Proportion data

In this lecture we will go through logistic regression for binomial data and Poisson regression for Poisson (count) data. Keep in mind that other GLMs exist, specifically beta regression and gamma regression, among others.

The order of complexity (and flexibility) of the models we will be discussing is: 

Standard linear regression $<$ Generalized linear models (GLMs) $<$ Generalized Additive Models (GAMs).

Logistic regression
------------

Logistic regression is used to model Bernoulli or binomial response variables. Bernoulli data includes:

* Survival for individual organisms

* Presence/absence for individual organisms

* Allele or phenotype of type 1 or type 2 for an individual

The underlying equation is

$$
Y_i \sim \mathrm{Bernoulli}(p_i)
$$
The response is either 0 or 1, with probability $p$. For Bernoulli data (or binomial), the link function $g()$ is

$$
\log \left( \frac{p_i}{1 - p_i} \right) = \log \left( ''odds'' \right) = \mathrm{logit}(p_i)
$$

This function is known as the **logit function** and also represents the log odds of a response. By using the logit function as the link function for a logistic regression, we are saying that the logit function maps the linear model ($\beta_0 + \beta_1 X_i$) to the parameter(s) of the Bernoulli (or binomial). Why use the logit function? We want to map values in the range $[0, 1]$ to $[- \infty, \infty]$.

The variance function describes the distribution of the error in the GLM:

$$
\epsilon_i \sim \text{Variance function}
$$

Roughly, a variance function relates the mean of the distribution to the variance (see Aho 9.20.7.3 for more details). For example, the variance function of the normal distribution is 1, $\mathrm{V}[\theta] = 1$ (describing constant variance, i.e., homoscedasticity). *Note that $\mathrm{V}[\theta]$ is not equivalent to $\mathrm{Var}[\theta]$, though they are related (see Appendix A.6 in Aho for details).* The variance of $Y_{i}$ for the Bernoulli, where $n_i=1$is given by:

$$
\mathrm{Var}(Y_i) = p_i (1 - p_i)
$$

While for a normal linear regression we need errors to be the same (homoscedasctic), with a logistic regression, variation is largest around $p = 0.5$ and shrinks to zero at the end points of $p=0$ and $p=1$.

```{r fig.height=4, fig.width=5}
p <- seq(0, 1, 0.1)
plot(x = p, y = p * (1 - p), type = "l")
```

The complete model for a logistic regression with Bernouilli data $Y_i$ and covariate $X_i$ is:

$$
Y_i \sim \mathrm{Bernoulli}(p_i) \text{, where } \log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
$$

Keep in mind that for Bernoulli data, each data point is either a 0 or a 1 (a "success" or a "fail"). The probability $p_{i}$ for each data point $Y_{i}$ is a continuous variable from [0,1]. The value of the covariate $X_{i}$ determines the probability $p_{i}$, which in turn "weights the coin" and controls the probability that the outcome $Y_{i}$ will be 0 or 1.

The binomial is closely related to a Bernoulli. Whereas the Bernoulli represents a single "coin flip", the bimomial represents a collection of coin flips. To model a binomial response variable:

$$
Y_i \sim \mathrm{Binomial}(n_i, p_i) \text{, where } \log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
$$

Now, the variance of $Y_i$ is:

$$
\mathrm{Var}(Y_i) = n_i p_i (1 - p_i)
$$

Some examples of data that are best modelled as either Bernoulli or Binomial are:
```{r echo=FALSE, fig.align='center'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/JointDamage.png')
```
```{r echo=FALSE, fig.align='center'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/TurtleEggs.png')
```

By modeling our data as it exists, we have solved four problems

1. The errors now reflect the binomial process

2. The variance reflects the binomial process

3. The response $Y_i$ is now bounded $[0, 1]$

4. We have maintained the information on sample size for each data point (only relevant when data are binomial)

An example with the beetle dataset (To be worked out on your own)

```{r fig.height=4, fig.width=5}
beetle.glm.fit <- glm(formula = ANAT ~ Wood.density, data = beetle, family = "binomial")
beetle.glm.vals <- predict(beetle.glm.fit, newdata = new.predictor, type = "response")
beetle.glm.predict <- data.frame(WoodDensity = new.predictor[[1]], Presence = beetle.glm.vals)
ggplot(data = beetle, aes(x = Wood.density, y = ANAT)) + geom_point(col = "gray55") + 
  geom_line(data = beetle.glm.predict, aes(x = WoodDensity, y = Presence)) +
  labs(x = expression(paste("Wood density (g cm" ^ "-3", ")")), y = "Presence of wood-boring beetle", parse = TRUE) + 
  theme_classic() + theme(text = element_text(size = text.size))
```

**Question: How do we interpret this GLM?**

With increasing wood density, the probability of finding wood-boring beetles decreases.
 

And let's see our predictions of the response again:

```{r}
predict(beetle.glm.fit, newdata = list(Wood.density = 0.1), type = "response")
predict(beetle.glm.fit, newdata = list(Wood.density = 0.4), type = "response")
```

The predictions are $\hat{Y}_i$, which is equal to the expected value of $Y_i$ given $X_i$ ($\mathrm{E}(Y_i)$). With a normally distributed regression, $\hat{Y}_i = \mu_i = \beta_0 + \beta_1 X_i$. With GLMs, we have to consider the link function. Now, $\mathrm{E}(Y_i) = p_i = \hat{Y}_i$. Thus, $\hat{Y}_i$ are probabilities in logistic regression. This is why `predict()` doesn't give us 0's and 1's.

```{r}
predict(beetle.glm.fit, newdata = list(Wood.density = 0.1), type = "response")
predict(beetle.glm.fit, newdata = list(Wood.density = 0.4), type = "response")
```

The argument `type = "response"` in `predict()` back-transforms the predicted response using the inverse logit function.
 
In general, the outputs ($\hat{Y}_i$) from logistic regression are in logit units because of the link function we applied, $\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i$. This also means that the relationship between the link function and the predictor, expressed with the parameters $\beta_0$ and $\beta_1$ is linear on the scale of the logit probability. We can transform the logit output to units of odds (using the exponential function) or probability (using the inverse logit function).

If the probability of an event is 0.3:

The odds of the event occurring: $\text{odds} = \frac{0.3}{0.7} = 0.43$

The log odds (logit) of the event occurring: $\text{log odds} = \ln \frac{0.3}{0.7} = -0.85$

The probability of the event (which we already knew) can be obtained using the logit using the inverse logit: $\frac{\exp(\text{log odds})}{1 + \exp(\text{log odds})}$

```{r echo = FALSE, fig.width = 4, fig.height=3}
log.odds <- seq(from = -5, to = 5, by = 0.25)
p <- plogis(log.odds)
dat <- data.frame(log.odds, p)
ggplot(dat, aes(x = log.odds, y = p)) + geom_line() + geom_hline(aes(yintercept = 0.5), colour = "gray",linetype = "dashed") + geom_vline(aes(xintercept = 0.0), colour = "gray", linetype = "dashed") + scale_x_continuous(breaks = seq(-5, 5, by = 1)) + labs(x = "log odds (logit)", y = "probability") + theme_classic() + theme(text = element_text(size = text.size))
```

Adapted from [this website](http://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf). 

```{r}
summary(beetle.glm.fit)
```

The relationship between the link function and the predictor,  $\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i$, is **linear** on the scale of **logit units** (see Aho example 9.28). 

If the logistic model is fit without an intercept, the best fit line predicts $p = 0.5$ when $X = 0$. The intercept $\hat{\beta_0}$ shifts the probability for $X = 0$ up or down relative to 0.5. In logit units, the probability is shifted up (approximating 1) when $X = 0$.

```{r}
beetle.glm.fit$coefficients[1]  # logit units
inv.logit <- function(x) {
  exp(x) / (1 + exp(x))
}
inv.logit(beetle.glm.fit$coefficients[1])  # units of probability (but no longer linear)
```

Since the parameters are linear on the scale of logit units, the logit (log odds) of beetle presence decreases by 52.6 given a 1 g/cm<sup>3</sup> increase in wood density (decreases to almost 0 probability).


```{r}
beetle.glm.fit$coefficients[2]  # logit units
```

Be aware that the significance of coefficients fit for GLMs is done using a Wald test, which relies on asymptotic (large-sample) estimates of standard errors of the coefficients. The Wald test uses a test statistic, $z$, that is the parameter estimate divided by the estimated standard error of the parameter estimate, which is asymptotically standard normal under $H_0 = 0$.

Fitting a GLM
-------------

To fit GLMs, we need another method besides ordinary least squares (which assumes data are normally distributed). We need to fit the parameters using maximum likelihood.

We'll use binomial data as an example. The likelihood for binomial data, where we have $N$ data points $Y = \{ Y_1, Y_2, ...,Y_N \}$, each a draw from $\mathrm{Binomial}(n_i, p_i)$:

$$
\mathcal{L}(Y_i | p_i) = \prod^N_{i = 1} \frac{n_i !}{Y_i! (n_i - Y_i)!} p_i^{Y_i} (1 - p_i)^{n_i - Y_i}
$$
Note that I'll use $\mathcal{L}$ to represent likelihood and $\ell$ to represent log-likelihood (as in Aho).

We can express $p_i$ as a function of the model parameters by rearranging the model equation:

$$
\log \left( \frac{p_i}{1 - p_i} \right) = \beta_0 + \beta_1 X_i
$$

$$
p_i = \frac{\exp(\beta_0 + \beta_1 X_i)}{1 + \exp (\beta_0 + \beta_1 X_i)}
$$
So, we can substitute this expression for $p_i$ into the likelihood (after converting to negative log-likelihood), then we get the likelihood as a function of the model parameters $\beta_0$ and $\beta_1$, $\text{NLL}(\beta_0, \beta_1 | X_i)$. We can solve for the maximum likelihood estimate (MLE) for $\hat{\beta_0}$:

$$
\frac{\partial \text{NLL}}{\partial \beta_0} = 0 \rightarrow \hat{\beta_{0}}
$$
$$
\frac{\partial \text{NLL}}{\partial \beta_1} = 0 \rightarrow \hat{\beta_{1}}
$$

These equations cannot be solved algebraically. They must be solved numerically, for example, using iteratively weighted least squares with optimizing algorithms. When you think about the computing power needed to fit GLMs (which is no big deal today), it makes sense why it was traditional to transform your data to become normally distributed rather than model your data as it exists using GLM.

How do we interpret the coefficients?

If we fit a logistic model without an intercept, the best fit line must predict p=0.5 when X=0. So the intercept in this case shifts the probability for X=0 up or down relative to 0.5.

The “slope” ($\widehat{\beta_{1}}$)̂ can be interpreted as follows:

$$
e^{\widehat{\beta_{1}}}=\mbox{="change in the odds for a 1 unit change in X"}
$$

**Important note** Note that the sampling variability is fully prescribed by the model used to describe the response 

$$
Y_i \sim Binomial(n_i,p_i)
$$

In other words, by using a logistic model, we have implicitly stated that the variability in the response that is due to sampling variation (‘residual error’) is entirely driven by the variation that would be expected by a Binomial distribution. We do not have a separate “error model”. We have one model that contains information both on the expected (mean) value of the response for each value of the covariate, and on the amount of random noise in that response that we would expect from one data point to the next (for the same value of the covariate).

Poisson regression
------------

Poisson regression is used to model response variables that are counts. Poisson data includes:

* Number of plants is a given area

* Number of offspring for an individual

* Number of bacterial colonies in a Petri dish

The underlying model of the data is

$$
Y_i \sim \mathrm{Poisson}(\lambda_i)
$$

**Question: What are the issues with modeling Poisson data as Normal?**

If you fit a normal linear regression to Poisson data, your regression might predict values that are negative. Also, the variance for Poisson is equal to the mean (the Normal has no such restriction).

For Poisson data, the link function $g()$ is

$$
g (\lambda_i) = \log (\lambda_i)
$$
This means that we map the linear model $\beta_0 + \beta_1 X_i$ to the parameter of the Poisson using a log function. Why use a log function? We want to map values that are non-negative integers to $[- \infty, \infty]$.

The variance of $Y_i$ for a Poisson regression is:

$$
\mathrm{Var}(Y_i) = \lambda_i
$$

The complete model for a Poisson regression with data $Y_i$ and covariate $X_i$ is:

$$
Y_i \sim \mathrm{Poisson}(\lambda_i) \text{, where } \log (\lambda_i) = \beta_0 + \beta_1 X_i
$$

To fit this GLM using maximum likelihood, we could plug in $\lambda_i = \exp(\beta_0 + \beta_1 X_i)$ to the likelihood function for a Poisson distributed variable.

$$
\mathcal{L}(Y_i | \lambda_i) = \prod^N_{i = 1} \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
$$
As before, we solve for the parameters $\beta_{0}$ and $\beta_{1}$ by plugging into the likelihood $\lambda_{i} = e^{\beta_{0}+\beta_{1}X_{i}}$, taking the partial derivatives with respect to $\beta_{0}$ and $\beta_{1}$, respectively, setting those partial derivatives to zero, and solving for $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$.

By modeling our data as it exists, we have solved three problems:

1. The errors now reflect the Poisson process

2. The variance reflects the Poisson process

3. The response is now restricted to non-negative integers

However, there are things you need to understand about your data before fitting a Poisson regression:

1. Data are often **overdispersed**, or the variance is larger than the mean. To resolve this, you can add an additional term to the model. These models are called **quasi-poisson models**.

2. Data may have an unusually large number of zeros. To resolve this, you can fit a **zero-inflated Poisson model**, which models two components, the zero component (as a logistic regression), and the Poisson regression component. These classes of models (with multiple distributions) are more generally called mixture models.

These types of Poisson regressions are common in ecology and evolution!

Deviance
-------------

When we were fitting linear regression models, we assessed model fit using the coefficient of determination $R^{2}$. To measure fit of GLMs we use the **deviance**. Conceptually, it is not all that different from other fit metrics, and is analogous to the residual sum of squares for a linear model (actually it is equal to the residual sum of squares, see table 9.3 in Aho).

The deviance compares the NLL of the proposed model with the NLL of the "saturated" model. The saturated model is a model in which the predicted mean response matches each data point exactly, with one parameter for each observation. Note that the saturated model is a theoretical construct used to calculate deviance, not a model that you actually fit yourself.

Deviance is calculated as:

$$
D = 2 (\ell_{\text{saturated model}} - \ell_{\text{proposed model}})
$$
If the proposed model fits the data nearly as well as the saturated model, then

$$
D | H_0 \sim \chi^2_{n - p}
$$
The degrees of freedom for the chi-square distribution is calculated using $n$, the number of parameters in the saturated model (usually the number of data points), and $p$, the number of parameters in the proposed model. The saturated model has a likelihood greater than or equal to your proposed model, so the deviance is positive. We calculate the *P*-value as $P(D | H_0 \ge D_{\text{observed}})$ (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the model fit is poor.

In addition to assessing how well the model fits the data, we can use deviance to compare two models.  We can use deviance to compare **nested** models. When one model is a reduced version of another model, the models are nested.

When comparing nested models, we no longer need to use the idea of the saturated model. We just compare the *difference* in deviance between the two nested models.

$$
\Delta D = 2 (\ell_{\text{larger model}} - \ell_{\text{smaller model}}) = -2 (\ell_{\text{smaller model}} - \ell_{\text{larger model}})
$$
Under the null hypothesis that the smaller model is the true model:

$$
\Delta D | H_0 \sim \chi^2_{\text{additional parameters in larger model}}
$$
Models with more parameters have higher likelihood, because any additional parameters allow the model more "freedom" to fit the data. This means that the difference in log likelihoods ($\ell_{\text{larger model}} - \ell_{\text{smaller model}}$) will be positive and therefore the deviance difference will be negative. 

Adding additional parameters **always** improves model fit. BUT, we want to know whether the fit improves above and beyond what would be expected purely by adding additional parameters to the model? We calculate the *P*-value as $P(\Delta D | H_0 \ge \Delta D_{\text{observed}})$ (notice that this is a one-tailed test). If deviance (observed test statistic) is larger than predicted by the chi-square distribution (our distribution of the test statistic under the null hypothesis), the larger model fits the data above and beyond what we would expect.

We already used this concept of deviance to construct 95% confidence intervals on a maximum likelihood estimate!

From Week 4's Problem Set, "Using your function for the negative log-likelihood, calculate the MLE for $\lambda$ and the 95th percentile confidence interval. What would the 99th percentile confidence interval be? (4 pts) [Hint: The 95th percentile cut-off is $0.5 \times \chi^2_{df = 1}(0.95)$ or `0.5 * qchisq(0.95, df = 1) = 1.92` in R; in other words, the NLL would have to be >1.92 higher than the minimum to fall outside the 95th percentile confidence interval. Likewise, the 99th percentile cut-off is $0.5 \times \chi^2_{df = 1}(0.99) = 3.32$.]

So, you used the difference in NLL required to find the 95% CIs, but now you can see where that comes from.

An alternative approach to assessing the “importance” of a variable is to look at the test statistic for the model parameters the same way that we did for ordinary linear regression.

$$
z=\frac{\mbox{parameter estimate}}{\mbox{s.e. of parameter estimate}}
$$

The test statsitic z is approximately normal for large sample sizes.

Other methods -- LOESS, splines, GAMs
-------------------

Traditional regression methods are incredibly useful for predicting or explaining empirical data. However, in some situations, these methods are not flexible enough. For these cases, there are non-parametric approaches to curve fitting.

These methods may be useful for:

* Visualization of patterns

* Interpolating discrete data

* Identifying a threshold

* Prediction (e.g., niche modeling)

Keep in mind that LOESS, splines, and GAMS do not produce models that are straightforward and mechanistic. It is difficult/impossible to explain what the coefficients they produce actually mean.

\subsection{LOESS}

LOESS (LOWESS), or locally weighted regression, is a non-parameteric method for fitting a curve through data. The dependent variable is fit in a moving fashion (like a moving window average)

$$
Y_i = g(X_i) + \epsilon_i \text{, where } \epsilon_i \sim \mathrm{N} (0, \sigma^2)
$$

The only assumption about the function $g()$ is that it is a smooth function. In LOESS, the regression curve is fit using a moving window and weighted linear regression where each point in the neighborhood is weighted according to its distance from $X$.

$$
Y_i = \beta_0 + w_1(X_i - X_0) + w_2 (X_i - X_0)^2 + ... + w_p (X_i - X_0)^p + \epsilon_i
$$

```{r fig.width=4, fig.height=3}
bomregions2012<-read.csv("/Users/meganwyatt/Desktop/Biometry2021-master/_data/bomregions2012.csv")
ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() + geom_smooth(method = "loess") +
    labs(x = "Year", y = "Northern rainfall", parse = TRUE) + 
  theme(text = element_text(size = text.size)) + theme_classic()
```

\subsection{Splines}

A smoothing spline is a model whose metric of fit is the sum of a measure of residuals, as well as a measure of roughness:

$$\sum^n_{i = 1} (Y_i - f(X_i))^2 + h \int^{x_{\text{max}}}_{x_{\text{min}}} f''(X_i)^2 dx$$

The first term is the residual sum of squares, and the second term is a roughness penalty. $h$ is a smoothing parameter. A larger $h$ means a smoother line, because curves are more heavily penalized. A curve with $h = 0$ will just interpolate the data and a curve with a very large $h$ will fit a linear regression. The endpoints of the integral enclose the data.
 
```{r fig.width=4, fig.height=3}
ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() +
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 10), se = FALSE) +
  labs(x = "Year", y = "Northern rainfall", parse = TRUE) + 
  theme(text = element_text(size = text.size)) + theme_classic()
```

\subsection{GAMs}

Generalized additive models (GAMs) allow us specify both smoothing and conventional parametric terms for models. GAMs are compatible with nonlinearity and nonnormal errors by using link functions (like with GLMs).  With one covariate:

$$Y = \beta_0 + g(X)$$

This function, $g()$, could be smoothing splines, LOESS smoothers, kernel smoothers, etc.

```{r fig.width=4, fig.height=3}
ggplot(data = bomregions2012, aes(x = Year, y = northRain)) + geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs")) +
  labs(x = "Year", y = "Northern rainfall", parse = TRUE) + 
  theme(text = element_text(size = text.size)) + theme_classic()
```

Personally, I really dislike GAMs and would discourage their use. GAMs rarely if ever provide information that you couldn't see with the naked eye, and their use lends an air of statistical rigor to an analysis that is usually unjustified. I often see GAMs used as a crutch to avoid thinking seriously about the statistical model, and it tends to produce features that are artifacts of the data rather than meaningful information about the underlying process.

\subsection{Multiple regression}

Multiple regression is **not** fundamentally different from what we have discussed in the past three weeks. The major differences are that we need to be careful about what covariates we include, as well as our interpretations of linear model coefficients. *Note that covariates, predictors, and explanatory variables all refer to the same thing, what I've referred to in linear regression as $X_i$.*

Multiple regression is useful to:

1) Build better predictive models

2) Investigate the relative effects of each covariate standardized across the effects of the other covariates. With multiple regression, we use **partial regression lines**, where the slope of any single partial regression line is the effect of that covariate *holding all the other covariates at their mean value.*

$$
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + ... + \beta_{p - 1} X_{(p - 1)i} + \epsilon_i
$$

Where $Y_i$ is the response for the $i^{\text{th}}$ data point, $X_{(p - 1)i}$ is the covariate $p - 1$ for the $i^{\text{th}}$ data point. There are $p$ parameters in the model ($p - 1$ partial regression slopes and one intercept).

Note that we could write this is matrix form like we did with the linear models in Week 8:

$$\mathbf{Y} = 
\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix}
\mathbf{X} =
\begin{bmatrix}
  1 & X_{11} & ... & X_{(p - 1)1} \\
  1 & X_{12} & ... & X_{(p - 1)2} \\
  1 & \vdots & \ddots & \vdots \\
  1 & X_{1n} & ... & X_{(p - 1)n}
\end{bmatrix}
\mathbf{b} = 
\begin{bmatrix}
  \beta_0 \\
  \beta_1 \\
  \vdots \\
  \beta_{p - 1}
\end{bmatrix}
\mathbf{e} = 
\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}$$

The null hypothesis for each of the regression coefficients is: $\beta_0 = 0$ (the intercept is equal to zero), $\beta_1 = 0$, ..., $\beta_{p - 1} = 0$ (the partial regression slope of $X_{p - 1}$ on $Y$ is zero). Multiple regression is fit the exact same way as simple linear regression (see Aho 9.5.1).

The first three assumptions are the same as linear regression with one covariate. The final assumption is new and relevant only with multiple covariates.

1. Linearity (A linear model appropriately describes the relationship between $X$ and $Y$)

2. Normality (For any given value of $X$, the sampled $Y$ values are independent with normally distributed errors)

3. Homogeneity of variances (Variances are constant along the regression line)

4. The covariates, $X_1, X_2, ..., X_{p-1}$, are uncorrelated. **Multicollinearity** occurs when covariates are correlated with one another. 

Multicollinearity does NOT bias parameter estimates! But it still can be a big problem.

1. It causes instability of estimated partial regression slopes, meaning that small changes in the data cause large changes in the parameter estimates.

2. It inflates standard errors and confidence intervals of parameter estimates, increasing the chance of a Type II error. ANother way to say this is that multicollinearity reduces power, or increases the chance that you won't find a significant result even if the null hypothesis is false.

For simple linear regression with a single covariate, the slope parameter had the following sampling distribution:

$$
\hat{\beta_1} \sim \mathrm{N} {\left( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_i - \bar{X})^2} \right)}
$$

**Review Question: What is $\sigma_\epsilon^2$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The population error variance (the expected value of squared errors). In practice, we estimate this as $s^2_\epsilon$ from the data. It's also called the mean squared error.
</span>
</details> 

When we have multiple covariates, the sampling distribution for the partial slope parameter (the slope for the $X_1$ covariate, for example) is:

$$
\hat{\beta_1} \sim \mathrm{N} {\left ( \beta_1, \frac{\sigma_\epsilon^2}{\sum_{i = 1}^n (X_{1i} - \bar{X_1})^2} \frac{1}{1 - R^2_1} \right)}
$$

where $R^2_1$ is the multiple coefficient of determination, or the fraction of variance in the covariate 1 explained by all other covariates, $X_1 \sim X_2 + X_3 + ... + X_{p - 1}$. 

The bigger that $R^2_1$ is, the larger the standard error of $\hat{\beta_1}$:

```{r}
Var.inflation <- function(R2.1) {
  1 / (1 - R2.1)
}
Var.inflation(0.1)
Var.inflation(0.6)
Var.inflation(0.9)
```

How do we diagnose multicollinearity?

1. Look at the pairwise correlations between all of he predictor variables either by a correlation matrix or by a scatterplot matrix (try the package `corrplot`)

2. Calculate the Variance Inflation Factor. A large VIF may be > 5 or > 10 (depends who you ask). Two covariates have to be very correlated for this to be an issue.

$$
\text{VIF} = \frac{1}{1 - R^2_{p - 1}}
$$
3. Do a principal components analysis (PCA) to see which covariates are highly correlated (we will get into this Week 14).

How do we deal with multicollinearity?

1. If the goal of your model is **prediction**, multicollinearity isn't necessarily a problem. It may be best to leave all predictors in the model, even if they are collinear.

2. Remove the highly correlated predictors, starting with the least biologically interesting ones

3. Do a PCA, because the principal components are guaranteed to be orthogonal (completely uncorrelated). However, this does make hypothesis testing difficult, because the parameters will be combinations of the original variables.

How can we test hypotheses about model parameters in multiple regression?

1. Compare individual parameters using a *t*-test

$$
\frac{\hat{\beta_1} - \beta_{1 | H_0}}{\text{SE}_{\hat{\beta_1}}} \sim t_{df}
$$

How many degrees of freedom do we have for each model parameter? Let's jump back to the distribution for the partial slope of covariate $X_1$ out of $p - 1$ total covariates. In our estimate of the unknown error variance $\sigma^2_\epsilon$ (back from Week 9 lecture):

$$
s_\epsilon^2 = \frac{1}{n - p} \sum_{i = 1}^n (Y_i - \hat{Y_i})^2
$$

Calculating $\hat{Y_i}$ requires $\bar{Y}$ as well as the mean of each covariate ($\bar{X_1}, \bar{X_2}, ..., \bar{X}_{p - 1}$), so we lose $p$ degrees of freedom.

2. Compare the full model to the reduced model in which that particular term has been removed:

$$
\text{Full model: } Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \epsilon_i
$$

$$
\text{Reduced model: } Y_i = \beta_0 + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \epsilon_i
$$

**Question: How can we compare these two models?**

Since the models are nested, we can use the deviance difference.

Week 11 Lecture
=============

Outline:

1. Basic idea behind ANOVA

2. Single-factor ANOVA

3. Fixed effects (Model I ANOVA) vs. Random effects (Model II ANOVA)

4. Follow up analyses to ANOVA

The model structure of ANOVA is identical to a linear regression of categorical covariate(s). Specifically, ANOVA involves statistics used to estimate group/treatment effects. 

$$
Y_{ij} = \mu + A_i + \epsilon_{ij} \text{, where } \epsilon_{ij} \sim \mathrm{N}(0, \sigma^2)
$$

The value of each data point, $Y_{ij}$ is modeled as a function of: 1) the overall mean $\mu$, 2) the modification of the mean due to membership in group $i$, $A_i$, and 3) the unexplained variability remaining after the group-level effect is $\epsilon_{ij}$. Variation **among** groups is represented by $A_i$ and variation **within** groups is represented by $\epsilon_{ij}$. The statistical question is whether or not “group” has a statistically significant effect on the overall mean $\mu$. This is identical to the linear models we have already talked about, except that now we are focusing on models in which the predictor variables are categorical factors (or groups). 

**Question: What is the null hypothesis here?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Under the null hypothesis, this linear model can be written

$$
Y_{ij}=\mu+\epsilon_{ij}
$$

So the null hypothesis being tested is $H_{0}$: The differences **among** groups is no larger than would be expected based on the differences **within** groups. Therefore, there are no significant differences among the groups.
</span>
</details> 

<p>&nbsp;</p>

So, do the groups all come from the same population? Based on what we know already know, we might try using pairwise t-tests. But wait! If we use pairwise *t*-tests to determine which groups were different, we would have $a(a - 1) / 2$ pairwise comparisons with $a$ group means. 

**Question: Why is this ill advised?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Multiple comparisons will inflate the familywise error rate.
</span>
</details> 

<p>&nbsp;</p>

ANOVA is an **omnibus test**, meaning that if we reject the null hypothesis, we only know that **at least one group is different**. We do not know which groups in particular are different. 

Omnibus tests are very useful to keep familywise error rates down while testing many groups.

So, our null and alternative hypothesis are:

$H_0$: The groups come from the same population

$H_A$: At least one of the groups comes from a different population

An ANOVA with one categorical predictor with two levels is identical to a two sample *t*-test.

Before we discuss the mathematics behind ANOVA, we will start with a conceptual example adapted from the McKillup textbook (the chapter is posted in "Additional Readings").

We are studying the effect of experimental drugs on the growth of brain tumors in humans. We have 12 experimental patients with brain tumors. We assign four to a control group (no drug), we treat four with Tumostat, and four with Inhibin 4. We are measuring (our response variable is) tumor diameter.

We want to know whether there is a difference in the mean tumor diamerer among the drug treatments.

Imagine our results look like this:

```{r echo=FALSE, warning=FALSE, fig.height=3, fig.width=4}
library(ggplot2)
set.seed(4981)
text.size <- 16
replicates <- 4
treatment.names <- c("Control", "Tumostat", "Inhibin 4")
y <- rnorm(replicates * length(treatment.names))
x <- rep(treatment.names, each = replicates)
dat <- data.frame(Treatment = x, TumorDiameter = y)
dat$GroupMeans <- rep(c(mean(dat$TumorDiameter[dat$Treatment == "Control"]), 
                       mean(dat$TumorDiameter[dat$Treatment == "Tumostat"]),
                       mean(dat$TumorDiameter[dat$Treatment == "Inhibin 4"])), 
                      each = replicates)
dat$Treatment <- factor(dat$Treatment, levels = treatment.names)

ggplot(data = dat, aes(x = Treatment, y = TumorDiameter)) + geom_point() + 
  geom_hline(aes(yintercept = mean(TumorDiameter))) +
  geom_segment(aes(y = GroupMeans, yend = GroupMeans, x = as.numeric(Treatment) - 0.2, xend = as.numeric(Treatment) + 0.2)) +
  annotate(geom = "text", label = "Grand mean", x = 3.2, y = mean(dat$TumorDiameter) + 0.1) + 
  labs(x = "Treatment", y = "Tumor diameter") + 
  theme_classic() + theme(text = element_text(size = text.size))
```

How do we test our hypothesis? First let's relate the figure back to the equation: $Y_{ij}$ are each of the individual data points (e.g., $Y_{21}$ is the first individual in treatment 2, Tumostat). The horizonal line is equal to $\mu$, and $A_i$ is equal to the difference between $\mu$ and the treatment means (narrow horizontal lines).

Notice that there is variation at two levels: 1) variation among individuals within a given treatment group, and 2) variation among the three treatment group means. **We can partition the variance into these two components.**

### Variation within treatment group

Variation among individuals within a treatment group is residual variation. This variation exists because of any number of factors we weren't able to measure.

```{r echo=FALSE, warning=FALSE, fig.height=5, fig.width=6}
arrow.space <- c(-0.1, -0.05, 0.05, 0.1)
ggplot(data = dat, aes(x = Treatment, y = TumorDiameter)) + geom_point() + 
  geom_hline(aes(yintercept = mean(TumorDiameter))) +
  geom_segment(aes(y = GroupMeans, yend = GroupMeans, x = as.numeric(Treatment) - 0.2, xend = as.numeric(Treatment) + 0.2)) +
  annotate(geom = "text", label = "Grand mean", x = 3.2, y = mean(dat$TumorDiameter) + 0.1) + 
  geom_segment(aes(x = as.numeric(Treatment) + arrow.space, 
                   xend = as.numeric(Treatment) + arrow.space, y = GroupMeans, yend = TumorDiameter), size = 0.5,
               arrow = arrow(length = unit(0.2, "cm"))) +
  labs(x = "Treatment", y = "Tumor diameter") + 
  theme_classic() + theme(text = element_text(size = text.size))
```

### Variation among treatment group means

Variation among treatment means is variation due to the effect of the treatment (if there is an effect) *in addition to the individual variation*.

```{r echo=FALSE, warning=FALSE, fig.height=5, fig.width=6}
ggplot(data = dat, aes(x = Treatment, y = TumorDiameter)) + geom_point() + 
  geom_hline(aes(yintercept = mean(TumorDiameter))) +
  geom_segment(aes(y = GroupMeans, yend = GroupMeans, x = as.numeric(Treatment) - 0.2, xend = as.numeric(Treatment) + 0.2)) +
  annotate(geom = "text", label = "Grand mean", x = 3.2, y = mean(dat$TumorDiameter) + 0.1) + 
  geom_segment(aes(x = as.numeric(Treatment) + 0.05, 
                   xend = as.numeric(Treatment) + 0.05, y = GroupMeans, yend = mean(TumorDiameter)), size = 0.5,
               arrow = arrow(length = unit(0.2, "cm"))) +
  labs(x = "Treatment", y = "Tumor diameter") + 
  theme_classic() + theme(text = element_text(size = text.size))
```

### Comparing variance components

We can relate these two components of variation to one another, and this is our test statistic. Informally (we will get into the mathematical formulas soon) we can write this as:

$$
\frac{\text{Among group variance (group effect + error)}}{\text{Within group variance (error)}}
$$

**Question: Approximately what is this ratio equal to under the null hypothesis for ANOVA?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Under the null hypothesis, we assume there is no group effect. So, the ratio is about 1.
</span>
</details>  

<p>&nbsp;</p>

Let's imagine our data looked slightly different:

```{r echo=FALSE, warning=FALSE, fig.height=5, fig.width=6}
set.seed(4981)
treatment.names <- c("Control", "Tumostat", "Inhibin 4")
y <- rnorm(replicates, mean = 1, sd = 0.5)
y <- c(y, rnorm(replicates * 2, mean = -1, sd = 0.5))
x <- rep(treatment.names, each = replicates)
dat <- data.frame(Treatment = x, TumorDiameter = y)
dat$GroupMeans <- rep(c(mean(dat$TumorDiameter[dat$Treatment == "Control"]), 
                       mean(dat$TumorDiameter[dat$Treatment == "Tumostat"]),
                       mean(dat$TumorDiameter[dat$Treatment == "Inhibin 4"])), 
                      each = replicates)
dat$Treatment <- factor(dat$Treatment, levels = treatment.names)

ggplot(data = dat, aes(x = Treatment, y = TumorDiameter)) + geom_point() + 
  geom_hline(aes(yintercept = mean(TumorDiameter))) +
  geom_segment(aes(y = GroupMeans, yend = GroupMeans, x = as.numeric(Treatment) - 0.2, xend = as.numeric(Treatment) + 0.2)) +
  annotate(geom = "text", label = "Grand mean", x = 3.2, y = mean(dat$TumorDiameter) + 0.1) + 
  geom_segment(aes(x = as.numeric(Treatment) + arrow.space, 
                   xend = as.numeric(Treatment) + arrow.space, y = GroupMeans, yend = TumorDiameter), size = 0.5,
               arrow = arrow(length = unit(0.2, "cm"))) +
    geom_segment(aes(x = as.numeric(Treatment) + 0.05, 
                   xend = as.numeric(Treatment) + 0.05, y = GroupMeans, yend = mean(TumorDiameter)), size = 0.5,
               arrow = arrow(length = unit(0.2, "cm")), color = "dodgerblue3") +
  labs(x = "Treatment", y = "Tumor diameter") + 
  theme_classic() + theme(text = element_text(size = text.size))
```

How much variation is there among groups relative to within groups? Without knowing the details yet, do you think there is a significant difference among groups?


Comparing variance components
-----------------

**Question: Given that we are using null hypothesis significance testing methods, what do you think the next step is after we estimate the test statistic?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Compare the value of the test statistic to the distribution of the test statistic under the null hypothesis.
</span>
</details> 

<p>&nbsp;</p>

**Question: The distribution of the test statistic under the null hypothesis describes the ratios of variances. What distribution do you think it is?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The F distribution (see Week 5 lecture)
</span>
</details>  

<p>&nbsp;</p>

Another way we can phrase the statistical question is to ask whether the differences among group means are significantly different from the differences seen within groups. *In ANOVA, we compare variances in order to compare means.*

Two ways to estimate variance
--------------

1) Variation within groups

We can calculate the within-group variance as an average of the within group variances from each group. In other words:

$$
\sigma^2 = \frac{1}{a} (\text{Within-grp var grp }1 +\text{Within-grp var for grp }2+...+\text{Within-grp var group }a)
$$
This can be re-written as:

$$
\sigma^2 = \frac{1}{a} \sum_{i = 1}^a \text{Within-grp var group }i
$$
There is actually a second way we can calculate the within-group variance:

2) Variation among groups

Under $H_0$, the data in each group are an independent sample from the same underlying population. We can calculate a mean for each of the $a$ groups. Here we are assuming that each group has the same number ($n$) of data points. (This is called a "balanced design". More on this in a bit.) 

We can use the Central Limit Theorem to estimate the variance. We calculate the variation in means as: $\sigma^2_{\bar{Y}} = \frac{\sigma^2}{n}$, where $\sigma^2_{\bar{Y}}$ is our estimated variance of the group means and $\frac{\sigma^2}{n}$ is our overall uncertainty divided by $n$ (thus, our ability to estimate $\mu$ with $\bar{Y}$ improves with increasing sample size). 

We can rearrange this to get a second formula for the within-group variance:

$$
\sigma^2 = n \sigma^2_{\bar{Y}}
$$

**Under $H_0$, both ways of calculating variance (among groups using CLT or within groups as an average of group variance) will give equivalent estimates.**

$$
\text{Under H}_o \rightarrow n \sigma^2_{\bar{Y}} = \frac{1}{a} \sum_{i = 1}^a \text{Within-group variance in group }i
$$

**However, if there are true group differences, the variation among groups will be large compared to the variation within groups.**

Now we'll use our example of the tumor diameter data with **no true treatment differences** and run simulations of the experiment with 100 samples in each treatment in order to estimate and compare variance within groups and variance among groups.

```{r}
set.seed(3467)
replicates <- 100
var.among <- c()
var.within <- c()
for (i in 1:1000) {
  y <- rnorm(replicates * length(treatment.names))
  x <- rep(treatment.names, each = replicates)
  dat <- data.frame(Treatment = x, TumorDiameter = y)
  dat$GroupMeans <- rep(c(mean(dat$TumorDiameter[dat$Treatment == "Control"]), mean(dat$TumorDiameter[dat$Treatment == "Tumostat"]), mean(dat$TumorDiameter[dat$Treatment == "Inhibin 4"])), each = replicates)
  dat$Treatment <- factor(dat$Treatment, levels = treatment.names)
  var.among[i] <- replicates * var(unique(dat$GroupMeans))
  v1 <- var(dat$TumorDiameter[which(dat$Treatment == "Control")])
  v2 <- var(dat$TumorDiameter[which(dat$Treatment == "Tumostat")])
  v3 <- var(dat$TumorDiameter[which(dat$Treatment == "Inhibin 4")])
  var.within[i] <- (1 / 3) * sum(c(v1, v2, v3))
}
mean(var.among)
mean(var.within)
```

## Single-factor ANOVA

First we will start will the simplest ANOVA type, where we have a single factor (discrete covariate). We will work through an example involving the heights of plants, $Y$, under three different experimental treatments ($i$), low nitrogen (N), ambient N, and high N. We have four replicates in each treatment. We index the heights as $Y_{ij}$, where $i$ is treatment $i = (1, ..., a)$ and $j$ is the individual plant $j = (1, ..., n)$. There will be a lot of variation in plant heights for a million reasons we can't measure. But, we want to partition the variance into 1) the variance due to individual fluctuations ("error"), and 2) the variance due to the treatment (nitrogen level).

This is a single-factor (one-way) ANOVA because we have one grouping category, nitrogen treatment, with three mutually exclusive levels (low N, ambient N, high N).

Our data is as follows:

```{r echo=FALSE}
library(knitr)
plant.dat <- data.frame(Height = c(10, 12, 12, 13, 9, 11, 11, 12, 12, 13, 15, 16),
                        Treatment = rep(c("low N", "ambient N", "high N"), each = 4))
kable(plant.dat, align = "l")
```

Week 12 Lecture
=============

```{r include=FALSE, warning=FALSE}
library(ggplot2)
text.size <- 16
```

Outline:

1. ANOVA for two-way factorial design

2. 

   (a) Dealing with unbalanced design – unequal sample sizes: Type I, II, and III SS

   (b) Dealing with unbalanced design- missing cells
    
3. ANOVA for two-way nested design

4. Some more experimental design

Review: ANOVA with one factor
----------------

With single factor ANOVA, we compared multiple levels of a factor. Another way to say this is that we only had one categorical covariate in our linear model (e.g., the effect of zinc concentration on diatom diversity).

```{r echo=FALSE, fig.height=4, fig.width=4}
medley <- read.csv("/Users/meganwyatt/Desktop/Biometry2021-master/_data/medley.csv", header = TRUE)
medley$ZINC <- factor(medley$ZINC, levels = c("HIGH", "MED", "LOW", "BACK"))
ggplot(data = medley, aes(x = ZINC, y = DIVERSITY)) + geom_boxplot() +
  labs(x = "Zinc level", y = "Diatom diversity") + theme_classic() + 
  theme(text = element_text(size = text.size))
```

### ANOVA with more than one factor

With two-way (and higher) ANOVA, we look at more than one factor at a time (e.g., the effect of density and season on limpet egg production). There are different ways in which multiple factors can be modeled, and this depends on the design of your study. We will focus our discussion primarily on two ways in which multiple factors can be applied in an experimental design: Nested vs Factorial. Note that in some complicated experiments, there may be elements that are nested and others that are factorial. This will become clear as we work through some examples.

In this example below, the factors are crossed (two-way factorial design).

```{r echo=FALSE, fig.height=4, fig.width=5.5}
quinn <- read.csv("/Users/meganwyatt/Desktop/Biometry2021-master/_data/quinn.csv", header = TRUE)
quinn$DENSITY <- factor(as.character(quinn$DENSITY), levels = c("8", "15", "30", "45"))
ggplot(data = quinn, aes(x = DENSITY, y = EGGS)) + 
  facet_grid(. ~ SEASON) + geom_boxplot() +
  labs(x = "Density", y = "Num. egg masses/limpet") + theme_classic() + 
  theme(text = element_text(size = text.size))
```

The two factors are fully crossed, or all combinations of factors are included in the design and every level of every factor occurs in combination with every level of the other factors.

We will start with factorial designs because these are the most logical extension of the one-way analyses we discussed last week.

### Two-way ANOVA factorial designs

In the previous example, the number of egg masses per limpet was measured at different densities (factor A) and in different seasons (factor B). Two-way factorial designs can be represented in a table, where each cell is a combination of factor A and factor B. Each cell has multiple replicates.

| Factor level | $B_1$        | $B_2$       |
| ------------------- |:---------:|:---------:|
| $A_1$ | 8 animals/enclosure, spring | 8 animals/enclosure, summer |
| $A_2$ | 15 animals/enclosure, spring | 15 animals/enclosure, summer |
| $A_3$ | 30 animals/enclosure, spring | 30 animals/enclosure, summer |
| $A_4$ | 45 animals/enclosure, spring | 45 animals/enclosure, summer |

In a two-way factorial design, adding in an additional factor gives us a new issue to contend with. We are trying to measure the effect of factor A, marginalizing across factor B, or the effect of factor B, marginalizing across factor A. The marginal means for density (averaged across both seasons) and for season (averaged across all densities) are shown:

```{r echo=FALSE, fig.height=3, fig.width=3}
ggplot(data = quinn, aes(x = DENSITY, y = EGGS)) + 
  geom_boxplot() +
  labs(x = "Density", y = "Num. egg masses/limpet") + theme_classic() + 
  theme(text = element_text(size = text.size))
ggplot(data = quinn, aes(x = SEASON, y = EGGS)) + 
  geom_boxplot() +
  labs(x = "Season", y = "Num. egg masses/limpet") + theme_classic() + 
  theme(text = element_text(size = text.size))
```

But, what if season has an effect on density, or, the two factors influence one another? With two-way designs we need to account for this. We call this potential $\text{factor} \times \text{factor}$ influence an **interaction**, which we include in our model as a parameter that allows for the effect of factor A to depend on factor B.

For a hypothetical example, let's say we are measuring plant biomass in different fertilizer treatments (factor A, levels 1-3: low N, ambient N, and high N) and  watering treatments (factor B, levels 1-4: no water, low water, ambient water, excess water), where $Y_{ijk}$ is the $k^{\text{th}}$ individual with fertilizer treatment $i$ and watering treatment $j$. All possible combinations of treatments were measured. Interaction term ${AB}_{ij}$ allows for the effect of watering treatment to depend on fertilizer treatment (and vice versa).

$$
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2 )
$$

**Question: What is $A_i$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The effect of fertilizer treatment $i$, $\mu_i - \mu$. The effect of treatment $i$ is the mean for factor A, level $i$, pooling across all levels for factor B.
</span>
</details> 

<p>&nbsp;</p>

**Question: How do we write the model for the 2nd individual plant in the fertilizer treatment "low N" and the watering treatment "excess water"?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
$Y_{142} = \mu + A_1 + B_4 + {AB}_{14} + \epsilon_{142} \text{, where } \epsilon_{142} \sim \mathrm{N} ( 0, \sigma^2 )$
</span>
</details> 

<p>&nbsp;</p>

Each level of one factor is applied to all levels of the other factor, and all combinations are replicated.

Factor A, fertilizer treatment, has 3 levels and factor B (watering treatment) has 4 levels, so we have 12 possible combinations of factor A and B that must be represented. In a balanced design, you would have the same number of samples in each of the 12 combinations of factors A and B.

| Factor level | $B_1$        | $B_2$       | $B_3$        | $B_4$         |
| ------------------- |:---------:|:---------:|:---------:|:---------:|
| $A_1$ | $A_1 ,  B_1 = Y_{111}$ <br> $A_1 ,  B_1 = Y_{112}$ <br> $A_1 ,  B_1 = Y_{113}$| $A_1 ,  B_2 = Y_{121}$ <br> $A_1 ,  B_2 = Y_{122}$ <br> $A_1 ,  B_2 = Y_{123}$ | $A_1 ,  B_3 = Y_{131}$ <br> $A_1 ,  B_3 = Y_{132}$ <br> $A_1 ,  B_3 = Y_{133}$ | $A_1 ,  B_4 = Y_{141}$ <br> $A_1 ,  B_4 = Y_{142}$ <br> $A_1 ,  B_4 = Y_{143}$ |
| $A_2$ | $A_2 ,  B_1 = Y_{211}$ <br> $A_2 ,  B_1 = Y_{212}$ <br> $A_2 ,  B_1 = Y_{213}$ | $A_2 ,  B_2 = Y_{221}$ <br> $A_2 ,  B_2 = Y_{222}$ <br> $A_2 ,  B_2 = Y_{223}$ | $A_2 ,  B_3 = Y_{231}$ <br> $A_2 ,  B_3 = Y_{232}$ <br> $A_2 ,  B_3 = Y_{233}$ | $A_2 ,  B_4 = Y_{241}$ <br> $A_2 ,  B_4 = Y_{242}$ <br> $A_2 ,  B_4 = Y_{243}$ |
| $A_3$ | $A_3 ,  B_1 = Y_{311}$ <br> $A_3 ,  B_1 = Y_{312}$ <br> $A_3 ,  B_1 = Y_{313}$ | $A_3 ,  B_2 = Y_{321}$ <br> $A_3 ,  B_2 = Y_{322}$ <br> $A_3 ,  B_2 = Y_{323}$ | $A_3 ,  B_3 = Y_{331}$ <br> $A_3 ,  B_3 = Y_{332}$ <br> $A_3 ,  B_3 = Y_{333}$ | $A_3 ,  B_4 = Y_{341}$ <br> $A_3 ,  B_4 = Y_{342}$ <br> $A_3 ,  B_4 = Y_{343}$ |


**Question: How would you find the mean biomass for factor B, level 1 (no water)?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
$\bar{Y}_j = \bar{Y}_1$ = mean of all biomass values in column 1
</span>
</details> 

<p>&nbsp;</p>

**Question: What about the mean for factor A, level 3 (high N)?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
$\bar{Y}_i = \bar{Y}_3$ = mean of all biomass values in row 3
</span>
</details> 

<p>&nbsp;</p>

**Question: What about the mean for factor A, level 2 AND factor B, level 4 (ambient N, excess water)?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
$\bar{Y}_{ij} = \bar{Y}_{24}$ = mean of all biomass values in the cell in row 2 and column 4
</span>
</details> 

<p>&nbsp;</p>

Interactions occur when: 

$$
\text{Effect of factor A alone} + \text{Effect of factor B alone} \neq \text{Effect of A and B together}
$$ 

Let's go through multiple examples of hypothetical two-way factorial ANOVA results to interpret the main effects and interactions in each outcome.

<p>&nbsp;</p>

**<span style="color: red;">Example #1</span>**

```{r echo = FALSE, fig.width=4, fig.height=4}
response <- c(1.5, 0.5, 1, 0.4)
factorA <- c("High", "Low", "High", "Low")
factorB <- c("High", "High", "Low", "Low")
dat <- data.frame(response, factorA, factorB)
dat$factorA <- factor(dat$factorA, levels = c("Low", "High"))
dat$factorB <- factor(dat$factorB, levels = c("Low", "High"))
ggplot(dat, aes(x = factorA, y = response, group = factorB)) + geom_line(aes(linetype = factorB)) +
  coord_cartesian(ylim = c(0, 2.1)) + labs(y = "Response", x = "Factor A") +
  scale_linetype_manual(name = "Factor B", values = c("dashed", "solid")) +
  theme_classic() +
  theme(text = element_text(size = text.size), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
        legend.justification = c(1, 1), legend.position = c(1, 1))
```

A large positive value for factor A is associated with an increase in the response variable, a large positive value for factor B is associated with an increase in the response variable, and there is an interaction of A $\times$ B, showing that B has little effect at low A, but a large effect at high A. We can see this more clearly in the diagram below. For evaluating the main effect of factor B (left panel), we average across ('marginalize out') all the A levels and see the impact of changing the B level. Likewise, evaluating the main effect of factor A (right panel) requires us to average across all the B levels. In both cases, when we average across the levels of the other factor, we see that both A and B are associated with a change in the mean response. (There is also an interaction here as well.)

```{r echo=FALSE, fig.align='center', fig.cap='The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Two_Way_Main_Effects_1.png')
```

<p>&nbsp;</p>

**<span style="color: red;">Example #2</span>**

```{r echo = FALSE, fig.width=4, fig.height=4}
response <- c(1.8, 0.8, 1.3, 0.3)
factorA <- c("High", "Low", "High", "Low")
factorB <- c("High", "High", "Low", "Low")
dat <- data.frame(response, factorA, factorB)
dat$factorA <- factor(dat$factorA, levels = c("Low", "High"))
dat$factorB <- factor(dat$factorB, levels = c("Low", "High"))
ggplot(dat, aes(x = factorA, y = response, group = factorB)) + geom_line(aes(linetype = factorB)) +
  coord_cartesian(ylim = c(0, 2.1)) + labs(y = "Response", x = "Factor A") +
  scale_linetype_manual(name = "Factor B", values = c("dashed", "solid")) +
  theme_classic() +
  theme(text = element_text(size = text.size), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
        legend.justification = c(1, 1), legend.position = c(1, 1))
```

Once again, we can see the main effects by averaging across the levels of the other factor. Here both A and B have a main effect on the response, but there is no interaction because the effect of factor A is the same for both levels of factor B (and vice versa).

```{r echo=FALSE, fig.align='center', fig.cap='The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Two_Way_Main_Effects_2.png')
```

<p>&nbsp;</p>

**<span style="color: red;">Example #3</span>**

```{r echo = FALSE, fig.width=4, fig.height=4}
response <- c(1.5, 1.5, 0.5, 0.5)
factorA <- c("High", "Low", "High", "Low")
factorB <- c("High", "High", "Low", "Low")
dat <- data.frame(response, factorA, factorB)
dat$factorA <- factor(dat$factorA, levels = c("Low", "High"))
dat$factorB <- factor(dat$factorB, levels = c("Low", "High"))
ggplot(dat, aes(x = factorA, y = response, group = factorB)) + geom_line(aes(linetype = factorB)) +
  coord_cartesian(ylim = c(0, 2.1)) + labs(y = "Response", x = "Factor A") +
  scale_linetype_manual(name = "Factor B", values = c("dashed", "solid")) +
  theme_classic() +
  theme(text = element_text(size = text.size), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
        legend.justification = c(1, 1), legend.position = c(1, 1))
```

Below we can see that there is no main effect for Factor A because when we average over the levels of B, A has no effect on the response (right panel). There is, however, a main effect of B (left panel). There is no interaction of A $\times$ B.

```{r echo=FALSE, fig.align='center', fig.cap='The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Two_Way_Main_Effects_3.png')
```

<p>&nbsp;</p>

**<span style="color: red;">Example #4</span>**

```{r echo = FALSE, fig.width=4, fig.height=4}
response <- c(2, 1, 1, 2)
factorA <- c("High", "Low", "High", "Low")
factorB <- c("High", "High", "Low", "Low")
dat <- data.frame(response, factorA, factorB)
dat$factorA <- factor(dat$factorA, levels = c("Low", "High"))
dat$factorB <- factor(dat$factorB, levels = c("Low", "High"))
ggplot(dat, aes(x = factorA, y = response, group = factorB)) + geom_line(aes(linetype = factorB)) +
  coord_cartesian(ylim = c(0, 2.1)) + labs(y = "Response", x = "Factor A") +
  scale_linetype_manual(name = "Factor B", values = c("dashed", "solid")) +
  theme_classic() +
  theme(text = element_text(size = text.size), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
        legend.justification = c(1, 1), legend.position = c(1, 1))
```

Below we see that Factor A has no main effect (right panel) and Factor B also has no main effect (left panel). There is, however, an interaction of A $\times$ B. In fact, the effect of A is negative for B=Low and positive for B=High. 


```{r echo=FALSE, fig.align='center', fig.cap='The green dots represent the average across Factor A for each B level. The pink dots represent the average across Factor B for each A level.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Two_Way_Main_Effects_4.png')
```

Let's continue interpreting two-way ANOVA plots of main effects and interactions with Aho Fig. 10.9. The response variable is biomass. Factor A, water level, is shown on the x-axis, and factor B, nutrient level, is shown with the line type, where a solid line represents added N, and a dashed line represents the control.

```{r echo = FALSE, fig.width=6, fig.height=6}
library(asbio)
Fig.10.9 <- function() {
par(mfrow=c(2,2))
par(mar=c(1.5,4.2,2.2,0.5))
##(a)
biomass<-c(12.7,13.7,14.7,15.7,16.7,17.7,13,14,15,16,17,18)
Fert<-rep(c("Co","Co","Co","N+","N+","N+"),2)
Water<-rep(c("Dry","Mesic","Wet"),4)
interaction.plot(x.factor=Water,trace.factor=Fert,
response=biomass,ylab="Mean biomass",xlab="",xaxt="n",legend=FALSE, cex.lab=1.5)
##(b)
par(mar=c(1.5,2.5,2.2,1.2))
biomass<-c(14.6,15.1,14.7,16.2,16.7,16.6,14.8,15.2,15,16.6,17,17.2)
Fert<-rep(c("Co","Co","Co","N+","N+","N+"),2)
Water<-rep(c("Dry","Mesic","Wet"),4)
interaction.plot(x.factor=Water,trace.factor=Fert,
response=biomass,ylab="Mean biomass",legend=FALSE,xlab="",xaxt="n", cex.lab=1.5)
##(c)
par(mar=c(4.2,4.2,0,0.5))
biomass<-c(14.6,15.1,14.7,16.2,16.7,12.6,14.8,15.2,15,16.6,17,12.2)
Fert<-rep(c("Co","Co","Co","N+","N+","N+"),2)
Water<-rep(c("Dry","Mesic","Wet"),4)
interaction.plot(x.factor=Water,trace.factor=Fert,
response=biomass,ylab="Mean biomass",legend=FALSE, cex.lab =1.5)
##(d)
par(mar=c(4.2,2.5,0,1.2))
biomass<-c(14.6,15.1,12.4,16.2,16.7,17.6,14.8,15.2,12,16.6,17,17.2)
Fert<-rep(c("Co","Co","Co","N+","N+","N+"),2)
Water<-rep(c("Dry","Mesic","Wet"),4)
interaction.plot(x.factor=Water,trace.factor=Fert,
response=biomass,ylab="Mean biomass",legend=FALSE,cex.lab=1.5)

mtext("(a)",3,outer = TRUE, at = .025, line = -1.8, cex = 1.6)
mtext("(b)",3,outer = TRUE, at = .52, line = -1.8, cex = 1.6)
mtext("(c)",3,outer = TRUE, at = .025, line = -18, cex = 1.6)
mtext("(d)",3,outer = TRUE, at = .52, line = -18, cex = 1.6)
}

Fig.10.9()
```

**Question: How would you interpret these results for each scenario (a,b,c, and d)?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
a) Water has a positive effect on biomass, Fertilization has a positive effect on biomass, and there is no significant interaction. b) There is no effect of water on biomass, Fertilization has a positive effect on biomass and there is no significant interaction. c) A significant interaction obscures the main effects. In wet conditions, Fertilizer has a negative effect on biomass. In dry conditions, Fertilizer has a positive effect on biomass. d) There is still an interaction, but it doesn't change (or obscure) the main effects. Ferilizer always has a positive effect on biomass. In wet conditions, this effect is more extreme. 
</span>
</details> 

<p>&nbsp;</p>

Let's come back to the equation for a two-factor factorial design:

$$
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2 )
$$

In the two-way factorial ANOVA model, there are separate null hypotheses for the two main effects and for the interaction term. The null hypotheses for the main effects (factors A and B) are: 

$$
H_0 (A): \mu_1 = \mu_2 = ... = \mu_i \text{ or } A_1 = A_2 = \dots = A_i = 0
$$

$$
H_0 (B): \mu_1 = \mu_2 = ... = \mu_j \text{ or } B_1 = B_2 = \dots = B_j = 0
$$

**Question: What's the difference between writing the null hypothesis as all $\mu_i$ equal vs. all $A_i = 0$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
First way: all population group means are equal, second way: all treatment effects (differences between group means and overall mean) are zero.
</span>
</details> 

<p>&nbsp;</p>

Null hypothesis for interaction (factors A and B)

$$
H_0 (AB): {AB}_{11} = {AB}_{12} = {AB}_{13} = \dots = {AB}_{ij} = 0
$$

Under the null hypothesis for the interaction term, the effects of factors A and B are additive. Another way to say this is, the effect of having factor A level 1 (low N treatment) and factor B level 3 (ambient water treatment) is equal to the sum of the effect of factor A and the effect of factor B.

| Source of variation | SS        | DOF       | MS        |
| ------------------- |:---------:|:---------:|:---------:|
| Among groups (factor A) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{i} - \bar{Y})^2$ | $a - 1$ | $\frac{\text{SS}_{A}}{\text{DOF}_{A}}$ |
| Among groups (factor B) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{j} - \bar{Y})^2$ | $b - 1$ | $\frac{\text{SS}_{B}}{\text{DOF}_{B}}$ |
| Interaction | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{ij} - \bar{Y}_{i} - \bar{Y}_{j} + \bar{Y})^2$ | $(a - 1) (b - 1)$ | $\frac{\text{SS}_{AB}}{\text{DOF}_{AB}}$ |
| Within groups (residual) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y}_{ij})^2$ | $ab (n - 1)$ | $\frac{\text{SS}_{\text{within}}}{\text{DOF}_{\text{within}}}$ |
| Total | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2$ | $abn - 1$ | |

Given our example of plant biomass ($Y$) under different fertilizer (factor A) and watering treatments (factor B).

**Question: What is $\bar{Y}$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The overall mean biomass, including all factors and levels.
</span>
</details> 

<p>&nbsp;</p>

**Question: What is $\bar{Y}_i$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The mean biomass in fertilizer (factor A) $i$, averaged across all watering treatments (factor B).
</span>
</details> 

<p>&nbsp;</p>

**Question: What is $\bar{Y}_j$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The mean biomass in watering treatment (factor B) $j$, marginalized across fertilizer treatments (factor A).
</span>
</details> 

<p>&nbsp;</p>

**Question: What is $\bar{Y}_{ij}$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The mean biomass in treatment combination fertilizer (factor A) $i$ and water level (factor B) $j$. We will refer to this as the **cell mean**.
</span>
</details> 

<p>&nbsp;</p>

**Question: What is going on with the sums of squares for factor A? There are no $k$ or $j$ subscripts in the formula $(\bar{Y}_{i} - \bar{Y})^2$, yet the summations $\sum^b_{j = 1}$ and $\sum^n_{k = 1}$ are included in the equation.**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
You end up multiplying the same value, $(\bar{Y}_{i} - \bar{Y})^2$ multiple times. The sums of squares for factor A is equal to $n b \sum^a_{i = 1} (\bar{Y}_{i} - \bar{Y})^2$ when you have a balanced design (equal number of replicates in each cell).
</span>
</details> 

<p>&nbsp;</p>

**Question: Why is $\text{DOF}_{A} = a - 1$ and  $\text{DOF}_{B} = b - 1$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
We estimate $a$ or $b$ group means, minus one for the overall mean.
</span>
</details> 

<p>&nbsp;</p>

**Question: Why is $\text{DOF}_{AB} = (a - 1) (b - 1)$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Multiply this out. $ab$ = number of combinations, we estimate $ab$ cell means, minus $a$, minus $b$ for each of the factor means, and minus one for the overall mean.
</span>
</details> 

<p>&nbsp;</p>

**Question: Why is $\text{DOF}_{\text{within}} = a b (n - 1)$?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
Multiply this out. $abn$ = total number of data points. Subtract $ab$ for the cell means.
</span>
</details> 

<p>&nbsp;</p>

The confusing sums of squares for the interaction can be rearranged to make more sense:

$$
\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{ij} - \bar{Y}_{i} - \bar{Y}_{j} + \bar{Y})^2 = \sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} ((\bar{Y}_{ij} - \bar{Y}) - (\bar{Y}_{i} - \bar{Y}) - (\bar{Y}_{j} - \bar{Y}))^2
$$

The interaction term represents the difference between the cell mean and overall mean relative to the difference between the main effects mean and the overall mean. This tells us how "special" this cell is relative to being in factor A alone or factor B alone.

To test the null hypothesis that factor A has no effect ($H_0(A): A_i = 0$), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: $P ( X \ge F^*), X \sim F_{[\text{DOF}_{A}, \text{DOF}_{\text{within}}]}$.

To test the null hypothesis that factor B has no effect ($H_0(B): B_j = 0$), $P ( X \ge F^*), X \sim F_{[\text{DOF}_{B}, \text{DOF}_{\text{within}}]}$.

Last, to test the null hypothesis that there is no interaction ($H_0(AB): AB_{ij} = 0$), $P ( X \ge F^*), X \sim F_{[\text{DOF}_{AB}, \text{DOF}_{\text{within}}]}$.

In our data, there is some total amount of variation:

$$
\text{SS}_{\text{total}} = \sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2
$$

We have partitioned the variance into the variance explained by factor A ($\text{SS}_{A}$), the variance explained by factor B ($\text{SS}_{B}$), the variance explained by the interaction between factor A and factor B ($\text{SS}_{AB}$), and the unexplained (residual) variance ($\text{SS}_{\text{within}}$).

So, **if our ANOVA is balanced**:

$$
\textbf{SS}_{\text{total}} = \textbf{SS}_{A} + \textbf{SS}_{B} + \textbf{SS}_{AB} + \textbf{SS}_{\text{within}}
$$
What if factors A and B are both random effects? Our null hypotheses would be:

$$
H_0: \sigma^2_A = 0 \text{ and } \sigma^2_B = 0
$$

These null hypotheses mean that there is no added variance due to the levels within factors A or B, respectively.

What about the interaction in this case?

$$
H_0: \sigma^2_{AB} = 0
$$

This means that there is no added variance due to the *combination* of A and B.

If both effects are random, most of the ANOVA table is exactly the same, however, the **F-ratio is calculated differently**.

| Source of variation | F ratio for fixed effects only  | F ratio for random effects only | F ratio for A random / B fixed |
| ------------------- |:-------------------------------:|:-------------------------------:|:-------------------------------:|
| Factor A | $\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}$ | $\frac{\text{MS}_{A}}{\text{MS}_{AB}}$ | $\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}$ |
| Factor B | $\frac{\text{MS}_{B}}{\text{MS}_{\text{within}}}$ | $\frac{\text{MS}_{B}}{\text{MS}_{AB}}$ | $\frac{\text{MS}_{B}}{\text{MS}_{AB}}$ |
| Interaction | $\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}$ |  $\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}$ | $\frac{\text{MS}_{AB}}{\text{MS}_{\text{within}}}$ |

The F ratio is the mean squared error (MS) of the factor of interest divided by the mean squared error for the term that has everything *but* the factor of interest. When you have factor A and are considering interactions with a random effects variable B, that adds a new component to the expected variance of A (remember the new variance term, $\sigma_{AB}^2$ with random effects). Therefore, the appropriate comparison for the F ratio test is the mean squared error for the interaction term, which includes both the within group error and this additional variance component associated with the random factor.

Remember from last week:

$$
\text{Among group variance (group effect + error)}
$$

When factors A and B are random, the estimate of $\text{MS}_{A}$ can be described as:

$$
\text{Among group variance component}_A \text{(group A variance + interaction variance + residual variance)}
$$

And when factors A and B are random, the estimate of $\text{MS}_{AB}$ can be described as:

$$
\text{Interaction variance component}_{AB} \text{(interaction variance + residual variance)}
$$

So, the F ratio is the mean squares of the factor of interest divided by the MS for the term that has everything but the factor of interest.

**Review question: How did we estimate the F ratio with a single factor ANOVA with a random effect?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The F ratio was exactly the same as for fixed effects (denominator was mean square within groups). Only your interpretation of the test changes with a single factor random effects ANOVA.
</span>
</details> 

<p>&nbsp;</p>

Why bother with random effects?
---------------

You have a study measuring the effect of different levels of zinc contamination on diatom diversity, measured in multiple streams. You could model the data with or without stream as a random effect. The model without a random effect (pooling all data from different streams), where $A_i$ is the effect of zinc level $i$, would be:

$$
Y_{ij} = \mu + A_i + \epsilon_{ij} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma_{\epsilon1}^2 )
$$

The model with stream as a random effect, where $B_j$ is the random effect of stream, and $AB_{ij}$ is also a random effect describing whether the effect of zinc is consistent across streams, would be:

$$
Y_{ijk} = \mu + A_i + B_j + {AB}_{ij} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma_{\epsilon2}^2 ) \\
B_j \sim \mathrm{N}(0, \sigma^2_B) \\
{AB}_{ij} \sim \mathrm{N}(0, \sigma^2_{AB})
$$

Your data is exactly the same in both cases, but **in the model with stream as a random effect, you have decided to let the random effect of stream absorb some of the residual/unexplained variation in the model**. Therefore, $\sigma_{\epsilon2} \leq \sigma_{\epsilon1}$. In the case where you ignored the effect of stream, the unexplained variation included some variation that could have been explained by stream. By lumping these together, you lose statistical power to test the null hypothesis on $A_{i}$.

Mixed model
--------------

What if we have one fixed effect and one random effect. Models with both fixed and random effects are called mixed models. Our null hypothesis in this case is:

Factor A (fixed effect) $H_0: A_1 = A_2 = \dots = 0$

**Question: How can we interpret this null hypothesis?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
The means of each population of factor A, pooled over all levels of random factor B, are equal.
</span>
</details> 

<p>&nbsp;</p>

Factor B (random effect) $H_0: \sigma^2_B = 0$

**Question: How can we interpret this null hypothesis?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
There is no added variance due to the levels within factor B.
</span>
</details> 

<p>&nbsp;</p>

Interaction $H_0: \sigma^2_{AB} = 0$

Important: The interaction between a fixed factor and a random factor is a random factor.

**Question: How can we interpret this null hypothesis?**

<details>
  <summary>Click for Answer</summary>
<span style="color: blueviolet;">
There is no added variance due to the combinations of factor A or B.
</span>
</details> 

<p>&nbsp;</p>

Unbalanced designs
--------------

Unbalanced designs have a surprising influence on our estimates in ANOVA.

Earlier, we discussed partitioning the total variance in our data into many components:

$$
\text{SS}_{\text{total}} = \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
$$

When we have an unbalanced design, the assignment of the variance to different components becomes ambiguous, and depends on the order that we assign variance. There is no longer a simple way to partition the variance into components of $\text{SS}_{\text{total}}$. The formulae in the two-way factorial ANOVA table are no longer applicable. Were we to calculate these variance components:

$$
\text{SS}_{\text{total}} \ne \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
$$

There are multiple ways that a design can be unbalanced, for example, having different sample sizes in different treatments:

| Factor level | $B_1$        | $B_2$       | $B_3$        | $B_4$         |
| ------------------- |:---------:|:---------:|:---------:|:---------:|
| $A_1$ | $A_1 ,  B_1 = Y_{111}$ <br> $A_1 ,  B_1 = Y_{112}$ <br> $A_1 ,  B_1 = Y_{113}$| $A_1 ,  B_2 = Y_{121}$ <br> $A_1 ,  B_2 = Y_{122}$ <br> $A_1 ,  B_2 = Y_{123}$ | $A_1 ,  B_3 = Y_{131}$ <br> $A_1 ,  B_3 = Y_{132}$ <br> $A_1 ,  B_3 = Y_{133}$ | $A_1 ,  B_4 = Y_{141}$ <br> $A_1 ,  B_4 = Y_{142}$ <br> $A_1 ,  B_4 = Y_{143}$ |
| $A_2$  | $A_2 ,  B_1 = Y_{211}$ <br> $A_2 ,  B_1 = Y_{212}$ | $A_2 ,  B_2 = Y_{221}$ <br> $A_2 ,  B_2 = Y_{222}$ <br> $A_2 ,  B_2 = Y_{223}$ | $A_2 ,  B_3 = Y_{231}$ <br> $A_2 ,  B_3 = Y_{232}$ <br> $A_2 ,  B_3 = Y_{233}$ | $A_2 ,  B_4 = Y_{241}$ <br> $A_2 ,  B_4 = Y_{242}$ <br> $A_2 ,  B_4 = Y_{243}$ |
| $A_3$ | $A_3 ,  B_1 = Y_{311}$ <br> $A_3 ,  B_1 = Y_{312}$ <br> $A_3 ,  B_1 = Y_{313}$ | $A_3 ,  B_2 = Y_{321}$ <br> $A_3 ,  B_2 = Y_{322}$ <br> $A_3 ,  B_2 = Y_{323}$ | $A_3 ,  B_3 = Y_{331}$ <br> $A_3 ,  B_3 = Y_{332}$ | $A_3 ,  B_4 = Y_{341}$ <br> $A_3 ,  B_4 = Y_{342}$ <br> $A_3 ,  B_4 = Y_{343}$ |

Or, one or more cells (factor A B combination) may be missing entirely:

| Factor level | $B_1$        | $B_2$       | $B_3$        | $B_4$         |
| ------------------- |:---------:|:---------:|:---------:|:---------:|
| $A_1$ | $A_1 ,  B_1 = Y_{111}$ <br> $A_1 ,  B_1 = Y_{112}$ <br> $A_1 ,  B_1 = Y_{113}$| $A_1 ,  B_2 = Y_{121}$ <br> $A_1 ,  B_2 = Y_{122}$ <br> $A_1 ,  B_2 = Y_{123}$ | $A_1 ,  B_3 = Y_{131}$ <br> $A_1 ,  B_3 = Y_{132}$ <br> $A_1 ,  B_3 = Y_{133}$ | $A_1 ,  B_4 = Y_{141}$ <br> $A_1 ,  B_4 = Y_{142}$ <br> $A_1 ,  B_4 = Y_{143}$ |
| $A_2$ | | $A_2 ,  B_2 = Y_{221}$ <br> $A_2 ,  B_2 = Y_{222}$ <br> $A_2 ,  B_2 = Y_{223}$ | $A_2 ,  B_3 = Y_{231}$ <br> $A_2 ,  B_3 = Y_{232}$ <br> $A_2 ,  B_3 = Y_{233}$ | $A_2 ,  B_4 = Y_{241}$ <br> $A_2 ,  B_4 = Y_{242}$ <br> $A_2 ,  B_4 = Y_{243}$ |
| $A_3$ | $A_3 ,  B_1 = Y_{311}$ <br> $A_3 ,  B_1 = Y_{312}$ <br> $A_3 ,  B_1 = Y_{313}$ | $A_3 ,  B_2 = Y_{321}$ <br> $A_3 ,  B_2 = Y_{322}$ <br> $A_3 ,  B_2 = Y_{323}$ | $A_3 ,  B_3 = Y_{331}$ <br> $A_3 ,  B_3 = Y_{332}$ <br> $A_3 ,  B_3 = Y_{333}$ | $A_3 ,  B_4 = Y_{341}$ <br> $A_3 ,  B_4 = Y_{342}$ <br> $A_3 ,  B_4 = Y_{343}$ |

### Unbalanced design -- Different sample sizes

We'll look into the first situation first. Again, when you have an unbalanced design, not only is your ANOVA more sensitive to deviations from the assumptions of ANOVA (i.e., cells with different sample sizes might have different variances), but the sum of squares can no longer be neatly partitioned as we have assumed in the past:

$$
\text{SS}_{\text{total}} \neq \text{SS}_{A} + \text{SS}_{B} + \text{SS}_{AB} + \text{SS}_{\text{within}}
$$

Because the partitioning of variance is ambiguous and depends on the order in which we estimate the components, there are three different ways to calculate the sums of squares for the main effects terms. **These are called Type I, Type II, and Type III sums of squares**.

Why does unbalanced design change estimates of SS?

In a two-way ANOVA, there are two ways to interpret the main effects:

1. What is the effect of factor A on $Y$, **IGNORING** factor B?

2. What is the effect of factor A on $Y$, **CONTROLLING** for factor B?

It turns out that if you have equal numbers of observations in each cell, then these are the same question, but if you have an unbalanced design, then these are actually different questions. Why?

Let’s say that we have unbalanced data. In our sample, men are more likely to have PhDs than women. Also, PhDs make more money than non-PhDs. In this case, if you just considered the influence of gender on salary, you might conclude that men make more money than women even if there is actually no influence of gender. 

In other words, the factors gender and education are correlated, and there is some amount of overlap in the variance explained by each predictor.


```{r echo=FALSE, fig.align='center', fig.cap='Source: Logan (2010)', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Logan_BalancedUnbalanced.png')
```

In our previous example, **“ignoring”** education would mean that you let gender “take credit” for all of the variance it explains, even the variance that it shares with education. *Note that this is implicitly the case with all variables not included in your study (hidden explanatory variables).*

**“Controlling”** for education would mean that you are testing the effect of gender only after the effect of education had already been taken into account.

Another way of looking at it:

These two approaches actually address different hypotheses. “Ignoring” education tests whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample. “Controlling for” education tests whether men make more money than women in a population in which all educational levels are equally likely. 

Usually, we are interested in inference where we are controlling for the other variables. When we have unbalanced designs, this is the only approach that makes much sense. But a word of warning (that will be often repeated), this is not the default in some major ANOVA functions in R!

We will now define each of the types of sums of squares. Note that each definition assumes the two-way factorial design: $Y \sim A + B + A \times B$.

### Type I (sequential) sums of squares

$\text{SS}(A)$ for factor A

$\text{SS}(B | A)$ for factor B

With sequential sums of squares, we first test the main effect A. Then, we estimate the main effect of B AFTER the main effect of A has "taken up" the shared variation. Bringing this back to our earlier example, with Type I SS, we would be “ignoring” education by testing whether men make more money than women in a population that has the same proportions of advanced degrees as the ones in the sample.

The order in which you add factors in a model has a huge influence. This is the default in R's function `anova()`, despite often not being what you are interested in!

```{r echo=FALSE, fig.align='center', fig.cap='Source: Logan (2010)', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Logan_TypeISS.png')
```

The sums of squares for factors A and B in Type I models are estimated using the differences in the sums of squares error for a model with just the overall mean, to a model with just factor A, to a model with factor A and factor B (but no interaction). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with just the overall mean to the model with just factor A:

$$
Y_{ijk} = \mu + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + \epsilon_{ijk}
$$

Notice that the variance associated with $\epsilon_{ijk}$ on the left hand side gets divided up: some will be 'assigned' to factor $A$ and some will still be left over in $\epsilon_{ijk}$ on the right hand side. 

In other words,

$$
\epsilon_{ijk} \sim N(0,\sigma^2_{\epsilon1}) \longrightarrow \epsilon_{ijk} \sim N(0,\sigma^2_{\epsilon2})
$$

where $\sigma^2_{\epsilon_2} < \sigma^2_{\epsilon_1}$ because some of that variation is now explained by factor $A$. We use that difference in residual variation as a measure of how much variation is 'taken up' or 'explained' by the factor $A$. 

$$
\text{SS}_{A} = \text{SSE}(\mu) - \text{SSE}(A)
$$

In the above equation, $SS_{A}$ is the sum of squares associated with the factor A. In other words, it is the sum of squares error that factor A "takes credit for". SSE is the sum-of-squares error, or the residual sum-of-squares variation left over after the model. The model with no factors is comparing each data point to the grand mean $\mu$, so here $SSE(\mu)$ is just the total sum-of-squares variation. (In other words, with no covariates, *all* variation is residual.) $SSE(A)$ is the residual sum-of-squares variation with $A$ in the model.

To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor A to the model with factor A and factor B (but no interaction). In other words, we add the factor $B$

$$
Y_{ijk} = \mu + A_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
$$

and calculate the decrease in the residual variation in going from an A-only model to an (A+B) model.

$$
\text{SS}_{B} = \text{SSE}(A) - \text{SSE}(A + B)
$$

### Type II (hierarchical) sums of squares

$\text{SS}(A | B)$ for factor A

$\text{SS}(B | A)$ for factor B

With hierarchical sums of squares, we assume no significant interaction. This does not depend on the order that factors are input. This can be done using the `Anova()` function in the package `car`.

```{r echo=FALSE, fig.align='center', fig.cap='Source: Logan (2010)', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Logan_TypeIISS.png')
```

The sums of squares for each main effect are calculated by comparing the sums of squares error in a model with the factor of interest to a model without it (including all other terms at the same or lower level). To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the model with factor A to the model without it (notice the missing interaction):

$$
Y_{ijk} = \mu + B_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
$$

$$
\text{SS}_{A} = \text{SSE}(B) - \text{SSE}(A + B)
$$

To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the model with factor B to the model without it:

$$
Y_{ijk} = \mu + A_i + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + \epsilon_{ijk}
$$

$$
\text{SS}_{B} = \text{SSE}(A) - \text{SSE}(A + B)
$$

### Type III (marginal) sums of squares

$\text{SS}(A | B, AB)$ for factor A

$\text{SS}(B | A, AB)$ for factor B

With marginal sums of squares, we are estimating the marginal effect of a factor after the effect of the other factors (and interactions) have been taken into account. Back to our earlier example, we would be “controlling for” education, by testing whether men make more money than women in a population in which all educational levels are equally likely, or “controlling” for education by estimating the effect of gender only after the effect of education had already been taken into account.  This can be done using the `Anova()` function in the package `car`.

```{r echo=FALSE, fig.align='center', fig.cap='Source: Logan (2010)', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Logan_TypeIIISS.png')
```

The sums of squares are estimated in Type III models by comparing the difference in the sums of squares between the full model and the model without the main effect being measured. To estimate the sums of squares for factor A, we compare the difference in the sums of squares error between the full model and the model missing factor A:

$$
Y_{ijk} = \mu + B_i + (AB)_{ij} + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}
$$

$$
\text{SS}_{A} = \text{SSE}(B + A:B) - \text{SSE}(A + B + A:B)
$$

To estimate the sums of squares for factor B, we compare the difference in the sums of squares error between the full model and the model missing factor B:

$$
Y_{ijk} = \mu + A_i + (AB)_{ij} + \epsilon_{ijk} \longrightarrow Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}
$$

$$
\text{SS}_{B} = \text{SSE}(A + A:B) - \text{SSE}(A + B + A:B)
$$


### Comparing type I, II, and III SS

When the design is balanced (equal sample sizes in each category), the factors are “orthogonal," and Types I,II, and III all give equivalent results. When the interaction AB is not significant, then Type II and III SS estimates are equivalent. Aho 10.14 contains more detail.

Note that $\text{SS}_{AB}$ and $\text{SS}_{\text{within}}$ are the same for all three ways of calculating SS. $\text{SS}_{\text{within}}$ is the sum of squared deviations between each fitted data point $\hat{Y}_{ijk} = \bar{Y}_{ij}$ and the overall mean for the full model $Y_{ijk} = \mu + A_i + B_i + (AB)_{ij} + \epsilon_{ijk}$ and $\text{SS}_{AB}$ is calculated as the difference in sums of squares between the full model and the model without the interaction term. There are only differences in estimates of SS for the main effects terms for the three different SS methods because it depends on the way you calculate **marginal** means.

Unbalanced design -- Missing cell
-----------

When an entire cell (a combination of factors) is missing, it is not possible to test all the main effects and interactions. One solution is to fit a large single factor ANOVA with as many levels as there are cells, and then compare combinations using specific contrasts to tests hypotheses of interest. This is like treating each factor combination like a dummy coded variate. This is called a “cell means model.” This approach is worked out in Logan 12.6. 

Two factor nested ANOVA
-----------------

In nested designs, the categories of the nested factor within each level of the main factor are unique. Usually this happens because 1) you have unique organisms within each treatment, or 2) you have unique plots within each treatment. The nested factors are usually random effects (but not always). Nested designs refer to any design in which there is subsampling within the replicates. Nested designs, or hierarchical designs, can have many levels. For example, if you were interested in barnacle diversity, you could have subsamples within replicates, replicates nested within intertidal zones, intertidal zones nested with shores, shores nested within regions, etc. 

Let's imagine we are measuring the amount of glycogen in rat livers. We have **three treatments** that we gave rats. We included **two rats in each treatment**. We took six liver samples from each rat and measured each one of those liver samples once. (We will tackle a slightly more complex nested version in the lab.) There are 36 total measurements.

```{r echo=FALSE, fig.align='center', fig.cap='Nested design of the rat experiment.', out.width='100%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/RatDesign_Simplified.png')
```

What would our model equation look like?

$$
Y_{ijk} = \mu + A_i + B_{j(i)} + \epsilon_{ijk} \text{, where } \epsilon_{ijk} \sim \mathrm{N} ( 0, \sigma^2)
$$

$A_i$ is the effect of treatment, and $B_{j(i)}$ is the nesting factor (rats within treatment). $Y_{ijk}$ is the $k^{th}$ liver sample from rat $j$ nested in treatment $i$. It would be impossible to have a crossed design for this experiment, for example, rat 1 cannot be in treatment 1, 2, and 3.

Why use a nested structure?

Nested designs are actually quite powerful. Single units in an experimental design may not adequately represent the populations. By only measuring the response in single units in an experiment, this can actually **increases** the unexplained variation in the system, which can mask the true differences.

**<span style="color: read;">By subsampling within each sample, we get a more precise estimate of the mean response within each sampling unit (think: Central Limit Theorem).</span>** If we estimate the response in three subsamples, we can get a more precise estimate of the treatment mean than if we only had one sample per replication. This way, we reduce unexplained variability.

Now we know why subsampling is helpful, but why not just average among the subsamples? A nested analysis allows you to partition the variance into the amount of variation at each level of the hierarchy. Why not just *ignore* the nestedness of the data? Because by ignoring the nestedness of the data, you would treat all the data as being independent, but because of the nested samping design, this is inappropriate.


| Source of variation | SS        | DOF       | MS        | F         |
| ------------------- |:---------:|:---------:|:---------:|:---------:|
| Among groups (factor A) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{i} - \bar{Y})^2$ | $a - 1$ | $\frac{\text{SS}_{\text{among grp.}}}{\text{DOF}_{\text{among grp.}}}$ | $\frac{\text{MS}_{\text{among grp.}}}{\text{MS}_{\text{among rep.}}}$ |
| Among replicates within groups (nesting factor B) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (\bar{Y}_{j(i)} - \bar{Y}_{i})^2$ | $a (b - 1)$ | $\frac{\text{SS}_{\text{among rep.}}}{\text{DOF}_{\text{among rep.}}}$ | $\frac{\text{MS}_{\text{among rep.}}}{\text{MS}_{\text{residual}}}$ | 
| Subsamples within replicates (residual) | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y}_{j(i)})^2$ | $ab (n - 1)$ | $\frac{\text{SS}_{\text{residual}}}{\text{DOF}_{\text{residual}}}$ | |
| Total | $\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{k = 1} (Y_{ijk} - \bar{Y})^2$ | $abn - 1$ | | |

In the above table, the notation is as follows:

$\bar{Y}$ is the grand mean (i.e. the average across all 36 measurements)

$\bar{Y}_{i}$ is the mean within each group. In the rat example, this is the mean for each treatment.

$\bar{Y}_{j(i)}$ = is the mean within of the subsets within that group. In the rat example, this would be the mean within each rat.

Notice that there are 6 measurements for each rat and there is variation *within* these measurements. Given this experimental set-up, this is the residual variation. The mean squared error at this bottom level of the model is $\text{MS}_{within}$. Notice that in the ANOVA table, this is the denominator for all F ratios. **Large residual variation is going to make it difficult to say that variation between groups higher in the hierarchy are actually statistically significant.**

To test the null hypothesis that factor A has no effect ($H_0(A): A_i = 0$), we find the probability of obtaining an F ratio greater than the F ratio we calculated with our data: $P ( X \ge F^*), X \sim F_{[\text{DOF}_{\text{among grp.}}, \text{DOF}_{\text{within}}]}$. To test the null hypothesis that nesting factor B has no effect ($H_0(B): B_j = 0$), $P ( X \ge F^*), X \sim F_{[\text{DOF}_{\text{among rep.}}, \text{DOF}_{\text{within}}]}$. Again, there is no interaction term with nested designs.

Finally, nested design arise frequently in a spatial context, where you might have multiple plots within fields receiving different treatments. The most common scenario would look like this:

```{r echo=FALSE, fig.align='center', fig.cap='Nested ANOVA with sub-sampling', out.width='60%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/NestedANOVA_WithSubsampling.png')
```

### Potential issues with nested designs

Nested designs are often mis-analyzed, because people often incorrectly treat subsamples as independent replicates. This ignores correlation among subsamples and artificially boosts sample size. As a result, the chance of Type I error increases dramatically. 

Nested designs are difficult if not impossible to analyze correctly if sample sizes are not the same in each group.

The power of ANOVA designs is more sensitive to the number of independent replicates than it is to the precision with which you can estimate the mean of each replicate, which means that you should never divert resources away from more independent replicates in favor of subsampling the replicates you have.

**All of these potential issues can be resolved with careful experimental design.**

Experimental design
---------------

Experimental design is a very important part of statistics that deals with the structure of an experiment or sampling scheme (experimental design is still important even with observational studies). Proper experimental design is essential to correctly be able to make inferences. More on this topic in Aho Ch. 7 and the Hurlbert (1989) paper.

### Completely randomized design

We didn’t say so explicitly, but when we discussed single-factor ANOVA before we assumed a **completely randomized design**. In a completely randomized design, treatments are assigned at random to experimental units (plots, organisms, patients). A negative of completely randomized design is that if there is an environmental gradient, or “noise,” then the completely randomized design will have low power. This is because differences among treatments will be swamped by background “noise” not accounted for in the model.

```{r echo=FALSE, fig.align='center', fig.cap='Environmental gradient', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/EnvGradient.png')
```

### Randomized block design

A “block” is a unit of space or time within which conditions are considered to be relatively homogeneous. Blocks may be placed randomly or systematically, but they should be arranged so that **environmental conditions are more are more similar within blocks than between them**.Within each block, treatments are assigned randomly. Each treatment is assigned once per block. Blocks should be placed far enough apart that blocks can be considered independent.

```{r echo=FALSE, fig.align='center', fig.cap='Randomized block design', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/RandomizedBlockDesign.png')
```

Randomized block designs **can account for some of the background heterogeneity that completely randomized designs miss**. When environmental heterogeneity is present, the randomized block design is more efficient than a completely randomized layout and will need fewer replicates for the same statistical power. A negative of randomized block design is that we assume no interaction between the block and the treatments (in other words, it assumes that the ranking of treatment responses will be the same in each block).

Randomized block design models are very similar to two-way factorial designs, but **without an interaction term**. $Y_{ijk}$ is the $k^{\text{th}}$ observation from the $j^{\text{th}}$ level in blocking factor B and the $i^{\text{th}}$ level in factor A (a fixed effect). 

$$
Y_{ijk} = \mu + A_i + B_j +\epsilon_{ijk}, \text{where } \epsilon_{ijk} \sim \mathrm{N} (0, \sigma^2)
$$

Our null hypotheses are the effect of treatment factor A is zero ($H_0(A): \text{all } A_i = 0$), and the effect of blocking factor B is zero ($H_0(B): \text{all } B_i = 0$). If we reject the null hypothesis that block effects are significant, we know that using blocks as replicates was effective in addressing the homogeneity among replicates. We fully expect that the block effect will be significant, since this was our rationale for using a randomized block design!

| Source of variation | SS        | DOF       | MS        | F         |
| ------------------- |:---------:|:---------:|:---------:|:---------:|
| Among treatments (factor A) | $\sum^a_{i = 1} \sum^b_{j = 1} (\bar{Y}_{i} - \bar{Y})^2$ | $a - 1$ | $\frac{\text{SS}_{A}}{\text{DOF}_{A}}$ | $\frac{\text{MS}_{A}}{\text{MS}_{\text{within}}}$ |
| Among blocks (factor B) | $\sum^a_{i = 1} \sum^b_{j = 1} (\bar{Y}_{j} - \bar{Y})^2$ | $b - 1$ | $\frac{\text{SS}_{B}}{\text{DOF}_{B}}$ | $\frac{\text{MS}_{B}}{\text{MS}_{\text{within}}}$ |
| Within groups (residual) | $\sum^a_{i = 1} \sum^b_{j = 1} (Y_{ij} - \bar{Y}_{j} - \bar{Y}_{i} + \bar{Y})^2$ | $(b - 1)(a - 1)$ | $\frac{\text{SS}_{\text{within}}}{\text{DOF}_{\text{within}}}$ | |

There is no $\sum^n_{k = 1}$ because there is only one replicate of each treatment in each block. Also notice the similarity of the residual SS to the interaction mean squares in other designs.

### Latin square design

A Latin square design is a special case of a randomized block design for cases where environmental heterogeneity may occur along two dimensions (east-west and north-south for example). In this case you want each treatment to occur exactly once in each row and in each column (like Sudoku).

```{r echo=FALSE, fig.align='center', fig.cap='Latin square design. Source: Wikipedia', out.width='40%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/LatinSquare.png')
```

### Split plot design

With a split plot design, we have an experimental design that exists at two scales: a large unit (which has a whole plot treatment assigned), and smaller units within the large unit (which have split plot treatments). This is a hierarchical design in that the large unit treatments are pseudoreplicates, but the split plots have informative factor levels that mean exactly the same thing in other plots (unlike rat individuals in nested designs). For example, the large unit could be "pond," to which you apply different nutrient addition treatments, and the smaller unit could be "predation treatment" where you multiple different cage setups within each pond. The predation treatments mean the same thing in the other ponds.

Often one factor is applied at the block level because of logistical constraints. One example might be an experiment to test insecticide application vs. grass seed type. If insecticides are being applied by airplane, then it is easier to apply one insecticide treatment to a whole field instead of just to a portion of field, in which case insecticide might be applied at the field level but grass variety might vary among subplots within the larger field (“block”).  

Week 13 Lecture
=============

```{r include=FALSE, warning=FALSE}
library(ggplot2)
library(carData)
text.size <- 16
```

Before we can decide on what the “best” model is, we need to decide what we mean by the best model. There are two reasonable definitions of the best model based on the two separate and distinct goals of *testing a mechanism* and *making predictions*.

## Model criticism

$$
Y = f(X) + \epsilon
$$

Models are always imperfect representation of reality (“All models are wrong but some are useful”). However, when you fit a model, you need to show that the model is a good representation of the data. This is important because if your data do not fit the assumptions of the model, the model can misrepresent the data. Model criticism and selection involves three parts:

1. **Assumptions/Robustness:** Are any of the assumptions of the model violated? Are any model assumptions having an undue impact on the results? 

2. **Assessment:** Does the model provide adequate fit to the data? 

3. **Selection:** Which model (or models) should we choose for final inference or prediction? 

The difference between the predicted value (based on the regression equation) and the actual, observed value. Remember that a lot of the assumptions for linear regression have to do with the residuals, (e.g., we often assumed $\epsilon_i \sim \mathrm{N} (0, \sigma^2)$).

```{r, fig.width=5, fig.height=4, echo=FALSE, message=FALSE}
library(asbio)
library(ggplot2)
text.size <- 16
data(webs)
y.bar <- mean(webs[, 2])
web.fit <- lm(formula = length ~ temp.C, data = webs)
x.i <- webs[12, 3]
y.i.hat <- web.fit$coefficients[1] + web.fit$coefficients[2] * x.i
y.i <- webs[12, 2]
ggplot(data = webs, aes(x = temp.C, y = length)) + geom_point(col = "gray37") + 
  geom_point(x = x.i, y = y.i, col = "firebrick1", size = 3) + 
    geom_point(x = x.i, y = y.i.hat, col = "palegreen3", size = 3) + 
  annotate(geom = "text", x = x.i + 1.5, y = y.i.hat, label = "hat(Y[i])", parse = TRUE, col = "palegreen3", size = 5) +
  annotate(geom = "text", x = x.i + 1, y = y.i - 0.0001, label = "Y[i]", parse = TRUE, col = "firebrick1", size = 5) +
  geom_hline(aes(yintercept = mean(length)), col = "dodgerblue1") + 
  geom_abline(intercept = web.fit$coefficients[1], slope = web.fit$coefficients[2], col = "palegreen1") +
  annotate("text", x = 12, y = 0.9987, label = "bar(Y)", parse = TRUE, size = 5, col ="dodgerblue1") +
  labs(x = "Temperature (in C)", y = "Relative length of spider webs") + theme_classic() + theme(text = element_text(size = text.size))
```

Plots of residuals should not change with the fitted values. Sometimes, systematic features in residual plots can suggest specific failures of model assumptions.

We will look at a past Biometry exam question to both practice for the exam and link what we are talking about today to past lectures.

Below are four plots depicting the residuals of a linear model plotted as a function of $\hat{Y}$. For each panel, state whether the model violates any of the assumptions of linear regression and, if yes, which assumption(s) of linear regression are violated. 

```{r echo=FALSE, fig.align='center', fig.cap='Environmental gradient', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Residuals.png')
```

a. No apparent violations

b. Variance is heteroscedastic (not constant). Larger fitted values have larger variance. Depending on your data, what type of regression may be more appropriate here? Poisson regression (why? mean = variance). You could also consider transforming the data or fitting a weighted regression to resolve this pattern.

c. Linear pattern in the residuals, the errors are not independent

d. Nonlinear pattern in the residuals (consider transforming data or fitting the predictor quadratically).


## Residuals

In linear regression, an outlier is an observation with a **large residual**. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity, a data entry error, or other problem.

```{r fig.width=5, fig.height=4, echo=FALSE}
outliers <- which(Duncan$education < 60 & Duncan$income > 60)
Duncan.fit <- lm(formula = income ~ education, data = Duncan)
x.i <- Duncan[outliers[1], 3]
y.i.hat <- Duncan.fit$coefficients[1] + Duncan.fit$coefficients[2] * x.i
y.i <- Duncan[outliers[1], 2]
ggplot(data = Duncan, aes(x = education, y = income)) + geom_point() +
  geom_point(x = x.i, y = y.i, col = "firebrick1", size = 3) + 
  geom_point(x = x.i, y = y.i.hat, col = "palegreen3", size = 3) + 
  annotate(geom = "text", x = x.i + 3, y = y.i.hat, label = "hat(Y[i])", parse = TRUE, col = "palegreen3", size = 5) +
  annotate(geom = "text", x = x.i + 3, y = y.i - 0.0001, label = "Y[i]", parse = TRUE, col = "firebrick1", size = 5) +
  geom_abline(intercept = Duncan.fit$coefficients[1], slope = Duncan.fit$coefficients[2], col = "palegreen1") +
  labs(x = "Percent high-school graduates", y = "% earning $3500 or more in 1950") + theme_classic() + 
  theme(text = element_text(size = text.size))
```

## Leverage

An observation with an extreme value for a **predictor variable/independent variable/covariate** (often $X$) is a point with high leverage. Leverage for a point is high when $(X - \bar{X})^2$ is large. X-values further from $\bar{X}$ influence $\hat{Y}_i$ more than those close to $\bar{X}$. High leverage points may have a large effect on the estimate of regression coefficients.

The leverage of the $i^{\text{th}}$ point is called $h_{ii}$ (the $i^{\text{th}}$ diagonal element in the hat matrix). The hat matrix relates the response $Y$ to the predicted response $\hat{Y}$. This hat value, $h_{ii}$ thus is a measure of the contribution of data point $Y_i$ to all fitted values $\hat{Y}$. 

Average leverage is considered to be equal to $p / n$, or the number of parameters divided by the sample size. Twice the average leverage is considered high.

## Influence

An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients. Influence can be thought of as the **product of leverage and outlierness**.

We assess influence by looking at the effect of deleting that data point. We compare the least squares estimate of coefficient $\beta$ for the full dataset to the least squares estimate of coefficient $\beta$ when the $i^{\text{th}}$ data point is removed.

$$
\text{Influence} \propto \hat{\beta} - \hat{\beta}_{-i}
$$

## Comparing residuals, leverage, and influence

```{r, echo=FALSE, fig.width=6, fig.height=5}
Y <- rnorm(n = 10, mean = 5, sd = 2)
X <- Y + rnorm(n = 10)
xy.fit <- lm(Y ~ X)
X.1 <- c(X, 13)
Y.1 <- c(Y, 3)
xy.fit.1 <- lm(Y.1 ~ X.1)
X.2 <- c(X, 3)
Y.2 <- c(Y, 10)
xy.fit.2 <- lm(Y.2 ~ X.2)
X.3 <- c(X, 14)
Y.3 <- c(Y, 12)
xy.fit.3 <- lm(Y.3 ~ X.3)
par(mar = c(2, 2, 2, 2))
plot(X, Y, ylab = NA, xlab = NA, xaxt = "n", yaxt = "n", ylim = c(0, 15), xlim = c(0, 15), pch = 19)
  abline(a = xy.fit$coefficients[1], b = xy.fit$coefficients[2], lwd = 2)
  points(x = 13, y = 3, col = "indianred2", pch = 19, cex = 2)
  abline(a = xy.fit.1$coefficients[1], b = xy.fit.1$coefficients[2], col = "indianred2", lwd = 2, lty = "dashed")
  points(x = 3, y = 10, col = "dodgerblue2", pch = 19, cex = 2)
  abline(a = xy.fit.2$coefficients[1], b = xy.fit.2$coefficients[2], col = "dodgerblue2", lwd = 2, lty = "dashed")
  points(x = 14, y = 12, col = "seagreen3", pch = 19, cex = 2)
  abline(a = xy.fit.3$coefficients[1], b = xy.fit.3$coefficients[2], col = "seagreen3", lwd = 2, lty = "dashed")
```

The red and the blue points have large residuals.

The red and the green points have high leverage (large X values).

Only the red point here has high influence. Note that in your own work you would support these claims with statistics (which we will go over next).

Important:

1.	Outliers might not be influential. (Depends on the $X$ value)

2.	High leverage values may not be influential. (May fit right in with predicted line)

3.	Influential observations may not be outliers. (May just have high leverage)

We will use our two linear regressions of spider web length as a function of temperature, `web.fit`, and percent of males earning $3500 or more as a function of high school graduation rate, `Duncan.fit`, to go through the process of diagnosing a model. Do these models meet the assumptions of linear regression? Do they fit the data well?

Remember that certain analyses have certain assumptions. With a Poisson regression, you need to check for overdispersion or an abundance of zeros, and use the appropriate model.

If you have more than one covariate, you need to assess collinearity, which inflates Type II error and may mask covariate significance. We will go through this process in more detail in lab.

Are the residuals normally distributed? Histogram the residuals. They should be centered on zero and approximately normal (assuming a basic linear regression). Remember that with some analyses, like ANOVA, you assume normality within each group, so you would not look at all residuals together in that case. Also note that some analyses are fairly robust to non-normality.

```{r, fig.width=5, fig.height=4}
hist(web.fit$residuals)
```

Are the residuals normally distributed? Look at a Q-Q plot comparing the quantiles of the residuals against the quantiles of the distribution we expect them to match (usually normal).

```{r, fig.width=5, fig.height=4}
plot(web.fit, which = 2)
```

Are the residuals independent and homoscedasctic? Plot the residuals vs. the fitted values. The residuals should be uncorrelated with the fitted values and the variance of the residuals should not change as a function of the fitted values. 

```{r, fig.width=4, fig.height=4}
plot(web.fit, which = 1)
```

Are the residuals normally distributed? We can formally check the residuals are normally distributed using the Kolmogorov-Smirnov test (null hypothesis is that the two vectors come from the same distribution). In this case, we would check against a normal distribution with mean of 0 and standard deviation of $\sigma$ from the regression.

```{r, fig.width=4, fig.height=4}
ks.test(web.fit$residuals, y = "pnorm", mean = 0, sd = summary(web.fit)$sigma)
```

**Question: How do we interpret the results of this hypothesis test?**

<details>
  <summary>Click for Answer</summary>
  <span style="color: blueviolet;">
We have insufficient evidence to reject the null hypothesis that the residuals are normally distributed (both samples come from the same distribution).
</span>
</details>

<p>&nbsp;</p>

Are the residuals independent? Use the Durbin-Watson test, where the null hypothesis is that there is no correlation among residuals. If errors are correlated, we cannot assume that our errors are independent (an assumption of regression). These kind of correlations may be temporal or spatial, and can be controlled for with different modeling approaches. See Zuur et al. (2010) "Step 8" for details on how to deal with correlated errors.

$$d = \frac{\sum_{i = 2}^n (\epsilon_i - \epsilon_{i - 1})^2}{\sum_{i = 1}^n (\epsilon_i)^2}$$

```{r}
library(car)
durbinWatsonTest(web.fit)
```

**Question: How do we interpret the results of this hypothesis test?**

<details>
  <summary>Click for Answer</summary>
  <span style="color: blueviolet;">
We have insufficient evidence to reject the null hypothesis that the residuals are independent.
</span>
</details>

<p>&nbsp;</p>

Also be sure to look for outliers and points with high influence. There are a couple of ways you can determine if a point really is an outlier. We will use the Duncan regression to assess outliers, `Duncan.fit`.

What do you do with an outlier? 

Well...

```{r echo=FALSE, fig.align='center', fig.cap='One possible solution to outliers. (No!! Just kidding!! Do not do this.) Source: John Fox, Regression Diagnostics (1991)', out.width='25%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/Outlier_cartoon.png')
```

Don't remove it unless you can verifiably say it was recorded or measured in error.

1. Present the model with and without outliers, providing an explanation for the outliers (is there a potential unmeasured covariate that could explain the outlier?)

2. Use a robust method (tests based on rank, weighted models, etc.).

Calculate the leave-one-out residuals for each data point and scale them by the standard deviation of the leave-one-out residuals. Data points with $t_i > 2$ are potential outliers, $t_i > 3$ are more serious outliers.

$$
t_i = \frac{Y_i - \hat{Y}_{i, (-i)}}{\sqrt{\mathrm{Var} (Y_i - \hat{Y}_{i, (-i)})}}
$$

```{r}
library(MASS)
Duncan[outliers, ]
studres(Duncan.fit)[which(studres(Duncan.fit) > 2)]
```

Cook’s distances are related to the studentized residuals in that both use leave-one-out residuals:

$$
D_i = \frac{\sum_{j = 1}^n (\hat{Y}_j - \hat{Y}_{j, (-i)})^2}{p \times \mathrm{MSE}}
$$

where *p* is number of parameters in the model, and MSE is the mean squared error. There are different cut-offs for what might be considered significant, but *D* > 1 would be a basic guideline. Another guideline from Chatterjee and Hadi is to adjust *D* for sample size, $4 / (n - p - 1)$. Cook's distance describes the **influence** of a point (a function of both leverage and outlierness).

```{r}
cutoff <- 4 / (nrow(Duncan) - 3 - 1)
cooks.distance(Duncan.fit)[which(cooks.distance(Duncan.fit) > cutoff)]  # in car package
```

## Residuals for GLMs

Note that the definition of residuals for GLMs is a bit different. Residuals can be defined as $\hat{Y}_i - Y_i$. For normally distributed linear models, this is equal to the statistical error $\epsilon_i = \mathrm{E}(Y_i) - Y_i$. Remember that $\mathrm{E}(Y_i)$ or $\hat{Y}_i$ for a binomial or Poisson GLM is $\hat{p_i}$ or $\hat{\lambda_i}$, respectively, so not on the same scale as the data ($Y_i$ for binomial or Poisson is an integer 0 through *n*). Also, nonconstant variance is an inherent part of GLMs.

We can use different types of residuals for model criticism of GLMs. We can look at the residuals on the scale of the fitted values or response (using the inverse link function): `residuals.glm(crab.glm, type = "response")`, or residuals that are scaled by the variance inherent to the GLM: deviance residuals `type = "deviance"` or Pearson residuals `type = "pearson"` (see 9.20.7.1 in Aho for details). R provides these in diagnostic plots.

```{r fig.height=3, fig.width=4}
crabs <- read.csv(file = "/Users/meganwyatt/Desktop/Biometry2021-master/_data/crabs.csv")
crab.glm <- glm(satell ~ width,  family = "poisson", data = crabs)
plot(crab.glm, which = 1)
```

```{r fig.height=3, fig.width=4}
crab.glm <- glm(satell ~ width,  family = "poisson", data = crabs)
plot(crab.glm, which = 2)
```

```{r fig.height=3, fig.width=4}
crab.glm <- glm(satell ~ width,  family = "poisson", data = crabs)
plot(crab.glm, which = 5)
```


## Model selection vs. model criticism

Before we move on to model selection, let's review the difference between model selection and model criticism: Model criticism involves determining whether the model is appropriate, meets assumptions, and accurately represents the data. Model selection gives us the best model, the best suite of models, or the best averaged model.

Your analyses should always involve model criticism, and may also additionally involve model selection.

To decide what model is best, we first need to determine what our goal was in modeling. If our goal was to make predictions, or definition of what makes a model the best is different from if our goal was to test a mechanism.

We will revisit some ideas from the Shmueli (2010) paper that was assigned Week 8. 

If our goal is to explain, then we are interested in the amount of variation in the response variable that is explained by each of the covariates.

With mechanism-based, explanatory models, we use Occam's razor—the law of **parsimony**. By applying this principle, we identify models that are simple but effective, and we do not consider models that are needlessly complex. In other words, given two models, **we prefer the smaller model unless the larger model is significantly better**. 

Additionally, all parameters should be **biologically important**, as identified by the modeler.

If our goal is to predict, then we want to **minimize the expected prediction error**, or our expectation of the difference between a new predicted value and the fitted value. For a model:

$$
Y = f(X) + \epsilon, \text{ where } \epsilon \sim \mathrm{N} (0, \sigma^2 )
$$

our expected prediction error of new data point $(X^\ast, Y^\ast)$ is:

$$
\text{EPE} = \mathrm{E} [ (Y^\ast - \hat{Y})^2] = \mathrm{E} [ (Y^\ast - f(X^\ast))^2]
$$

The expected prediction error boils down to three components of variance:

$$
\sigma^2 + (\text{bias}_{\text{prediction}})^2 + \mathrm{Var} (\text{prediction})
$$

$\sigma^2$ represents the variation inherent to the process. This error is irreducible.

**Bias**, $(\text{bias}_{\text{prediction}})^2$, represents the difference between the expected prediction of our model (if we could repeat the process of collecting data and building the model many times) and the underlying true value we are trying to predict. 

**Variance**, $\mathrm{Var} (\text{prediction})$, represents the variability of model predictions for a given data point, if we could repeat the data-gathering and model building process many times.

If we had both: 1) the true model (an exact picture of the true mean behavior of the mechanism), and 2) infinite data (no random fluctuations due to finite sampling), we could theoretically minimize both bias and variance. However, that's not the case in reality, and we tend to find a trade-off between bias and variance.

$$
\sigma^2 + (\text{bias}_{\text{prediction}})^2 + \mathrm{Var} (\text{prediction})
$$

```{r echo=FALSE, fig.align='center', fig.cap='Environmental gradient', out.width='50%'}
knitr::include_graphics('/Users/meganwyatt/Desktop/Biometry2021-master/BiasVariance.png')
```

## Comparing two models

If we are generating models to test mechanisms, we can use the following three criteria to decide which is best. All three use the likelihood of the models.

1. Likelihood ratio test (LRT)

2. Akaike's Information Criterion (AIC)

3. Bayesian Information Criterion (BIC)

The likelihood ratio test is one way of comparing two nested models. Before we get into the details of the LRT, let's review for a second what we mean by *nested*.

### Nested or not?

**Question: Are these models nested?**

$$
Y \sim X_1 + X_2 + X_3 \\
Y \sim X_1 + X_2 + X_4
$$

No, neither model is a subset of the other

**Question: What about these models?**

$$
Y \sim X_1 + X_2 + X_3 \\
Y \sim X_1 + X_2
$$

Yes, the second model is a subset of the first model

Note that this usage of "nested" is completely different from the way we used it with nested ANOVAs.

**Question: Nested?**

$$
Y = a \\
Y = a + b X
$$

Nested. The second model is equal to the first model with $b = 0$.

**Question: Nested?**

$$
Y = a + b X \\
Y = a + b X + c X^2
$$

Nested. The second model is equal to the first model with $c = 0$.


**Question: Nested?**

$$
Y = r \left ( 1 - \frac{N_t}{K} \right ) \\
Y = r \left ( 1 - \left ( \frac{N_t}{K} \right ) ^\gamma \right )
$$

Nested. The second model is equal to the first model with $\gamma = 1$. This model is called the Ricker model (top model is the usual form and the bottom model is the generalized version). It's used in ecology to model the abundance of animal populations with density dependence.

### Likelihood Ratio Test (LRT)

Likelihoods are crucial for model selection, they can be used generally for a wide variety of models. We'll start with a brief review of likelihood.

When we were fitting statistical distributions to data, we assumed that each data point was independent. Therefore, for independent events $X_1$, $X_2$, and $X_3$:

$$
\mathrm{P}(X_1 \cap X_2 \cap X_3) = \mathrm{P}(X_1) \times \mathrm{P}(X_2) \times \mathrm{P}(X_3)
$$

We then calculate the joint density of the data as a function of the unknown parameters (multiplying the probability of each data point given the PDF and as a function of the unknown parameters): 

$$
\mathcal{L}(\text{parameters} | \text{data}) = \prod^N_{i = 1} \text{PDF} (X_i | \text{parameters})
$$

Last we find the value(s) of the parameter(s) that maximize the likelihood (in practice, by minimizing the log-likelihood).

Now we are using the same idea to fit models to data:

$$
\mathcal{L}(\text{model parameters} | \text{data}) = \prod^N_{i = 1} \text{PDF} (X_i | \text{model parameters})
$$

**Question: What are the parameters in a linear regression with one continuous predictor?**

$\beta_0$, $\beta_1$, and $\sigma$

**Question: What are the parameters in a linear model with one categorical predictor (which has four levels)?**

$\mu$, $\alpha_1$, $\alpha_2$, $\alpha_3$ (depends on how you code the model, but there will be four parameters describing the mean behavior if the model is not overparameterized), and $\sigma$


**Question: What are the parameters in a Poisson generalized linear model with one continuous predictor?**

*Hint:* $Y_i \sim \mathrm{Pois} (\lambda_i), \quad log(\lambda_i) = \beta_0 + \beta_1 X_i$


$\beta_0$ and $\beta_1$

Remember the deviance difference from Week 10? Under the null hypothesis that the larger model fits no better than the smaller model, the deviance difference goes as a chi-square distribution.

$$D = 2 * \log \frac{\mathcal{L}_{\text{larger model}}}{\mathcal{L}_{\text{smaller model}}} = 2 (\ell_{\text{larger model}} - \ell_{\text{smaller model}}) \\
D | H_0 \sim \chi^2_{\text{additional parameters in larger model}}$$

This is called a likelihood ratio test. A large value for $D$ (likelihood ratio), unexpected under the null distribution, would indicate a significant **improvement** in using the larger model. In this case, we would prefer the larger model over the smaller model. Note that we can only do this for nested models.

**Question: Will the likelihood be larger for the larger model or for the smaller model?**

The likelihood will always be larger for the larger model (more parameters = more ability to fit the data).

**Question: Will the deviance difference, D, be positive or negative?**

Because the likelihood will always be larger for the larger model, D will always be positive. This makes sense because the $\chi^2$ distribution is always positive.


```{r}
Duncan.smaller <- lm(formula = prestige ~ income, data = Duncan)
Duncan.larger <- lm(formula = prestige ~ income + education, data = Duncan)
logLik(Duncan.larger)
logLik(Duncan.smaller)
Duncan.D <- 2 * (logLik(Duncan.larger) - logLik(Duncan.smaller))
Duncan.D
pchisq(q = Duncan.D, df = 1, lower.tail = FALSE)
```

```{r, fig.height=4, fig.width=5}
xvals <- seq(0, 40, by = 0.1)
par(mar = c(2, 2, 2, 2))
plot(x = xvals, y = dchisq(x = xvals, df = 1), type = "l", xlab = "Quantiles of chi-square[DOF = 1]", ylab = "Probability")
abline(v = qchisq(0.95, df = 1), col = "indianred2")
  text(x = qchisq(0.95, df = 1) + 4, y = 0.5, col = "indianred2", labels = "Critical value")
abline(v = Duncan.D, col = "dodgerblue2")
    text(x = Duncan.D + 6, y = 0.5, col = "dodgerblue2", labels = "Observed test stat.")
```

**Question: How do we interpret the results of our likelihood ratio test?**

The larger model explains significantly more variation in the data than we would expect.

### Akaike's Information Criterion (AIC)

AIC is just one of the many **information theoretic** alternatives that have been developed to compare the “divergence” between our particular model (which is an approximation of the true mechanism) and the “true” model (the mechanism that's actually going on in the real world). We always lose some information when approximating the truth using a model.

If you have a set of models that you could consider different alternative hypotheses, information theoretic methods allow you to rank them.

In practice, all information theoretic methods reduce to finding the model that minimizes some criterion that includes the sum of two terms: one is based on the **likelihood** and the other is a **penalty term** which penalizes for increasing model complexity. For AIC:

$$
\mathrm{AIC} = -2 \ell + 2 k
$$

where $\ell$ is the log-likelihood and $k$ is the total number of parameters in the model (including variance as a parameter when appropriate,  $\sigma^2$). **Smaller AIC means better model fit**.

```{r}
-2 * logLik(Duncan.smaller) + 2 * 3
-2 * logLik(Duncan.larger) + 2 * 4
```

**Question: Which model fits best?**

The larger model.


$$
\mathrm{AIC} = -2 \ell + 2 k
$$

The penalty term in AIC enforces parsimony, because adding an additional parameter will increase AIC by 2 unless the more complex model considerably increases the log-likelihood. 

When you have small sample size, or $n / k < 40$, use the small sample corrected AIC (AICc). This converges to AIC when sample size is large.

$$
\mathrm{AICc} = \mathrm{AIC} + \frac{2 k (k + 1)}{n - k - 1}
$$

```{r}
AIC(Duncan.smaller) + (2 * 3 * (3 + 1)) / (nrow(Duncan) - 3 - 1)
AIC(Duncan.larger) + (2 * 4 * (4 + 1)) / (nrow(Duncan) - 4 - 1)
```

**Question: Which model fits best?**

The larger model.

We can calculate the AIC of all possible candidate models and compare by calculating the difference in AIC between model *i* and the AIC of the best candidate model:

$$
\Delta \mathrm{AIC}_i = \mathrm{AIC}_i - \mathrm{AIC}_{\min}
$$

Information Criterion methods **do not allow us say that one model is significantly better**. We do not have a sampling distribution for differences in AIC, and we cannot use AIC to calculate a *P*-value for the significance of a model term. That said, there are some rules of thumb:

$\Delta \mathrm{AIC} < 2$ are considered equivalent

$4 < \Delta \mathrm{AIC} < 7$ are considered clearly different

$\Delta \mathrm{AIC} > 10$ are definitely different

**Question: Given this definition, is the larger model definitely still the better model?**

Yes, $\Delta \mathrm{AIC} > 10$.


### Bayesian Information Criterion (BIC)

BIC uses the same idea as AIC, but with a different penalty for model complexity. This is sometimes called the Schwarz criterion. **Smaller BIC means better model fit**.

$$
\mathrm{BIC} = -2 \ell + 2 k \times \ln(n)
$$

```{r}
-2 * logLik(Duncan.smaller) + 2 * 3 * log(nrow(Duncan))
-2 * logLik(Duncan.larger) + 2 * 4 * log(nrow(Duncan))
```

**Question: Which model fits best?**

The larger model.


The penalty for BIC is different than for AIC. For AIC, the probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on $k$ but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as sample size, $n$, gets large. Proponents of BIC like that it tends to select simpler models (fewer parameters) relative to AIC, because it is a consistent estimator of model fit.

However, Johnson and Omland (2004) include a strong critique of BIC. They state that it is not based in KL information theory, and it relies on the assumption that there is one true model and it is contained in the set of candidate models.

### Comparing LRT and AIC/BIC

1. LRT allows you to put a p-value on model terms. Because you have a sampling distribution for the difference in deviances, you can compare the difference in deviance to the quantiles on the chi-squared distribution to get a p-value.

2. AIC/BIC values do not allow you to refer to one model as being significantly different than another. No p-values.

3. LRT require nested models and Information Theoretic Criterion like AIC do not. This is a major advantage for AIC/BIC!

Information theoretic critera (AIC, AICc, BIC) allows you to compare all candidate models at once. We'll go through the process of model seelction and model averaging using AIC as an example, though you could use the same approach with any criterion.

1. Decide on your candidate set of models

2. Compute the likelihood and AIC for each of the candidate models

3. Rank the models in order of increasing AIC

After that, you have three options:

**Option 1:** Choose the model with the lowest AIC.
This is not recommended! You may have several models that are very close in AIC, so it is arbitrary to select the lowest AIC model as the unambiguously best model.

**Option 2:** Report all the models with $\Delta \mathrm{AIC} = 2$ of the best model.
This is better, because you are reporting all models that are basically indistinguishable from the best model, but there is no way to say how much better one model is from another.

**Option 3:** Model weighting. You consider the parameter estimates from all candidate models, where the better models are weighted more than the worse models.

Burnham and Anderson define model weights from the AIC values, where the weight is the proportion of times under repeated sampling that the $i^{\text{th}}$ model would be the best model.

$$
\Delta_i = \mathrm{AIC}_i - \min (\mathrm{AIC}) \\
w_i = \frac{\mathrm{e}^{-\Delta_i / 2}}{\sum_{i = 1}^M \mathrm{e}^{-\Delta_i / 2}}
$$

The denominator is summed over all candidate models. Burnham et al. (2011) describe the model weight as the strength of evidence, or the probability of model $i$ given the data and all $M$ models. 

## Model weighting

Model weights provide a number of benefits:

1. They explicitly address the fact that you don’t know the best model, so it is a straightforward way of presenting the relative strength of evidence for each of the models in the set of candidate models.

2. The model weights present the possibility of calculating weighted parameter estimates. This is the idea behind AIC **model averaging**. With AIC model averaging, we use the Akaike weights to weight the parameter estimates and variances (i.e., standard errors) from each model and combine those. Thus, we incorporate model selection uncertainty directly into the parameter estimates via the Akaike weights.

Model weighting and model averaging account for two types of uncertainty: the uncertainty about the parameters themselves, and the uncertainty about the model.

Let’s work through an example. Let’s say we have four possible candidate models – note that they are linear but not nested. Weighted parameters only work with linear models because the parameters have the same interpretation across the models.

$$
\text{Model 1}: Y = a, \quad \hat{a} = 1 \\ 
\text{Model 2}: Y = a + b X_1, \quad \hat{a} = 2, \hat{b} = 3 \\ 
\text{Model 3}: Y = a + b X_1 + c X_2, \quad \hat{a} = 1.5, \hat{b} = 2, \hat{c} = 7 \\ 
\text{Model 4}: Y = a + b X_1 + d X_3, \quad \hat{a} = 1.2, \hat{b} = 4, \hat{d} = 4
$$

Let’s say we calculated the following model weights:

$$
\Delta_i = \mathrm{AIC}_i - \min (\mathrm{AIC}) \\
w_i = \frac{\mathrm{e}^{-\Delta_i / 2}}{\sum_{i = 1}^M \mathrm{e}^{-\Delta_i / 2}}
$$

Model 1: *w* = 0.6,
Model 2: *w* = 0.1,
Model 3: *w* = 0.15,
Model 4: *w* = 0.15,

The model averaged estimate of parameter $a$ would be calculated by 

$$
0.6(1) + 0.1(2) + 0.15(1.5) + 0.15(1.2) = 1.205
$$

How do we calculate the model averaged estimate of $b$? This is a little trickier, because $b$ only appears in models 2, 3, 4. 

One method we can use is to calculate new weights for the only set of models in which $b$ appears, so our new model weights are: 

$$
\text{Model 2}: w = 0.1 / (0.1 + 0.15 + 0.15) = 0.25 \\
\text{Models 3 and 4}:w = 0.15 / (0.1 + 0.15 + 0.15) = 0.375
$$

and our model averaged estimate for $b$ is:

$$
0.25(3) + 0.375(2) + 0.375(4) = 3
$$

However, this doesn't take into account that $b$ may not appear in a model because it doesn't explain much variation in the response. We could also set $b$ to zero in models where it isn't included. This essentially "shrinks" the parameter estimate for $b$ if it doesn't appear in many models, bringing it closer to zero.

$$
0.6(0) + 0.1(3) + 0.15(2) + 0.15(4) = 1.2
$$

Be sure to explicitly state which of these two methods you used to weight parameter estimates in a report/thesis/manuscript.

What happens if all of your models are pretty bad, and you use model selection methods like the ones we've been discussing?

You'll still get a best/most parsimonious model!

You need to also provide evidence of model explanatory power using metrics like the coefficient of determination $R^2$ (or adjusted $R^2$, or marginal/conditional $R^2$ for mixed/hierarchical models, or goodness-of-fit tests) in addition to model fit diagnostics like AIC. Also don't forget about your coefficient hypothesis tests! See Mac Nally et al. (2017) for more on comparing model fit and model explanatory power.

$$
R^2 = \frac{\mathrm{SSR}}{\mathrm{SST}} = \frac{\sum_{i = 1}^n (Y_i - \hat{Y}_i)^2}{\sum_{i = 1}^n (Y_i - \bar{Y})^2}
$$

Which models to include in model selection?

In many situations, you might have a large number of potential covariates to include in a model, all of which could have biological significance to your system. You have many choices to make as the investigator. You could:

1. **Generate a subset of biologically reasonable models to test**. In this case you would support your selections by whatever theory is reasonable for your system.

2. **Test all possible subsets of models**. In this case, you would calculate AIC/BIC for models with all possible combinations of covariates. This becomes computationally expensive as the list of predictors increases in length, especially if you are testing for interactions as well as main effects.

3. **Use stepwise regression**. Here, you would add in or remove variables one at a time to find the best model based on some criterion.

## Stepwise regression

With stepwise regression, you add in or remove parameters one at a time, then a test is done to check whether some variables can be deleted without appreciably increasing the residual sum of squares (RSS) or some other criterion (for example, AIC can be used). The procedure stops when the available improvement falls below some critical value. Using this approach, you do not test all possible models, because the algorithm only compares models to the optimal model from the previous step.

The main approaches are:

* Forward selection, which involves starting with no variables in the model, trying out the variables one by one and including them if they are 'statistically significant'.

* Backward elimination, which involves starting with all candidate variables and testing them one by one for statistical significance, deleting any that are not significant.

* Methods that are a combination of the above, testing at each stage for variables to be included or excluded.

### Criticism of stepwise regression

A sequence of F-tests is often used to control the inclusion or exclusion of variables, but these are carried out on the same data and so there will be problems of multiple comparisons for which many correction criteria have been developed. It is therefore difficult to interpret the *P*-values associated with these tests, since each is conditional on the previous tests of inclusion and exclusion (see "dependent tests" in false discovery rate). When estimating the degrees of freedom, the number of the candidate independent variables from the best fit selected is smaller than the total number of final model variables, causing the fit to appear better than it is when adjusting the $R^2$ value for the number of degrees of freedom. It is important to consider how many degrees of freedom have been used in the entire model, not just count the number of independent variables in the resulting fit.

### Criticism of data dredging

Automated model selection can be considered data dredging (methods 2 and 3). Critics regard data dredging as substituting intense computation for subject area expertise. You may have many predictors even in a carefully thought out model because biological systems are complex and we may want to know which predictors, from a set of reasonable predictors, best explain the variation in the data. However, we should be careful to recognize that we don't really understand the Type I error rate for the entire model selection process and our findings should be considered exploratory; where automated model selection has been used to find the best set of predictors, you may need to use that information to propose new hypotheses to be tested more rigorously with a new dataset.

### Final thoughts on model selection

If the goal is hypothesis testing, think carefully about the models ahead of time and consider a smaller set of candidate models, all of which make biological sense.

If the goal is prediction, you can start with a wider set of candidate models, and you are less worried that the correlations stem from true causation.

**Don’t let the computation drive the biology.** It is OK and often appropriate to leave covariates in the model even if they are not statistically significant if you believe them to be biologically significant.

## Week 13 FAQ

**Question: How to we calculate explanatory power?**

$$
r^{2} = \frac{SSR}{SST} = \frac{1-SSE}{SST} = \frac{1-\mbox{sum-of-squares error}}{\mbox{sum-of-squares total}}
$$

Going back to Week 9, we note that the sum-of-squares total is given by

$$
SST = \sum_{i=1}^{n}(Y_{i}-\bar{Y})^{2}
$$
and the sum-of-squares error

$$
SSE = \sum_{i=1}^{n}(Y_{i}-\hat{Y_{i}})^{2}
$$

**Question: How exactly does Mallow's Cp work?**

Mallows Cp is used when you are comparing models with different numbers of parameters. If the model with p parameters is correct than Cp will tend to be close to or smaller than p. Mallows Cp will be close to p if the model is unbiased and so Mallows Cp is used if unbiasedness is a criteria of particular interest.

**Question: What is the rational behind the penalty factor in BIC and why does the penalty grow with n?**

If you are comparing two nested models, AIC is equivalent to using a cutoff for the difference in -2LogLik between the two models equal to 2*k, where k is the difference in the number of parameters between the two models. (Note that AIC does not require nested models, but the case of nested models allows us to understand the penalty factor for BIC.) The probability of Type I error (in this case, choosing the larger model erroneously, that is, when the smaller model is the true model) depends on “k” but does not depend on sample size. This means that as sample size goes to infinity, you still choose the larger model with some probability (the Type I error rate). When this happens, we say that an estimator is not “consistent”. The BIC corrects for this by increasing the penalty term as n gets large.

**Question: How do we do model averaging when the parameter in question is not included in all models?**

In these cases, we have to restrict attention to the models that include the parameter of question, and recalculate the AIC weights within that subset. For example, in the example presented in lecture, “b” only appears in Models 2,3, and 4.

Step 1 therefore is to recalculate AIC weights as follows:

$$
w_{AIC, model2} = \frac{0.1}{0.1+0.15+0.15} = 0.25
$$
$$
w_{AIC, model3} = \frac{0.15}{0.1+0.15+0.15} = 0.375
$$

$$
w_{AIC, model4} = \frac{0.15}{0.1+0.15+0.15} = 0.375
$$

The weighted parameter estimate for “b” would then be

$$
\hat{b} = (0.25 \times 3) + (0.375 \times 2) + (0.375 \times 4) = 3
$$

